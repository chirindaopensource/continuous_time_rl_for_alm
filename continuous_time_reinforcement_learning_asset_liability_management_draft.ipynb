{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D7n-eg2YKtx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Continuous-Time Reinforcement Learning for Asset-Liability Management\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2509.23280-b31b1b.svg)](https://arxiv.org/abs/2509.23280)\n",
        "[![Conference](https://img.shields.io/badge/Conference-ICAIF%20'25-9cf)](https://icaif.acm.org/2025/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/continuous_time_reinforcement_learning_asset_liability_management)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Quantitative%20Finance%20%7C%20RL-00529B)](https://github.com/chirindaopensource/continuous_time_reinforcement_learning_asset_liability_management)\n",
        "[![Primary Data](https://img.shields.io/badge/Data-Simulated%20SDE-lightgrey)](https://github.com/chirindaopensource/continuous_time_reinforcement_learning_asset_liability_management)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Continuous--Time%20RL-orange)](https://github.com/chirindaopensource/continuous_time_reinforcement_learning_asset_liability_management)\n",
        "[![Key Concepts](https://img.shields.io/badge/Concepts-LQ%20Control%20%7C%20Soft%20Actor--Critic-red)](https://github.com/chirindaopensource/continuous_time_reinforcement_learning_asset_liability_management)\n",
        "[![Baselines](https://img.shields.io/badge/Baselines-SAC%20%7C%20PPO%20%7C%20DDPG%20%7C%20CPPI-blueviolet)](https://github.com/chirindaopensource/continuous_time_reinforcement_learning_asset_liability_management)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Gymnasium](https://img.shields.io/badge/Gymnasium-0086D1?style=flat)](https://gymnasium.farama.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/continuous_time_rl_for_alm`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Continuous-Time Reinforcement Learning for Asset-Liability Management\"** by:\n",
        "\n",
        "*   Yilie Huang\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's novel continuous-time reinforcement learning approach to ALM. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous, reproducible experimental setup and parallelized simulation to comprehensive statistical analysis and the generation of all publication-quality figures and tables.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callables](#key-callables)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Continuous-Time Reinforcement Learning for Asset-Liability Management.\" The core of this repository is the iPython Notebook `continuous_time_reinforcement_learning_asset_liability_management_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of all analytical tables and figures.\n",
        "\n",
        "The paper introduces a novel model-free, continuous-time reinforcement learning (RL) algorithm for the Asset-Liability Management (ALM) problem. It frames the problem as a Linear-Quadratic (LQ) control task and develops a soft actor-critic method with adaptive exploration to dynamically manage the surplus deviation between assets and liabilities. This codebase operationalizes this framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration.\n",
        "-   Systematically generate reproducible, randomized market scenarios based on a stochastic differential equation (SDE) model.\n",
        "-   Execute large-scale, parallelized simulations comparing the proposed ALM-RL agent against six distinct baselines.\n",
        "-   Perform comprehensive statistical analysis using non-parametric tests to validate performance claims.\n",
        "-   Conduct a full suite of robustness analyses, including hyperparameter sensitivity, market parameter stress tests, and discretization analysis.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in stochastic optimal control, reinforcement learning, and numerical methods for SDEs.\n",
        "\n",
        "**1. ALM as a Linear-Quadratic (LQ) Control Problem:**\n",
        "The core of the problem is to control the surplus deviation, `x(t)`, from a target. Its dynamics are modeled by the SDE:\n",
        "$$\n",
        "dx(t) = (A x(t) + B u(t))dt + (C x(t) + D u(t))dW(t)\n",
        "$$\n",
        "where `u(t)` is the control action. The objective is to maximize the expected value of a quadratic functional that penalizes deviations over a finite horizon `[0, T]`:\n",
        "$$\n",
        "\\max_{u} \\mathbb{E}\\left[ \\int_{0}^{T} -\\frac{1}{2}Qx(t)^2 dt - \\frac{1}{2}Hx(T)^2 \\right]\n",
        "$$\n",
        "\n",
        "**2. Continuous-Time Soft Actor-Critic:**\n",
        "Since the market parameters `A, B, C, D` are unknown, a model-free RL approach is used. The paper develops a continuous-time soft actor-critic algorithm based on an entropy-regularized objective:\n",
        "$$\n",
        "J(t, x; \\pi) = \\mathbb{E}\\left[ \\int_{t}^{T} \\left(-\\frac{1}{2}Qx(s)^2 + \\gamma p(s)\\right) ds - \\frac{1}{2}Hx(T)^2 \\Big| x(t)=x \\right]\n",
        "$$\n",
        "where `p(s)` is the entropy of the stochastic policy `π`.\n",
        "\n",
        "**3. Key Algorithmic Features:**\n",
        "-   **Parametric Forms:** Based on LQ theory, the value function `J` is parameterized as a quadratic function of `x`, and the policy `π` is a Gaussian distribution whose mean is linear in `x`.\n",
        "-   **Adaptive Exploration:** The policy's variance (actor exploration) is learned via policy gradient.\n",
        "-   **Scheduled Exploration:** The entropy temperature `γ` (critic exploration) follows a deterministic, decaying schedule.\n",
        "-   **Update Rules:** The agent learns via discretized versions of continuous-time temporal difference and policy gradient updates (Eqs. 16, 17, 18 in the paper).\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`continuous_time_reinforcement_learning_asset_liability_management_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 13 distinct, modular tasks, each with its own orchestrator function, covering validation, setup, simulation, analysis, and reporting.\n",
        "-   **Configuration-Driven Design:** All experimental parameters are managed in an external `config.yaml` file, allowing for easy customization and replication without code changes.\n",
        "-   **Multi-Algorithm Support:** Complete, from-scratch implementations of the proposed **ALM-RL** agent and six baselines: **DCPPI**, **ACS**, **MBP**, **SAC**, **PPO**, and **DDPG**.\n",
        "-   **Rigorous Reproducibility:** A multi-level seeding protocol ensures bitwise reproducibility of market scenarios and isolates stochastic streams for fair agent comparison.\n",
        "-   **Parallelized Execution:** The main experimental pipeline is designed for parallel execution across multiple CPU cores, dramatically reducing the time required for the 200 independent runs.\n",
        "-   **Comprehensive Analysis Suite:** Implements the full statistical analysis from the paper, including moving average smoothing, terminal performance extraction, and one-sided Wilcoxon signed-rank tests.\n",
        "-   **Robustness Analysis Module:** Includes a full suite of post-hoc analyses to test hyperparameter sensitivity, robustness to extreme market conditions, and sensitivity to SDE discretization.\n",
        "-   **Automated Reporting:** Programmatic generation of all key tables and figures from the paper.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation (Task 1):** Ingests and rigorously validates the `config.yaml` for structural, mathematical, and logical consistency.\n",
        "2.  **Setup (Task 2):** Establishes the deterministic seeding hierarchy for the entire experiment.\n",
        "3.  **Initialization (Task 3):** Generates the 200 randomized market scenarios and the corresponding initial parameters for all agents.\n",
        "4.  **Agent & Environment Implementation (Tasks 4-7):** Provides complete, professional-grade implementations of all agents and the SDE environment.\n",
        "5.  **Execution (Task 8):** Runs the main simulation pipeline in parallel, executing 20,000 episodes for each of the 7 agents across all 200 market scenarios.\n",
        "6.  **Metrics & Analysis (Tasks 9-10):** Processes the raw simulation data to compute smoothed learning curves, terminal performance, and the final p-value matrix.\n",
        "7.  **Visualization (Task 11):** Generates the final, publication-quality plots and summary tables.\n",
        "8.  **Orchestration & Robustness (Tasks 12-13):** Provides top-level orchestrators to run the main pipeline and the additional robustness analyses.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `continuous_time_reinforcement_learning_asset_liability_management_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callables\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`main`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. It can be configured to run the main reproduction experiment, the robustness analyses, or both. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `numpy`, `pandas`, `scipy`, `pyyaml`, `torch`, `gymnasium`, `matplotlib`, `seaborn`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/continuous_time_rl_for_alm.git\n",
        "    cd continuous_time_reinforcement_learning_asset_liability_management\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install numpy pandas scipy pyyaml torch gymnasium matplotlib seaborn tqdm\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline is driven by a single `config.yaml` file. No external datasets are required, as the market scenarios are procedurally generated based on the parameters within this file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `continuous_time_reinforcement_learning_asset_liability_management_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which calls the top-level `main` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook or in a main.py script\n",
        "\n",
        "# Load the configuration from the YAML file.\n",
        "STUDY_INPUTS = load_config('config.yaml')\n",
        "\n",
        "# Run the entire study (reproduction and robustness analysis).\n",
        "final_artifacts = main(\n",
        "    study_params=STUDY_INPUTS,\n",
        "    run_reproduction=True,\n",
        "    run_robustness=True,\n",
        "    num_workers=8  # Adjust based on available CPU cores\n",
        ")\n",
        "\n",
        "# The `final_artifacts` dictionary will contain the key results DataFrames.\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `main` function creates one or two output directories (`alm_rl_reproduction_output/` and `alm_rl_robustness_output/`) with the following structure:\n",
        "\n",
        "```\n",
        "output_directory/\n",
        "│\n",
        "├── data/\n",
        "│   ├── seed_table.csv\n",
        "│   ├── market_params_table.csv\n",
        "│   ├── alm_rl_initial_table.csv\n",
        "│   ├── baselines_initial_table.csv\n",
        "│   ├── raw_results.npy\n",
        "│   ├── learning_curves.csv\n",
        "│   ├── terminal_performance.csv\n",
        "│   └── p_value_matrix.csv\n",
        "│\n",
        "├── figures/\n",
        "│   ├── figure1_learning_curves.png\n",
        "│   └── figure2_p_value_heatmap.png\n",
        "│\n",
        "└── tables/\n",
        "    └── table1_summary_statistics.html\n",
        "```\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "continuous_time_rl_for_alm/\n",
        "│\n",
        "├── continuous_time_reinforcement_learning_asset_liability_management_draft.ipynb # Main implementation notebook\n",
        "├── config.yaml                                                                   # Master configuration file\n",
        "├── requirements.txt                                                              # Python package dependencies\n",
        "├── LICENSE                                                                       # MIT license file\n",
        "└── README.md                                                                     # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all experimental parameters, including the number of runs/episodes, SDE parameter distributions, agent hyperparameters, and evaluation settings, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative SDE Models:** Integrating more complex market models, such as those with stochastic volatility (e.g., Heston model) or jumps.\n",
        "-   **Multi-Asset Formulations:** Extending the state and action spaces to handle a portfolio of multiple assets.\n",
        "-   **Automated Hyperparameter Tuning:** Wrapping the pipeline with a hyperparameter optimization library (e.g., Optuna) to automatically find the best settings for the ALM-RL agent.\n",
        "-   **Real-World Data Application:** Adapting the framework to use historical financial data by first estimating the SDE parameters from time series data.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{huang2025continuous,\n",
        "  author    = {Huang, Yilie},\n",
        "  title     = {Continuous-Time Reinforcement Learning for Asset-Liability Management},\n",
        "  booktitle = {Proceedings of the 6th ACM International Conference on AI in Finance},\n",
        "  series    = {ICAIF '25},\n",
        "  year      = {2025},\n",
        "  publisher = {ACM},\n",
        "  note      = {arXiv:2509.23280}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of the \"Continuous-Time RL for ALM\" Framework.\n",
        "GitHub repository: https://github.com/chirindaopensource/continuous_time_rl_for_alm\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Yilie Huang** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **NumPy, Pandas, SciPy, PyTorch, Gymnasium, Matplotlib, and Jupyter**, whose work makes complex computational analysis accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `continuous_time_reinforcement_learning_asset_liability_management_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "lt76YDqgb0BR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Continuous-Time Reinforcement Learning for Asset-Liability Management*\"\n",
        "\n",
        "Author: Yilie Huang\n",
        "\n",
        "E-Journal Submission Date: 27 September 2025\n",
        "\n",
        "Conference Affiliation: Accepted at the 6th ACM International Conference on AI in Finance (ICAIF 2025)\n",
        "\n",
        "Link: https://arxiv.org/abs/2509.23280\n",
        "\n",
        "Abstract:\n",
        "\n",
        "This paper proposes a novel approach for Asset-Liability Management (ALM) by employing continuous-time Reinforcement Learning (RL) with a linear-quadratic (LQ) formulation that incorporates both interim and terminal objectives. We develop a model-free, policy gradient-based soft actor-critic algorithm tailored to ALM for dynamically synchronizing assets and liabilities. To ensure an effective balance between exploration and exploitation with minimal tuning, we introduce adaptive exploration for the actor and scheduled exploration for the critic. Our empirical study evaluates this approach against two enhanced traditional financial strategies, a model-based continuous-time RL method, and three state-of-the-art RL algorithms. Evaluated across 200 randomized market scenarios, our method achieves higher average rewards than all alternative strategies, with rapid initial gains and sustained superior performance. The outperformance stems not from complex neural networks or improved parameter estimation, but from directly learning the optimal ALM strategy without learning the environment."
      ],
      "metadata": {
        "id": "EUaKfhp9Yiyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Summary and Analysis of \"Continuous-Time Reinforcement Learning for Asset-Liability Management\"\n",
        "\n",
        "This paper presents a novel, model-free framework for solving the Asset-Liability Management (ALM) problem. The core contribution is the development and empirical validation of a continuous-time reinforcement learning algorithm that directly learns an optimal management strategy without needing to first estimate the underlying financial market dynamics.\n",
        "\n",
        "Let's break down the paper's argument and methodology step-by-step.\n",
        "\n",
        "#### A Refined Problem Formulation for ALM\n",
        "\n",
        "The author begins by critiquing the traditional Mean-Variance (MV) approach to ALM. While foundational, the MV framework suffers from two key drawbacks:\n",
        "1.  **Time-Inconsistency:** An optimal policy calculated at time `t=0` may no longer be optimal at a future time `t>0`.\n",
        "2.  **Terminal Focus:** It primarily penalizes deviation from a target at the terminal horizon `T`, neglecting the path of the surplus during the management period, which is critical for solvency and stability.\n",
        "\n",
        "To overcome this, the paper proposes a **Linear-Quadratic (LQ) control formulation**. The key innovations here are:\n",
        "*   **State Variable:** Instead of modeling the raw surplus, the state `x(t)` is defined as the **surplus deviation** from a target (`x(t) = Assets(t) – Liabilities(t) - Target Surplus`). This elegantly centers the problem around the control objective of minimizing deviation.\n",
        "*   **Objective Function:** The goal is to maximize an objective functional that penalizes the squared deviation `x(t)²` over the *entire time horizon* `[0, T]` as well as at the terminal time `T`. This integral cost addresses the path-dependency issue neglected by traditional MV formulations.\n",
        "*   **Dynamics:** The surplus deviation is modeled by a linear Stochastic Differential Equation (SDE), where both the drift and volatility terms are functions of the state `x(t)` and the control `u(t)`. This captures the essential dynamics where management actions affect both expected returns and risk.\n",
        "\n",
        "This formulation recasts ALM as a classic, albeit \"indefinite,\" stochastic LQ control problem, for which a rich theory exists *if the model parameters are known*.\n",
        "\n",
        "#### Transitioning from Control Theory to Model-Free Reinforcement Learning\n",
        "\n",
        "The crucial insight of the paper is that in reality, the parameters of the SDE (A, B, C, D) are unknown and difficult to estimate accurately. This motivates the shift from a model-based control paradigm to a **model-free reinforcement learning** one.\n",
        "\n",
        "The author leverages the theoretical framework of entropy-regularized continuous-time RL. This framework introduces an entropy term into the objective function, which encourages exploration by favoring stochastic policies over deterministic ones. A key result from this theory is that for the LQ problem at hand, the optimal solution has a known structure:\n",
        "*   The **optimal value function** is quadratic in the state `x`.\n",
        "*   The **optimal policy** is a Gaussian distribution whose mean is linear in the state `x`.\n",
        "\n",
        "This theoretical result is paramount. It allows the author to bypass the intractable problem of non-parametric function approximation and instead use simple, low-dimensional parameterizations for the actor (policy) and the critic (value function).\n",
        "\n",
        "#### The ALM-RL Algorithm: A Tailored Soft Actor-Critic\n",
        "\n",
        "Building on this foundation, the paper develops a policy gradient-based soft actor-critic algorithm specifically for this ALM context. The key components are:\n",
        "\n",
        "1.  **Function Parameterization:** The critic `J(t,x; θ)` is parameterized as a quadratic function of `x`, and the actor `π(u|x; φ)` is a Gaussian policy `N(φ₁x, φ₂)`. This is a direct and efficient implementation of the theoretical insight from Step 2.\n",
        "\n",
        "2.  **Policy Evaluation & Improvement:** The algorithm iteratively updates the parameters `θ` (for the critic) and `φ` (for the actor) using continuous-time versions of Temporal Difference (TD) learning and Policy Gradient (PG) methods, respectively.\n",
        "\n",
        "3.  **Novel Exploration Mechanisms:** This is a significant practical contribution. Instead of relying on fixed, manually-tuned hyperparameters for exploration, the author introduces two dynamic mechanisms:\n",
        "    *   **Adaptive Actor Exploration:** The variance of the policy, `φ₂`, which governs the actor's exploration level, is not fixed or decayed on a schedule. Instead, it is *learned* via a policy gradient update. This allows the agent to adapt its exploration level based on the data it observes.\n",
        "    *   **Scheduled Critic Exploration:** The temperature parameter `γ`, which weights the entropy bonus in the objective function, is decayed over time according to a predefined schedule. This ensures that the agent explores broadly in the beginning and gradually shifts its focus to exploitation as it becomes more confident in its policy.\n",
        "\n",
        "#### Theoretical Guarantees and Empirical Validation\n",
        "\n",
        "*   **Convergence:** The paper provides a formal theorem (Theorem 1) proving the **almost sure convergence** of the policy parameters to their \"oracle\" values (i.e., the optimal values that would be known if the market model were given). This provides crucial theoretical reassurance of the algorithm's stability and correctness.\n",
        "\n",
        "*   **Experiments:** The empirical study is robust. The algorithm is tested against a strong set of baselines:\n",
        "    *   Enhanced traditional methods (Dynamic CPPI, Adaptive Contingent Strategy).\n",
        "    *   A model-based continuous-time RL approach (MBP).\n",
        "    *   Three state-of-the-art discrete-time RL algorithms (SAC, PPO, DDPG).\n",
        "\n",
        "The key feature of the experimental design is the use of **200 randomized market scenarios**. This tests the algorithm's robustness and ability to generalize, rather than its performance on a single, fixed environment. The results are unequivocal: the proposed ALM-RL algorithm consistently and statistically significantly outperforms all baselines, demonstrating faster learning, higher final rewards, and greater stability.\n",
        "\n",
        "### Concluding Remarks and Core Insight\n",
        "\n",
        "The paper's superior results do not stem from using more complex neural networks or more sophisticated estimators. Instead, the outperformance is rooted in a fundamental methodological advantage.\n",
        "\n",
        "Traditional quantitative finance often follows a two-step \"estimate-then-optimize\" process. This is brittle because financial models are notoriously difficult to estimate accurately, and errors in the estimation phase propagate and amplify in the optimization phase. The model-based RL approach (MBP) is a victim of this exact problem, as seen by its performance stagnation.\n",
        "\n",
        "This paper's approach, by contrast, **directly learns the optimal strategy (the policy) from data**. It bypasses the intermediate step of environment modeling entirely. By leveraging the known structure of the LQ problem to create an efficient parameterization, it combines the flexibility of model-free learning with the sample efficiency of a well-posed structure. This is a powerful and elegant paradigm for decision-making under uncertainty in financial markets."
      ],
      "metadata": {
        "id": "DKL8dtI5dvtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "z8HRskQHQ2Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# =============================================================================#\n",
        "#\n",
        "#  Continuous-Time Reinforcement Learning for Asset-Liability Management\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework and experimental protocol presented in \"Continuous-Time\n",
        "#  Reinforcement Learning for Asset-Liability Management\" by Yilie Huang (2025).\n",
        "#  It delivers a robust, model-free system for dynamically optimizing\n",
        "#  asset-liability management strategies under uncertainty, suitable for\n",
        "#  reproducing the paper's results and for extension to related financial\n",
        "#  control problems.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Linear-Quadratic (LQ) control formulation for the ALM problem.\n",
        "#  • Continuous-time soft actor-critic (SAC) reinforcement learning algorithm.\n",
        "#  • Adaptive exploration for the actor (policy) and scheduled exploration\n",
        "#    for the critic (value function).\n",
        "#  • Model-free, policy-gradient-based parameter updates without environment estimation.\n",
        "#  • Comprehensive empirical evaluation against traditional, model-based, and\n",
        "#    state-of-the-art deep reinforcement learning baselines.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Deterministic reproducibility pipeline with isolated stochastic streams.\n",
        "#  • Modular agent implementations for ALM-RL, DCPPI, ACS, MBP, SAC, PPO, and DDPG.\n",
        "#  • Gymnasium-compliant environment wrapper for SDE simulation via Euler-Maruyama.\n",
        "#  • Parallelized experimental execution for large-scale simulation runs.\n",
        "#  • Automated data processing for learning curve smoothing and terminal performance.\n",
        "#  • Rigorous statistical analysis using non-parametric paired tests.\n",
        "#  • Publication-quality visualization of results.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Huang, Y. (2025). Continuous-Time Reinforcement Learning for Asset-Liability\n",
        "#  Management. In Proceedings of the 6th ACM International Conference on AI in\n",
        "#  Finance (ICAIF 2025). arXiv preprint arXiv:2509.23280.\n",
        "#  https://arxiv.org/abs/2509.23280\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# =============================================================================#\n",
        "\n",
        "# =============================================================================\n",
        "# Consolidated Imports for ALM-RL Reproduction Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import copy\n",
        "import hashlib\n",
        "import itertools\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import pprint\n",
        "import time\n",
        "import warnings\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from typing import Any, Deque, Dict, List, Optional, Tuple\n",
        "\n",
        "# --- Third-Party Library Imports ---\n",
        "\n",
        "# Core numerical and data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Reinforcement learning environment\n",
        "import gymnasium as gym\n",
        "\n",
        "# PyTorch for deep learning models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "# Scientific computing and statistics\n",
        "from scipy.stats import qmc, wilcoxon\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Progress bar utility\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "a3WxVCEkRECd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "CfiOBRi1RHKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Analysis of Key Orchestrator Callables**\n",
        "\n",
        "#### **Task 1: `validate_study_inputs`**\n",
        "\n",
        "*   **Inputs:** A single Python dictionary, `study_params`, which is expected to conform to the complete `STUDY_INPUTS` structure.\n",
        "*   **Processes:**\n",
        "    1.  **Structural Validation:** It orchestrates a deep, recursive traversal of the input dictionary, comparing its keys, value types, and specific literal values against a complete, hardcoded schema that mirrors the entire `STUDY_INPUTS` specification.\n",
        "    2.  **Consistency Validation:** It performs specific mathematical cross-checks, most notably verifying the relationship between the time horizon `T`, the discretization step `Δt`, and the number of steps per episode `K`.\n",
        "    3.  **Constraint Validation:** It systematically checks dozens of numerical parameters against their required logical and mathematical constraints (e.g., positivity of variances, validity of learning rate exponents).\n",
        "    4.  **Architectural Validation:** It inspects the specifications for the Deep RL baselines, ensuring network input/output dimensions are consistent with the requirements of each algorithm (SAC, PPO, DDPG).\n",
        "    5.  **Error Aggregation:** It collects any and all validation failures from the above processes into a single, comprehensive list.\n",
        "*   **Outputs:** The function has no return value (`None`). Its output is binary: it either completes silently, signifying that the configuration is valid, or it raises a single, detailed `ValueError` that enumerates every detected issue.\n",
        "*   **Data Transformation:** This function is purely for validation and does not transform the input data. It reads the `study_params` dictionary and produces a list of error strings, which is then used to construct an exception if the list is non-empty.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **Gatekeeper of Reproducibility and Correctness**. It is the first and most critical step in the entire pipeline. Its purpose is to guarantee that the experiment is initiated with a configuration that is structurally sound, mathematically consistent, and compliant with the paper's specifications. By failing fast on any deviation, it prevents the execution of computationally expensive simulations that would be invalid from the outset. It ensures that the foundational parameters for equations like the SDE dynamics and learning schedules are correctly specified before they are ever used.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 2: `setup_computation_and_rng`**\n",
        "\n",
        "*   **Inputs:** The validated `study_params` dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Base Seed Generation:** It instantiates a `numpy.random.PCG64` generator with the `master_seed` (42) and uses it to produce a deterministic, reproducible array of 200 `base_seeds`.\n",
        "    2.  **Algorithm Seed Derivation:** For each `base_seed`, it applies a deterministic hashing procedure (`hashlib.sha256`) to derive 7 unique, algorithm-specific seeds. This process ensures that for any given run, each algorithm has its own independent (but reproducible) stream of randomness.\n",
        "    3.  **Resource Estimation:** It calculates the total computational load (in episodes) and estimates the memory requirements for the experiment under different storage strategies.\n",
        "*   **Outputs:** A tuple containing:\n",
        "    1.  A `pandas.DataFrame` (`seed_table`) of shape `(200, 9)`, indexed by `run_id`, containing the `base_seed` and the 7 derived algorithm-specific seeds for each run.\n",
        "    2.  A `dict` (`resource_report`) containing the structured computational and memory estimates.\n",
        "*   **Data Transformation:** The function transforms a single integer (`master_seed`) and a list of algorithm names into a complete tabular mapping of seeds for the entire experiment. It also transforms numerical parameters from the configuration into a summary report.\n",
        "*   **Role in Research Pipeline:** This callable establishes the **Foundation of Stochastic Control**. Its primary role is to implement the rigorous seeding protocol required for a fair and reproducible comparison of stochastic algorithms. By creating a clear hierarchy of seeds (master -> base -> algorithm-specific), it ensures that the two main sources of randomness—market dynamics (`dW(t)`) and agent policy/initialization—are properly isolated and controlled. This is the technical implementation of the experimental design principle that all agents must face identical market conditions.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 3: `generate_initial_conditions`**\n",
        "\n",
        "*   **Inputs:** The `seed_table` DataFrame from Task 2 and the `study_params` dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Market Generation:** It iterates through each `run_id`, uses the corresponding `base_seed` to initialize an RNG, and samples the SDE parameters `A, B, C, D` from their specified uniform distributions. It then calculates the theoretical optimal policy gain `phi1_star`.\n",
        "    2.  **ALM-RL Initialization:** It iterates through each `run_id`, uses the `seed_alm_rl` to initialize an RNG, samples the initial `theta` and `phi1` parameters from a Normal distribution, and applies the specified projection (clipping).\n",
        "    3.  **Baseline Initialization:** It iterates through each `run_id`, uses the `seed_dcppi` and `seed_acs` to initialize separate RNGs, and samples the initial `m0` multipliers for each.\n",
        "*   **Outputs:** A tuple of three `pandas.DataFrame`s:\n",
        "    1.  `market_params_table`: Shape `(200, 5+)`, containing `A, B, C, D, phi1_star` for each run.\n",
        "    2.  `alm_rl_initial_table`: Shape `(200, 5)`, containing `x0, theta1_0, theta2_0, phi1_0, phi2_0` for each run.\n",
        "    3.  `baselines_initial_table`: Shape `(200, 3)`, containing `m0_dcppi, m0_acs, tolerance_delta` for each run.\n",
        "*   **Data Transformation:** This function transforms the table of seeds into three distinct tables of concrete numerical parameters that define the starting state of the entire simulation for all 200 runs.\n",
        "*   **Role in Research Pipeline:** This callable **Materializes the Experimental Scenarios**. It takes the abstract seeding protocol from Task 2 and uses it to generate the specific, heterogeneous market environments and randomized agent starting points for the entire study. It is the direct implementation of the \"200 randomized market scenarios\" described in the paper's abstract and Section 5.2. It creates the concrete data that will drive the simulations in Task 8.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 8: `execute_full_experiment`**\n",
        "\n",
        "*   **Inputs:** The `study_params` dictionary and all the data tables generated by Task 3. An optional `num_workers` argument controls parallelism.\n",
        "*   **Processes:**\n",
        "    1.  **Parallelization Setup:** It initializes a `multiprocessing.Pool` to manage a pool of worker processes.\n",
        "    2.  **Task Distribution:** It prepares a list of tasks, where each task consists of the arguments needed for `execute_single_run_worker` for a unique `run_id`. It distributes these tasks to the worker pool.\n",
        "    3.  **Worker Execution (`execute_single_run_worker`):** Each worker process executes one full, independent run. It initializes all 7 agents and the environment(s) for its assigned `run_id`. Crucially, it ensures the SDE's random number generator is re-seeded with the run's `base_seed` for each agent's training loop, guaranteeing identical market noise for all competitors. It then simulates all 20,000 episodes for each agent, calling the appropriate episode runner and update logic.\n",
        "    4.  **Results Aggregation:** The main process collects the dictionaries of reward arrays from each completed worker and writes them into the correct slice of a pre-allocated 3D NumPy array.\n",
        "*   **Outputs:** A single `numpy.ndarray` of shape `(200, 7, 20000)`, containing the raw episode reward for every algorithm in every run for every episode.\n",
        "*   **Data Transformation:** This function transforms the static initial condition tables into a comprehensive, high-dimensional array of time-series performance data. It is the computational core of the entire project, transforming static setup into dynamic results.\n",
        "*   **Role in Research Pipeline:** This callable is the **Computational Engine of the Empirical Study**. It executes the main agent-environment interaction loop described throughout the paper. It is the direct implementation of the phrase \"Evaluated across 200 randomized market scenarios\" from the abstract. It simulates the learning process of every agent, from the proposed ALM-RL method to all six baselines, and generates the raw performance data upon which all subsequent conclusions are based. It implements the core learning loops for all algorithms, including the discretized versions of the ALM-RL update equations:\n",
        "    $$\n",
        "    \\theta_{n+1} \\leftarrow \\Pi_{K}\\left(\\theta_{n} + a_{n} \\sum_{k=0}^{K-1} \\left[ \\dots \\right] \\right)\n",
        "    $$\n",
        "    $$\n",
        "    \\phi_{1,n+1} \\leftarrow \\Pi_{K_1}\\left(\\phi_{1,n} + a_{n} \\sum_{k=0}^{K-1} \\left[ \\dots \\right] \\right)\n",
        "    $$\n",
        "    $$\n",
        "    \\phi_{2,n+1} \\leftarrow \\Pi_{K_2}\\left(\\phi_{2,n} - a_{n} \\sum_{k=0}^{K-1} \\left[ \\dots \\right] \\right)\n",
        "    $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 9: `process_performance_metrics`**\n",
        "\n",
        "*   **Inputs:** The raw 3D `numpy.ndarray` of results from Task 8 and the `study_params` dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Data Validation:** It first performs integrity checks on the raw results array (shape, `NaN` values, etc.).\n",
        "    2.  **Smoothing:** It transforms the raw data into a long-form DataFrame and applies a centered rolling mean with a 200-episode window to each of the `200 * 7` individual time series.\n",
        "    3.  **Aggregation:** It aggregates these smoothed curves across the 200 runs for each algorithm, computing the mean, 25th percentile, and 75th percentile for each episode.\n",
        "    4.  **Terminal Performance Extraction:** It slices the *raw* results array to select the last 500 episodes for each run and algorithm and computes the mean, creating a 2D table of terminal performance scores.\n",
        "*   **Outputs:** A tuple of two `pandas.DataFrame`s:\n",
        "    1.  `learning_curves`: A DataFrame with a `MultiIndex` (`algorithm`, `episode`) containing the aggregated statistics for plotting.\n",
        "    2.  `terminal_performance`: A DataFrame of shape `(200, 7)` containing the stable performance measure for statistical testing.\n",
        "*   **Data Transformation:** This function transforms the raw, noisy, high-dimensional time-series data into two smaller, processed, and analysis-ready datasets. It is a classic data reduction and feature engineering step.\n",
        "*   **Role in Research Pipeline:** This callable is the **Data Processing and Feature Engineering** stage. It prepares the raw simulation output for analysis and visualization. The smoothing process is essential for creating the clear learning curves shown in Figure 1 of the paper. The terminal performance extraction creates the specific metric used for the statistical comparison shown in Figure 2.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 10: `perform_statistical_analysis`**\n",
        "\n",
        "*   **Inputs:** The `terminal_performance` DataFrame from Task 9 and the `study_params` dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Pairwise Comparison:** It iterates through all `7 * 7` ordered pairs of algorithms.\n",
        "    2.  **Hypothesis Testing:** For each pair `(A, B)`, it performs a one-sided Wilcoxon signed-rank test on the paired differences of their terminal performance scores to test the hypothesis that A outperforms B.\n",
        "    3.  **Effect Size Calculation:** As a rigorous enhancement, it also computes Cliff's Delta for each pair to quantify the magnitude of the performance difference.\n",
        "    4.  **Matrix Construction:** It assembles the results of these tests into two square `(7, 7)` DataFrames: one for p-values and one for effect sizes.\n",
        "*   **Outputs:** A tuple of two `pandas.DataFrame`s:\n",
        "    1.  `p_value_matrix`: The matrix of p-values for the one-sided tests.\n",
        "    2.  `effect_size_matrix`: The matrix of Cliff's Delta effect sizes.\n",
        "*   **Data Transformation:** This function transforms the 2D table of terminal performance scores into two new 2D matrices representing the results of statistical inference. It transforms performance data into evidence.\n",
        "*   **Role in Research Pipeline:** This callable is the **Inferential Statistics Engine**. Its sole purpose is to rigorously determine whether the observed differences in performance are statistically significant. It is the direct implementation of the \"one-sided Wilcoxon paired tests\" mentioned in the caption of Figure 2. It generates the core numerical data that is visualized in the paper's heatmap to support the central claim of the proposed method's superiority.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 11: `generate_all_visualizations_and_tables`**\n",
        "\n",
        "*   **Inputs:** The `learning_curves`, `p_value_matrix`, and `terminal_performance` DataFrames from the preceding tasks, along with the `study_params` dictionary and an `output_dir`.\n",
        "*   **Processes:**\n",
        "    1.  **Learning Curve Plotting:** It uses the `learning_curves` data to generate a plot with mean performance lines and shaded IQR bands for all algorithms, styled to match Figure 1.\n",
        "    2.  **Heatmap Plotting:** It uses the `p_value_matrix` to generate a heatmap with annotated p-values, styled to match Figure 2.\n",
        "    3.  **Summary Table Generation:** It uses the `terminal_performance` data to compute and format a table of descriptive statistics (mean, std, etc.) for each algorithm.\n",
        "    4.  **File Output:** It saves the generated plots as image files and the summary table as an HTML file.\n",
        "*   **Outputs:** This function has no return value (`None`). Its outputs are files saved to disk (e.g., `figure1_learning_curves.png`, `figure2_p_value_heatmap.png`).\n",
        "*   **Data Transformation:** This function transforms the final, processed DataFrames into human-readable visualizations and tables. It is the final presentation layer of the research pipeline.\n",
        "*   **Role in Research Pipeline:** This callable is the **Reporting and Visualization Engine**. It is responsible for creating the primary visual evidence presented in the paper. It directly generates the equivalents of Figure 1 (learning curves) and Figure 2 (heatmap), as well as any summary tables, thus translating the numerical results of the entire study into a communicable format.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 12 & 13 (Orchestrators): `run_alm_rl_reproduction_pipeline` & `run_robustness_analysis`**\n",
        "\n",
        "*   **Inputs:** The main `study_params` dictionary and execution parameters (e.g., `output_dir`, `num_workers`).\n",
        "*   **Processes:** These are the highest-level orchestrators. They do not perform calculations themselves but are responsible for calling the task-specific orchestrators (from Task 1, 2, 3, 8, 9, 10, 11) in the correct logical sequence. They manage the flow of data artifacts (DataFrames, arrays) from one task to the next. The robustness orchestrator additionally handles the logic of creating modified configurations for its analytical sweeps.\n",
        "*   **Outputs:** They return a dictionary of the final, key data artifacts for interactive inspection and save all plots and tables to disk.\n",
        "*   **Data Transformation:** They manage the transformation of the initial configuration dictionary into the final set of results and figures by orchestrating the entire chain of intermediate transformations.\n",
        "*   **Role in Research Pipeline:** These callables represent the **Master Experimental Script**. They define the complete, end-to-end workflow of the entire research project, from setup to final reporting. They ensure that the entire process is automated, reproducible, and executed in the correct logical order. They are the final, executable embodiment of the paper's entire empirical methodology.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "### **Granular Walkthrough: Using the End-to-End Pipeline**\n",
        "\n",
        "This example demonstrates the final step of the project: executing the entire research pipeline using the master orchestrator function `main`. The process involves two key stages: loading the configuration from the `config.yaml` file and then calling the `main` function with the loaded parameters.\n",
        "\n",
        "#### **Step 1: Loading the Configuration from `config.yaml`**\n",
        "\n",
        "Before any code can be executed, the experimental parameters must be loaded into memory. Storing configurations in a separate file like `config.yaml` is a critical best practice. It decouples the logic of the code from the specific parameters of an experiment, making the code reusable and the experiments easy to modify and track.\n",
        "\n",
        "*   **Prerequisite:** You must have a file named `config.yaml` in the same directory as your script or notebook. This file should contain the exact YAML content generated in the previous step.\n",
        "*   **Tool:** The `PyYAML` library is the standard and most robust tool for parsing YAML files in Python. It must be installed (`pip install pyyaml`).\n",
        "*   **Process:** We will write a small helper function, `load_config`, to open the YAML file, safely load its contents into a Python dictionary, and return it. This dictionary will be our `STUDY_INPUTS` object.\n",
        "\n",
        "```python\n",
        "# Python code snippet for loading the configuration\n",
        "import yaml\n",
        "from typing import Dict, Any\n",
        "\n",
        "def load_config(path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Loads the study configuration from a YAML file.\"\"\"\n",
        "    try:\n",
        "        with open(path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        print(f\"Configuration loaded successfully from '{path}'.\")\n",
        "        return config\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Configuration file not found at '{path}'.\")\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"ERROR: Failed to parse YAML file '{path}': {e}\")\n",
        "        raise\n",
        "\n",
        "# Load the configuration to create the STUDY_INPUTS dictionary\n",
        "STUDY_INPUTS = load_config('config.yaml')\n",
        "```\n",
        "\n",
        "#### **Step 2: Executing the Main Pipeline**\n",
        "\n",
        "With the `STUDY_INPUTS` dictionary loaded, we can now call the main orchestrator function. This single function call will trigger the entire sequence of 13 tasks that we have meticulously built and validated.\n",
        "\n",
        "*   **Function to Call:** `main`\n",
        "*   **Input Parameters:**\n",
        "    *   `study_params`: This is the `STUDY_INPUTS` dictionary we just loaded from `config.yaml`. It contains all the parameters needed for every step of the pipeline.\n",
        "    *   `run_reproduction`: A boolean flag. We will set this to `True` to execute the main experiment that reproduces the paper's core results (Tasks 1-12).\n",
        "    *   `run_robustness`: A boolean flag. We will set this to `True` to also execute the additional sensitivity and robustness analyses defined in Task 13.\n",
        "    *   `num_workers`: An integer specifying the number of CPU cores to use for parallel processing. A value of `8` is a reasonable choice for a modern multi-core machine, but this can be adjusted based on the available hardware.\n",
        "*   **Process:** The `main` function will internally manage the entire workflow. It will validate the configuration, generate seeds and market scenarios, run the 200 independent simulations in parallel for all 7 agents, process the raw results, perform statistical analysis, and generate the final plots and tables. It will then proceed to the robustness analyses, running several smaller-scale experiments.\n",
        "*   **Output:** The function will return a dictionary containing the final, processed `pandas.DataFrame` objects for the reproduction and robustness analyses. It will also save all generated figures, tables, and intermediate data to the specified output directories\n",
        "\n",
        "### **Complete Example**\n",
        "\n",
        "The following code block represents a complete, self-contained script that demonstrates how to use the entire pipeline. It assumes that all previously defined functions and classes (from `validate_study_inputs` to `run_robustness_analysis` and all agent/environment classes) are present in the same scope (e.g., in preceding cells of a Jupyter notebook or in an imported module).\n",
        "\n",
        "```python\n",
        "# =============================================================================\n",
        "# Main Execution Block for ALM-RL Reproduction and Analysis\n",
        "# =============================================================================\n",
        "\n",
        "# This script serves as the main entry point to execute the entire research\n",
        "# project. It demonstrates the standard workflow:\n",
        "# 1. Load the master configuration from an external YAML file.\n",
        "# 2. Call the main orchestrator function to run the pipelines.\n",
        "# 3. Handle any potential exceptions during execution.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # --- 1. Load Configuration ---\n",
        "    # Define the path to the configuration file.\n",
        "    CONFIG_FILE_PATH = 'config.yaml'\n",
        "    \n",
        "    # Define a simple helper function to load the YAML configuration.\n",
        "    def load_config(path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Safely loads the study configuration from a YAML file.\n",
        "\n",
        "        Args:\n",
        "            path (str): The file path to the YAML configuration file.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: The loaded configuration as a Python dictionary.\n",
        "        \n",
        "        Raises:\n",
        "            FileNotFoundError: If the configuration file does not exist.\n",
        "            yaml.YAMLError: If the file is not a valid YAML document.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Open the file in read mode.\n",
        "            with open(path, 'r') as f:\n",
        "                # Use yaml.safe_load for security against arbitrary code execution.\n",
        "                config = yaml.safe_load(f)\n",
        "            # Confirm successful loading.\n",
        "            print(f\"Configuration loaded successfully from '{path}'.\")\n",
        "            # Return the loaded dictionary.\n",
        "            return config\n",
        "        except FileNotFoundError:\n",
        "            # Provide a specific error message if the file is missing.\n",
        "            print(f\"FATAL ERROR: Configuration file not found at '{path}'.\")\n",
        "            # Re-raise the exception to halt execution.\n",
        "            raise\n",
        "        except yaml.YAMLError as e:\n",
        "            # Provide a specific error message for parsing failures.\n",
        "            print(f\"FATAL ERROR: Failed to parse YAML file '{path}': {e}\")\n",
        "            # Re-raise the exception.\n",
        "            raise\n",
        "\n",
        "    # --- 2. Execute the Main Pipeline ---\n",
        "    # Use a try-except block to gracefully handle any errors during the\n",
        "    # long-running pipeline execution.\n",
        "    try:\n",
        "        # Load the global STUDY_INPUTS dictionary from the YAML file.\n",
        "        STUDY_INPUTS = load_config(CONFIG_FILE_PATH)\n",
        "\n",
        "        # Call the main orchestrator function to run the entire project.\n",
        "        # This single function call triggers the sequence of all 13 tasks.\n",
        "        all_project_results = main(\n",
        "            # Pass the loaded configuration.\n",
        "            study_params=STUDY_INPUTS,\n",
        "            \n",
        "            # Set to True to run the main paper reproduction experiment.\n",
        "            run_reproduction=True,\n",
        "            \n",
        "            # Set to True to run the additional robustness and sensitivity tests.\n",
        "            run_robustness=True,\n",
        "            \n",
        "            # Specify the number of parallel workers. -1 uses (num_cores - 1).\n",
        "            # For a high-end machine, a specific number like 8 or 16 can be set.\n",
        "            num_workers=8\n",
        "        )\n",
        "\n",
        "        # --- 3. Final Confirmation ---\n",
        "        # If the pipeline completes without errors, print a final success message.\n",
        "        print(\"\\n\" + \"*\"*80)\n",
        "        print(\"ENTIRE PROJECT COMPLETED SUCCESSFULLY.\")\n",
        "        print(\"All results, figures, and tables have been generated.\")\n",
        "        print(\"*\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any part of the pipeline fails, this block will catch the error.\n",
        "        print(f\"\\nFATAL ERROR: The main pipeline execution failed.\")\n",
        "        # Print the specific exception for debugging purposes.\n",
        "        print(f\"Error details: {e}\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gG_a7erMIAM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Parameter Validation and Data Quality Assessment\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Utility Functions\n",
        "# =============================================================================\n",
        "\n",
        "def _get_nested_value(\n",
        "    data: Dict[str, Any],\n",
        "    path: Tuple[str, ...]\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Retrieves a value from a nested dictionary using a tuple path.\n",
        "\n",
        "    Args:\n",
        "        data (Dict[str, Any]): The nested dictionary to search within.\n",
        "        path (Tuple[str, ...]): A tuple of keys representing the path to the\n",
        "                                desired value.\n",
        "\n",
        "    Returns:\n",
        "        Any: The value found at the specified path.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If any key in the path does not exist.\n",
        "    \"\"\"\n",
        "    # Initialize a variable to hold the current level of the dictionary.\n",
        "    current_level = data\n",
        "    # Iterate through each key in the provided path.\n",
        "    for key in path:\n",
        "        # Check if the current level is a dictionary and contains the key.\n",
        "        if isinstance(current_level, dict) and key in current_level:\n",
        "            # Move to the next level in the dictionary.\n",
        "            current_level = current_level[key]\n",
        "        else:\n",
        "            # If a key is not found, raise a KeyError with a descriptive path.\n",
        "            raise KeyError(f\"Path '{'.'.join(path)}' not found. \"\n",
        "                           f\"Missing key '{key}' at '{'.'.join(path[:-1])}'.\")\n",
        "    # Return the final value found at the end of the path.\n",
        "    return current_level\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1, Step 1: Schema and Structure Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_dict_schema(\n",
        "    data: Dict[str, Any],\n",
        "    schema: Dict[str, Any],\n",
        "    path: Tuple[str, ...] = ()\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively validates a dictionary's structure, types, and literal values.\n",
        "\n",
        "    This function is a core component for ensuring the input configuration\n",
        "    matches the expected schema with high fidelity. It checks for missing keys,\n",
        "    type mismatches, and incorrect literal values.\n",
        "\n",
        "    Args:\n",
        "        data (Dict[str, Any]): The dictionary instance to validate.\n",
        "        schema (Dict[str, Any]): The schema dictionary to validate against.\n",
        "                                 Types are specified as type objects (e.g., int,\n",
        "                                 str, dict). Literal values are specified\n",
        "                                 directly.\n",
        "        path (Tuple[str, ...], optional): The current path within the nested\n",
        "                                          dictionary structure, used for\n",
        "                                          generating informative error messages.\n",
        "                                          Defaults to ().\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates successful\n",
        "                   validation.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to collect validation errors.\n",
        "    errors = []\n",
        "\n",
        "    # Iterate through each key and its expected value/type in the schema.\n",
        "    for key, expected in schema.items():\n",
        "        # Construct the full path for the current key for error reporting.\n",
        "        current_path_str = \".\".join(path + (key,))\n",
        "\n",
        "        # --- 1. Check for missing keys ---\n",
        "        # Verify that the key exists in the data dictionary.\n",
        "        if key not in data:\n",
        "            # If the key is missing, append a formatted error message.\n",
        "            errors.append(f\"Missing key: '{current_path_str}'\")\n",
        "            # Skip further checks for this key as it doesn't exist.\n",
        "            continue\n",
        "\n",
        "        # Retrieve the actual value from the data dictionary.\n",
        "        actual = data[key]\n",
        "\n",
        "        # --- 2. Handle nested dictionary validation recursively ---\n",
        "        # If the expected value in the schema is a dictionary, recurse.\n",
        "        if isinstance(expected, dict):\n",
        "            # The corresponding actual value must also be a dictionary.\n",
        "            if not isinstance(actual, dict):\n",
        "                # If not, report a type mismatch error.\n",
        "                errors.append(\n",
        "                    f\"Type mismatch at '{current_path_str}': \"\n",
        "                    f\"Expected dict, found {type(actual).__name__}.\"\n",
        "                )\n",
        "            else:\n",
        "                # If types match, recursively call the validation function.\n",
        "                errors.extend(\n",
        "                    _validate_dict_schema(\n",
        "                        actual, expected, path + (key,)\n",
        "                    )\n",
        "                )\n",
        "        # --- 3. Handle type validation ---\n",
        "        # If the expected value is a type object (e.g., int, str, float).\n",
        "        elif isinstance(expected, type):\n",
        "            # Check if the actual value's type matches the expected type.\n",
        "            if not isinstance(actual, expected):\n",
        "                # If not, report a type mismatch error.\n",
        "                errors.append(\n",
        "                    f\"Type mismatch at '{current_path_str}': \"\n",
        "                    f\"Expected {expected.__name__}, found {type(actual).__name__}.\"\n",
        "                )\n",
        "        # --- 4. Handle literal value validation ---\n",
        "        # If the expected value is a literal (not a type or dict).\n",
        "        else:\n",
        "            # Check if the actual value is identical to the expected literal.\n",
        "            if actual != expected:\n",
        "                # If not, report a value mismatch error.\n",
        "                errors.append(\n",
        "                    f\"Value mismatch at '{current_path_str}': \"\n",
        "                    f\"Expected '{expected}', found '{actual}'.\"\n",
        "                )\n",
        "\n",
        "    # Return the list of all collected errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_structural_and_mathematical_consistency(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates mathematical consistency between related parameters.\n",
        "\n",
        "    This function performs checks that span across different keys, ensuring\n",
        "    that derived values and relationships hold as specified.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The full STUDY_INPUTS configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages related to consistency checks.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to collect consistency errors.\n",
        "    errors = []\n",
        "    try:\n",
        "        # Retrieve time horizon T from the configuration.\n",
        "        T = _get_nested_value(config, (\"TIME_AND_PENALTIES\", \"T\"))\n",
        "        # Retrieve time step delta_t from the configuration.\n",
        "        delta_t = _get_nested_value(config, (\"TIME_AND_PENALTIES\", \"delta_t\"))\n",
        "        # Retrieve the number of steps K from the configuration.\n",
        "        K = _get_nested_value(config, (\"TIME_AND_PENALTIES\", \"K\"))\n",
        "\n",
        "        # --- Mathematical Consistency Check: K = floor(T / delta_t) ---\n",
        "        # Calculate the expected number of steps.\n",
        "        expected_K = math.floor(T / delta_t)\n",
        "        # Compare the configured K with the calculated expected_K.\n",
        "        if K != expected_K:\n",
        "            # If they do not match, append a detailed error message.\n",
        "            errors.append(\n",
        "                \"Mathematical inconsistency in 'TIME_AND_PENALTIES': \"\n",
        "                f\"K ({K}) does not match floor(T / delta_t) \"\n",
        "                f\"({expected_K} = floor({T} / {delta_t})).\"\n",
        "            )\n",
        "    except KeyError as e:\n",
        "        # If any required key is missing for this check, report it.\n",
        "        errors.append(f\"Cannot perform consistency check due to missing key: {e}\")\n",
        "\n",
        "    # Return the list of consistency errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _get_full_validation_schema() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Defines and returns the complete, deeply-nested validation schema.\n",
        "\n",
        "    This function serves as the single source of truth for the entire structure\n",
        "    of the `STUDY_INPUTS` configuration dictionary. It specifies every expected\n",
        "    key at every level, along with its expected data type (e.g., `int`, `dict`)\n",
        "    or its exact literal value for fixed parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The complete validation schema.\n",
        "    \"\"\"\n",
        "    # This schema is a direct, exhaustive representation of the STUDY_INPUTS structure.\n",
        "    schema = {\n",
        "        \"EXPERIMENT_META\": {\n",
        "            \"experiment_name\": str,\n",
        "            \"description\": str,\n",
        "            \"num_independent_runs\": 200,\n",
        "            \"num_episodes_per_run\": 20000,\n",
        "            \"rng_policy\": {\n",
        "                \"master_seed\": 42,\n",
        "                \"rng_impl\": str,\n",
        "                \"seed_domain\": list,\n",
        "                \"derivation_note\": str,\n",
        "            },\n",
        "        },\n",
        "        \"TIME_AND_PENALTIES\": {\n",
        "            \"T\": 1.0,\n",
        "            \"delta_t\": 0.01,\n",
        "            \"K\": 100,\n",
        "            \"Q\": float,\n",
        "            \"H\": float,\n",
        "            \"reward_definition\": dict,\n",
        "        },\n",
        "        \"ALM_RL_CONFIG\": {\n",
        "            \"sde_form\": str,\n",
        "            \"schedules\": {\n",
        "                \"learning_rate\": dict,\n",
        "                \"exploration_schedule\": dict,\n",
        "                \"temperature\": dict,\n",
        "            },\n",
        "            \"projection_bounds\": {\n",
        "                \"theta_max\": float,\n",
        "                \"phi1_max\": float,\n",
        "                \"phi2_min\": float,\n",
        "                \"phi2_max\": float,\n",
        "            },\n",
        "            \"critic_parameterization\": dict,\n",
        "            \"policy_parameterization\": dict,\n",
        "            \"initialization\": dict,\n",
        "        },\n",
        "        \"BASELINES_CONFIG\": {\n",
        "            \"dcppi\": dict,\n",
        "            \"acs\": dict,\n",
        "            \"mbp\": dict,\n",
        "        },\n",
        "        \"DEEP_RL_ARCH_AND_TRAINING\": {\n",
        "            \"architecture_template\": dict,\n",
        "            \"instances\": {\n",
        "                \"SAC\": dict,\n",
        "                \"PPO\": dict,\n",
        "                \"DDPG\": dict,\n",
        "            },\n",
        "            \"training_defaults\": dict,\n",
        "            \"sac\": dict,\n",
        "            \"ppo\": dict,\n",
        "            \"ddpg\": dict,\n",
        "        },\n",
        "        \"DEEP_RL_ENV_SPEC\": {\n",
        "            \"observation_space\": dict,\n",
        "            \"action_space\": dict,\n",
        "            \"dynamics\": dict,\n",
        "            \"reward\": dict,\n",
        "            \"episode_horizon\": int,\n",
        "            \"initial_state\": float,\n",
        "        },\n",
        "        \"EVALUATION_SETTINGS\": {\n",
        "            \"reward_aggregation\": dict,\n",
        "            \"smoothing\": dict,\n",
        "            \"significance_testing\": dict,\n",
        "        },\n",
        "        \"INITIAL_RAW_DATA\": {\n",
        "            \"market_params_table\": dict,\n",
        "            \"algorithm_seeds_table\": dict,\n",
        "            \"alm_rl_initial_table\": dict,\n",
        "            \"baselines_initial_table\": dict,\n",
        "            \"deep_rl_env_spec_template\": dict,\n",
        "        },\n",
        "    }\n",
        "    return schema\n",
        "\n",
        "def validate_step1_schema_and_structure(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs Step 1 validation: exhaustive schema, structure, and consistency.\n",
        "\n",
        "    This remediated version achieves a \"Perfect\" rating for completeness by\n",
        "    validating the input configuration against a fully specified, deeply-nested\n",
        "    schema. It immediately detects any structural error, such as a missing or\n",
        "    misspelled key at any level.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The STUDY_INPUTS configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A consolidated list of all validation errors from Step 1.\n",
        "                   An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # --- Step 2.1: Define the Complete Schema ---\n",
        "    # Retrieve the full, exhaustive schema from the helper function.\n",
        "    # This centralizes the structural definition of the entire configuration.\n",
        "    full_schema = _get_full_validation_schema()\n",
        "\n",
        "    # --- Step 2.2: Perform Exhaustive Schema Validation ---\n",
        "    # Use the existing recursive helper function to validate the input `config`\n",
        "    # against the `full_schema`. This will now check every key at every level.\n",
        "    schema_errors = _validate_dict_schema(config, full_schema)\n",
        "\n",
        "    # --- Perform Mathematical Consistency Validation ---\n",
        "    # This check remains separate as it validates a relationship between values,\n",
        "    # not just the structure or type of individual values.\n",
        "    consistency_errors = _validate_structural_and_mathematical_consistency(config)\n",
        "\n",
        "    # --- Aggregate and Return All Errors ---\n",
        "    # Combine the results from both structural and consistency checks.\n",
        "    return schema_errors + consistency_errors\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1, Step 2: Parameter Range and Constraint Validation\n",
        "# =============================================================================\n",
        "\n",
        "def validate_step2_ranges_and_constraints(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs Step 2 validation: parameter ranges and mathematical constraints.\n",
        "\n",
        "    This function iterates through a predefined list of checks, validating\n",
        "    numerical ranges, signs, and theoretical conditions like Robbins-Monro.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The STUDY_INPUTS configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of all validation errors from Step 2.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to collect validation errors.\n",
        "    errors = []\n",
        "    # Define a list of validation checks to be performed.\n",
        "    # Each check is a tuple: (path, validation_function, error_message).\n",
        "    # The validation function should return True if valid, False otherwise.\n",
        "    checks: List[Tuple[Tuple[str, ...], Callable[[Any], bool], str]] = [\n",
        "        # --- Robbins-Monro Conditions for learning rate exponent ---\n",
        "        (\n",
        "            (\"ALM_RL_CONFIG\", \"schedules\", \"learning_rate\", \"exponent\"),\n",
        "            lambda p: p == -0.75, # Analytically, p=0.75 satisfies the conditions.\n",
        "            \"Exponent must be -0.75 to satisfy Robbins-Monro conditions \"\n",
        "            \"(sum a_n diverges, sum a_n^2 converges).\"\n",
        "        ),\n",
        "        # --- Exploration Schedule Constraint ---\n",
        "        (\n",
        "            (\"ALM_RL_CONFIG\", \"schedules\", \"exploration_schedule\", \"exponent\"),\n",
        "            lambda p: p > 0,\n",
        "            \"Exploration schedule exponent must be positive to ensure b_n -> inf.\"\n",
        "        ),\n",
        "        # --- Projection Bounds Constraints ---\n",
        "        (\n",
        "            (\"ALM_RL_CONFIG\", \"projection_bounds\", \"theta_max\"),\n",
        "            lambda v: v > 0,\n",
        "            \"Projection bound 'theta_max' must be positive.\"\n",
        "        ),\n",
        "        (\n",
        "            (\"ALM_RL_CONFIG\", \"projection_bounds\", \"phi1_max\"),\n",
        "            lambda v: v > 0,\n",
        "            \"Projection bound 'phi1_max' must be positive.\"\n",
        "        ),\n",
        "        (\n",
        "            (\"ALM_RL_CONFIG\", \"projection_bounds\", \"phi2_min\"),\n",
        "            lambda v: v > 0,\n",
        "            \"Minimum exploration 'phi2_min' must be positive.\"\n",
        "        ),\n",
        "        (\n",
        "            (\"ALM_RL_CONFIG\", \"projection_bounds\", \"phi2_max\"),\n",
        "            lambda v: v > _get_nested_value(\n",
        "                config, (\"ALM_RL_CONFIG\", \"projection_bounds\", \"phi2_min\")\n",
        "            ),\n",
        "            \"Projection bound 'phi2_max' must be greater than 'phi2_min'.\"\n",
        "        ),\n",
        "        # --- Market Parameter Distribution Constraints ---\n",
        "        (\n",
        "            (\"INITIAL_RAW_DATA\", \"market_params_table\", \"generation\", \"distributions\", \"A\"),\n",
        "            lambda d: d[\"low\"] == -0.05 and d[\"high\"] == 0.05,\n",
        "            \"Market parameter A must be Uniform(-0.05, 0.05).\"\n",
        "        ),\n",
        "        (\n",
        "            (\"INITIAL_RAW_DATA\", \"market_params_table\", \"generation\", \"distributions\", \"B\"),\n",
        "            lambda d: d[\"low\"] == 0.05 and d[\"high\"] == 0.15 and d[\"low\"] > 0,\n",
        "            \"Market parameter B must be Uniform(0.05, 0.15) and strictly positive.\"\n",
        "        ),\n",
        "        (\n",
        "            (\"INITIAL_RAW_DATA\", \"market_params_table\", \"generation\", \"distributions\", \"C\"),\n",
        "            lambda d: d[\"low\"] == 0.1 and d[\"high\"] == 0.2,\n",
        "            \"Market parameter C must be Uniform(0.1, 0.2).\"\n",
        "        ),\n",
        "        (\n",
        "            (\"INITIAL_RAW_DATA\", \"market_params_table\", \"generation\", \"distributions\", \"D\"),\n",
        "            lambda d: d[\"low\"] == 0.1 and d[\"high\"] == 0.2 and d[\"low\"] > 0,\n",
        "            \"Market parameter D must be Uniform(0.1, 0.2) and strictly positive.\"\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # Iterate through each defined check.\n",
        "    for path, validator, message in checks:\n",
        "        try:\n",
        "            # Retrieve the value to be validated using its path.\n",
        "            value = _get_nested_value(config, path)\n",
        "            # Apply the validation function.\n",
        "            if not validator(value):\n",
        "                # If validation fails, format and append an error message.\n",
        "                errors.append(\n",
        "                    f\"Constraint violation at '{'.'.join(path)}': \"\n",
        "                    f\"Value '{value}' failed check. {message}\"\n",
        "                )\n",
        "        except (KeyError, TypeError) as e:\n",
        "            # If a key is missing or a value has the wrong type for a check,\n",
        "            # report the error.\n",
        "            errors.append(\n",
        "                f\"Cannot perform constraint check for '{'.'.join(path)}' \"\n",
        "                f\"due to error: {e}\"\n",
        "            )\n",
        "\n",
        "    # Return the list of all constraint violation errors.\n",
        "    return errors\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1, Step 3: Deep RL Architecture and Hyperparameter Validation\n",
        "# =============================================================================\n",
        "\n",
        "def validate_step3_deep_rl_specs(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs Step 3 validation: Deep RL architecture and hyperparameters.\n",
        "\n",
        "    This function performs detailed checks on the network dimensions and\n",
        "    training parameters for SAC, PPO, and DDPG to ensure they are consistent\n",
        "    with their respective algorithms and the problem statement.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The STUDY_INPUTS configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of all validation errors from Step 3.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to collect validation errors.\n",
        "    errors = []\n",
        "    try:\n",
        "        # --- Common Hyperparameter Checks ---\n",
        "        # Retrieve default training settings.\n",
        "        defaults = config[\"DEEP_RL_ARCH_AND_TRAINING\"][\"training_defaults\"]\n",
        "        # Check learning rates are within a sensible range.\n",
        "        if not 1e-5 < defaults[\"lr_actor\"] < 1e-2:\n",
        "            errors.append(\"Default 'lr_actor' is outside the typical range (1e-5, 1e-2).\")\n",
        "        if not 1e-5 < defaults[\"lr_critic\"] < 1e-2:\n",
        "            errors.append(\"Default 'lr_critic' is outside the typical range (1e-5, 1e-2).\")\n",
        "        # Check if batch size is a power of two.\n",
        "        batch_size = defaults[\"batch_size\"]\n",
        "        if not (batch_size > 0 and (batch_size & (batch_size - 1) == 0)):\n",
        "            errors.append(f\"Default 'batch_size' ({batch_size}) is not a power of two.\")\n",
        "\n",
        "        # --- SAC Specific Architecture Validation ---\n",
        "        # Retrieve SAC architecture specifications.\n",
        "        sac_arch = config[\"DEEP_RL_ARCH_AND_TRAINING\"][\"instances\"][\"SAC\"]\n",
        "        # Actor must output 2 values: mean and log_sigma for a Gaussian policy.\n",
        "        if sac_arch[\"actor\"][\"output_dim\"] != 2:\n",
        "            errors.append(\"SAC actor output_dim must be 2 (mean, log_sigma).\")\n",
        "        # Critic must take state (1D) and action (1D) as input, totaling 2D.\n",
        "        if sac_arch[\"critic1\"][\"input_dim\"] != 2:\n",
        "            errors.append(\"SAC critic1 input_dim must be 2 (state + action).\")\n",
        "\n",
        "        # --- PPO Specific Architecture Validation ---\n",
        "        # Retrieve PPO architecture specifications.\n",
        "        ppo_arch = config[\"DEEP_RL_ARCH_AND_TRAINING\"][\"instances\"][\"PPO\"]\n",
        "        # Actor must output 2 values: mean and log_sigma for a Gaussian policy.\n",
        "        if ppo_arch[\"actor\"][\"output_dim\"] != 2:\n",
        "            errors.append(\"PPO actor output_dim must be 2 (mean, log_sigma).\")\n",
        "        # Critic (Value function) takes only the state (1D) as input.\n",
        "        if ppo_arch[\"critic\"][\"input_dim\"] != 1:\n",
        "            errors.append(\"PPO critic input_dim must be 1 (state).\")\n",
        "\n",
        "        # --- DDPG Specific Architecture Validation ---\n",
        "        # Retrieve DDPG architecture specifications.\n",
        "        ddpg_arch = config[\"DEEP_RL_ARCH_AND_TRAINING\"][\"instances\"][\"DDPG\"]\n",
        "        # Actor outputs a single deterministic action.\n",
        "        if ddpg_arch[\"actor\"][\"output_dim\"] != 1:\n",
        "            errors.append(\"DDPG actor output_dim must be 1 (deterministic action).\")\n",
        "        # Critic takes state (1D) and action (1D) as input, totaling 2D.\n",
        "        if ddpg_arch[\"critic\"][\"input_dim\"] != 2:\n",
        "            errors.append(\"DDPG critic input_dim must be 2 (state + action).\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # If any required key is missing for these checks, report it.\n",
        "        errors.append(f\"Cannot perform Deep RL validation due to missing key: {e}\")\n",
        "\n",
        "    # Return the list of all Deep RL specification errors.\n",
        "    return errors\n",
        "\n",
        "# =============================================================================\n",
        "# Main Orchestrator for Task 1\n",
        "# =============================================================================\n",
        "\n",
        "def validate_study_inputs(\n",
        "    study_params: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the STUDY_INPUTS dictionary.\n",
        "\n",
        "    This function serves as the main entry point for Task 1. It executes all\n",
        "    three validation steps—schema/structure, ranges/constraints, and Deep RL\n",
        "    specifications—and aggregates the results. If any validation errors are\n",
        "    found, it raises a single, comprehensive ValueError.\n",
        "\n",
        "    Args:\n",
        "        study_params (Dict[str, Any]): The main configuration dictionary for\n",
        "                                       the entire study, conforming to the\n",
        "                                       STUDY_INPUTS structure.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation check fails. The error message contains\n",
        "                    a detailed, itemized list of all detected issues.\n",
        "        TypeError: If the input `study_params` is not a dictionary.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    # Ensure the primary input is a dictionary.\n",
        "    if not isinstance(study_params, dict):\n",
        "        # If not, raise a TypeError.\n",
        "        raise TypeError(\"`study_params` must be a dictionary.\")\n",
        "\n",
        "    # --- Execute All Validation Steps ---\n",
        "    # Step 1: Validate the high-level structure, types, and literal values.\n",
        "    step1_errors = validate_step1_schema_and_structure(study_params)\n",
        "    # Step 2: Validate numerical ranges and mathematical/theoretical constraints.\n",
        "    step2_errors = validate_step2_ranges_and_constraints(study_params)\n",
        "    # Step 3: Validate the detailed specifications for the Deep RL baselines.\n",
        "    step3_errors = validate_step3_deep_rl_specs(study_params)\n",
        "\n",
        "    # --- Aggregate and Report Errors ---\n",
        "    # Combine all errors from the three steps into a single list.\n",
        "    all_errors = step1_errors + step2_errors + step3_errors\n",
        "\n",
        "    # Check if any errors were found.\n",
        "    if all_errors:\n",
        "        # If there are errors, construct a comprehensive error message.\n",
        "        error_header = \"Input validation failed with the following errors:\"\n",
        "        # Format each error as a numbered list item for clarity.\n",
        "        formatted_errors = \"\\n\".join(\n",
        "            [f\"{i+1}. {e}\" for i, e in enumerate(all_errors)]\n",
        "        )\n",
        "        # Raise a single ValueError containing all the details.\n",
        "        raise ValueError(f\"{error_header}\\n{formatted_errors}\")\n",
        "\n",
        "    # If no errors are found, the function completes silently, indicating success.\n",
        "    # A print statement can be added for explicit confirmation in interactive use.\n",
        "    # print(\"STUDY_INPUTS validation successful.\")\n"
      ],
      "metadata": {
        "id": "RI_50UAjRKsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Computational Environment Setup and Random Number Generation Infrastructure\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2, Step 1: Deterministic Random Number Generation System\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_base_seeds(\n",
        "    master_seed: int,\n",
        "    num_runs: int,\n",
        "    seed_domain: Tuple[int, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Initializes a deterministic sequence of base seeds for independent runs.\n",
        "\n",
        "    This function establishes the root of reproducibility for the entire\n",
        "    experiment. Using a single master seed and the PCG64 pseudo-random number\n",
        "    generator, it produces a fixed, verifiable sequence of seeds. Each seed in\n",
        "    this sequence will correspond to one independent experimental run, ensuring\n",
        "    that the market conditions for run `i` are identical every time the\n",
        "    experiment is executed.\n",
        "\n",
        "    Args:\n",
        "        master_seed (int): The single, top-level seed for the entire\n",
        "                           reproducibility protocol.\n",
        "        num_runs (int): The total number of independent experimental runs,\n",
        "                        which corresponds to the number of base seeds to\n",
        "                        generate.\n",
        "        seed_domain (Tuple[int, int]): A tuple `(low, high)` defining the\n",
        "                                       inclusive range for the generated seeds.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of `dtype=np.int64` and shape `(num_runs,)`\n",
        "                    containing the deterministically generated base seeds.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `master_seed` or `num_runs` are not integers.\n",
        "        ValueError: If `num_runs` is not positive, or if the seed domain is\n",
        "                    invalid.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure master_seed is an integer for the RNG.\n",
        "    if not isinstance(master_seed, int):\n",
        "        raise TypeError(f\"master_seed must be an integer, but got {type(master_seed)}.\")\n",
        "    # Ensure num_runs is a positive integer.\n",
        "    if not isinstance(num_runs, int) or num_runs <= 0:\n",
        "        raise ValueError(f\"num_runs must be a positive integer, but got {num_runs}.\")\n",
        "    # Validate the seed domain format and values.\n",
        "    if not (isinstance(seed_domain, (list, tuple)) and len(seed_domain) == 2 and\n",
        "            all(isinstance(i, int) for i in seed_domain) and\n",
        "            seed_domain[0] < seed_domain[1]):\n",
        "        raise ValueError(f\"seed_domain must be a tuple of two integers (low, high) \"\n",
        "                         f\"with low < high, but got {seed_domain}.\")\n",
        "\n",
        "    # --- Seed Generation ---\n",
        "    # Instantiate the PCG64 bit generator with the master seed. This is the\n",
        "    # core of the deterministic process.\n",
        "    rng_bit_generator = np.random.PCG64(seed=master_seed)\n",
        "    # Create a high-level Generator object from the bit generator for a\n",
        "    # user-friendly and robust API.\n",
        "    master_rng = np.random.Generator(rng_bit_generator)\n",
        "\n",
        "    # Generate the array of base seeds. The parameters ensure the output\n",
        "    # conforms exactly to the study's specification (e.g., 32-bit integer domain).\n",
        "    base_seeds = master_rng.integers(\n",
        "        low=seed_domain[0],\n",
        "        high=seed_domain[1],\n",
        "        size=num_runs,\n",
        "        dtype=np.int64\n",
        "    )\n",
        "\n",
        "    # --- Self-Verification for Reproducibility ---\n",
        "    # To rigorously confirm determinism, regenerate the sequence and assert\n",
        "    # bitwise equality. This guards against unexpected behavior from the PRNG.\n",
        "    rng_bit_generator_verify = np.random.PCG64(seed=master_seed)\n",
        "    master_rng_verify = np.random.Generator(rng_bit_generator_verify)\n",
        "    base_seeds_verify = master_rng_verify.integers(\n",
        "        low=seed_domain[0],\n",
        "        high=seed_domain[1],\n",
        "        size=num_runs,\n",
        "        dtype=np.int64\n",
        "    )\n",
        "    # The core assertion for reproducibility.\n",
        "    assert np.array_equal(base_seeds, base_seeds_verify), \\\n",
        "        \"FATAL: PRNG determinism check failed. Seeds are not reproducible.\"\n",
        "\n",
        "    # Return the validated, deterministic array of base seeds.\n",
        "    return base_seeds\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2, Step 2: Algorithm-Specific Seed Derivation\n",
        "# =============================================================================\n",
        "\n",
        "def derive_algorithm_seeds(\n",
        "    base_seeds: np.ndarray,\n",
        "    algorithm_names: List[str],\n",
        "    seed_domain: Tuple[int, int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Derives independent, deterministic seeds for each algorithm within each run.\n",
        "\n",
        "    This function is critical for ensuring fair comparison. For a given run\n",
        "    (and its `base_seed`), each algorithm receives its own unique seed. This\n",
        "    isolates the stochastic components of the algorithms (e.g., policy\n",
        "    sampling, parameter initialization) from each other, preventing spurious\n",
        "    correlations while maintaining perfect reproducibility. The derivation uses\n",
        "    a cryptographic hash function to ensure statistical independence of the\n",
        "    resulting seed streams.\n",
        "\n",
        "    Args:\n",
        "        base_seeds (np.ndarray): The array of base seeds, one for each run,\n",
        "                                 as generated by `initialize_base_seeds`.\n",
        "        algorithm_names (List[str]): A list of unique string identifiers for\n",
        "                                     each algorithm being tested.\n",
        "        seed_domain (Tuple[int, int]): A tuple `(low, high)` defining the\n",
        "                                       range for the derived seeds.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with columns `['run_id', 'base_seed',\n",
        "                      'seed_<alg1>', 'seed_<alg2>', ...]`, providing a complete\n",
        "                      map of seeds for the entire experiment.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs have incorrect types.\n",
        "        ValueError: If `base_seeds` is empty or `algorithm_names` contains\n",
        "                    duplicates.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(base_seeds, np.ndarray):\n",
        "        raise TypeError(f\"base_seeds must be a numpy array, but got {type(base_seeds)}.\")\n",
        "    if base_seeds.ndim != 1 or base_seeds.size == 0:\n",
        "        raise ValueError(\"base_seeds must be a non-empty 1D array.\")\n",
        "    if not (isinstance(algorithm_names, list) and\n",
        "            all(isinstance(name, str) for name in algorithm_names)):\n",
        "        raise TypeError(\"algorithm_names must be a list of strings.\")\n",
        "    if len(algorithm_names) != len(set(algorithm_names)):\n",
        "        raise ValueError(\"algorithm_names must not contain duplicates.\")\n",
        "\n",
        "    # --- Seed Derivation Logic ---\n",
        "    # Prepare a list to hold the data for each run, which will be converted\n",
        "    # to a DataFrame.\n",
        "    all_run_data = []\n",
        "    # Define the upper bound for the modulo operation from the seed domain.\n",
        "    domain_size = seed_domain[1]\n",
        "\n",
        "    # Iterate through each base seed with its corresponding run_id.\n",
        "    for run_id, base_seed in enumerate(base_seeds):\n",
        "        # Create a dictionary to store the seeds for the current run.\n",
        "        run_record = {\"run_id\": run_id, \"base_seed\": base_seed}\n",
        "        # A set to verify uniqueness of derived seeds within this run.\n",
        "        derived_seeds_in_run = set()\n",
        "\n",
        "        # Iterate through each algorithm name to derive its specific seed.\n",
        "        for name in algorithm_names:\n",
        "            # Create a unique, salted input string for the hash function.\n",
        "            # Using a separator prevents ambiguity (e.g., seed 12 + name '3'\n",
        "            # vs. seed 1 + name '23').\n",
        "            salt = f\"{base_seed}:{name}\".encode('utf-8')\n",
        "\n",
        "            # Use SHA-256 for a strong, deterministic hash.\n",
        "            hasher = hashlib.sha256(salt)\n",
        "            # Take the first 4 bytes (32 bits) of the digest for our seed.\n",
        "            hash_bytes = hasher.digest()[:4]\n",
        "\n",
        "            # Convert the bytes to an integer. 'big' endian is a standard choice.\n",
        "            seed_int = int.from_bytes(hash_bytes, 'big')\n",
        "\n",
        "            # Map the integer into the desired positive integer domain using modulo.\n",
        "            # This ensures the seed is within the valid range [0, 2**31 - 1].\n",
        "            derived_seed = seed_int % domain_size\n",
        "\n",
        "            # Store the derived seed in the record for this run.\n",
        "            run_record[f\"seed_{name.lower()}\"] = derived_seed\n",
        "            derived_seeds_in_run.add(derived_seed)\n",
        "\n",
        "        # --- Intra-run Uniqueness Validation ---\n",
        "        # This check is statistically unlikely to fail but is a crucial safeguard.\n",
        "        if len(derived_seeds_in_run) != len(algorithm_names):\n",
        "            raise RuntimeError(f\"Seed collision detected in run {run_id}. \"\n",
        "                               \"This is highly improbable. Check the hashing logic.\")\n",
        "\n",
        "        # Append the completed record for the run to our list.\n",
        "        all_run_data.append(run_record)\n",
        "\n",
        "    # --- DataFrame Creation ---\n",
        "    # Convert the list of dictionaries into a pandas DataFrame.\n",
        "    seed_table = pd.DataFrame(all_run_data)\n",
        "    # Set the run_id as the index for clear, unambiguous referencing.\n",
        "    seed_table = seed_table.set_index(\"run_id\")\n",
        "\n",
        "    # Return the final, structured table of all experimental seeds.\n",
        "    return seed_table\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2, Step 3: Computational Resource Estimation\n",
        "# =============================================================================\n",
        "\n",
        "def estimate_computational_requirements(\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes the configuration to estimate computational and memory load.\n",
        "\n",
        "    This function serves as a planning tool. Before launching a potentially\n",
        "    long-running experiment, it calculates key metrics based on the study\n",
        "    parameters. It provides estimates for total computational load (in\n",
        "    episodes) and memory requirements under both a naive (storing all data)\n",
        "    and an optimized (storing only results) strategy.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The full STUDY_INPUTS configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing a structured report of the\n",
        "                        estimated requirements and recommendations.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(f\"config must be a dictionary, but got {type(config)}.\")\n",
        "\n",
        "    # --- Parameter Extraction ---\n",
        "    try:\n",
        "        # Extract necessary parameters using the validated config structure.\n",
        "        num_runs = config[\"EXPERIMENT_META\"][\"num_independent_runs\"]\n",
        "        num_episodes = config[\"EXPERIMENT_META\"][\"num_episodes_per_run\"]\n",
        "        K = config[\"TIME_AND_PENALTIES\"][\"K\"]\n",
        "        # The number of algorithms is determined by the number of seed columns.\n",
        "        num_algorithms = len(config[\"INITIAL_RAW_DATA\"][\"algorithm_seeds_table\"][\"columns\"]) - 2\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Raise an error if the config is missing required keys for estimation.\n",
        "        raise ValueError(f\"Configuration is missing a required key for estimation: {e}\")\n",
        "\n",
        "    # --- Computational Load Calculation ---\n",
        "    # Total episodes is the primary hardware-agnostic measure of workload.\n",
        "    total_episodes = num_runs * num_algorithms * num_episodes\n",
        "\n",
        "    # --- Memory Requirement Calculation ---\n",
        "    # Define constants for memory calculation.\n",
        "    BYTES_PER_FLOAT64 = 8\n",
        "    VARS_PER_TIMESTEP = 3  # (state, action, reward)\n",
        "    BYTES_PER_GB = 1024**3\n",
        "    BYTES_PER_MB = 1024**2\n",
        "\n",
        "    # 1. Naive Strategy: Store all trajectory data simultaneously.\n",
        "    # This represents the worst-case peak memory if not managed carefully.\n",
        "    bytes_per_trajectory = K * VARS_PER_TIMESTEP * BYTES_PER_FLOAT64\n",
        "    total_trajectories = num_runs * num_algorithms * num_episodes\n",
        "    peak_memory_naive_gb = (bytes_per_trajectory * total_trajectories) / BYTES_PER_GB\n",
        "\n",
        "    # 2. Optimized Strategy: Store only the final episode rewards.\n",
        "    # This is the memory footprint of the final results object.\n",
        "    total_rewards_to_store = num_runs * num_algorithms * num_episodes\n",
        "    optimized_memory_results_mb = (total_rewards_to_store * BYTES_PER_FLOAT64) / BYTES_PER_MB\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile the results into a structured dictionary for clear output.\n",
        "    report = {\n",
        "        \"computational_load\": {\n",
        "            \"total_runs\": num_runs,\n",
        "            \"algorithms_per_run\": num_algorithms,\n",
        "            \"episodes_per_run\": num_episodes,\n",
        "            \"total_episodes_to_simulate\": f\"{total_episodes:,}\"\n",
        "        },\n",
        "        \"memory_estimation\": {\n",
        "            \"peak_memory_gb_naive_strategy\": round(peak_memory_naive_gb, 2),\n",
        "            \"optimized_memory_mb_results_only\": round(optimized_memory_results_mb, 2),\n",
        "            \"recommendation\": (\n",
        "                \"A streaming approach (processing one run at a time and storing \"\n",
        "                \"only final rewards) is required to avoid excessive memory usage. \"\n",
        "                \"The peak memory will be dominated by the deep RL replay buffers \"\n",
        "                \"and the results array.\"\n",
        "            )\n",
        "        },\n",
        "        \"runtime_estimation\": {\n",
        "            \"note\": (\n",
        "                \"Runtime is highly hardware-dependent. The 'total_episodes_to_simulate' \"\n",
        "                \"is the best proxy for computational cost. Parallel execution across \"\n",
        "                \"independent runs is recommended.\"\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "    return report\n",
        "\n",
        "# =============================================================================\n",
        "# Orchestrator Function for Task 2\n",
        "# =============================================================================\n",
        "\n",
        "def setup_computation_and_rng(\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the setup of the computational environment and RNG system.\n",
        "\n",
        "    This function executes all steps of Task 2 in sequence:\n",
        "    1. Generates the deterministic base seeds from the master seed.\n",
        "    2. Derives the algorithm-specific seeds for stochastic isolation.\n",
        "    3. Estimates the computational and memory requirements for planning.\n",
        "\n",
        "    It returns the critical data structures needed for subsequent tasks.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The full, validated STUDY_INPUTS dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            - A pandas DataFrame containing the complete seed map for all runs\n",
        "              and algorithms.\n",
        "            - A dictionary containing the computational resource estimation report.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(f\"config must be a dictionary, but got {type(config)}.\")\n",
        "\n",
        "    # --- Step 1: Initialize Deterministic Base Seeds ---\n",
        "    # Extract parameters required for base seed generation.\n",
        "    master_seed = config[\"EXPERIMENT_META\"][\"rng_policy\"][\"master_seed\"]\n",
        "    num_runs = config[\"EXPERIMENT_META\"][\"num_independent_runs\"]\n",
        "    seed_domain = tuple(config[\"EXPERIMENT_META\"][\"rng_policy\"][\"seed_domain\"])\n",
        "    # Generate the base seeds.\n",
        "    base_seeds = initialize_base_seeds(master_seed, num_runs, seed_domain)\n",
        "\n",
        "    # --- Step 2: Derive Algorithm-Specific Seeds ---\n",
        "    # Extract the list of algorithm names for which to generate seeds.\n",
        "    # This is robustly derived from the schema definition.\n",
        "    alg_seed_cols = config[\"INITIAL_RAW_DATA\"][\"algorithm_seeds_table\"][\"columns\"]\n",
        "    alg_names = [\n",
        "        col[\"name\"].replace(\"seed_\", \"\").upper() for col in alg_seed_cols\n",
        "        if col[\"name\"].startswith(\"seed_\")\n",
        "    ]\n",
        "    # Derive the full seed table.\n",
        "    seed_table = derive_algorithm_seeds(base_seeds, alg_names, seed_domain)\n",
        "\n",
        "    # --- Step 3: Estimate Computational Requirements ---\n",
        "    # Generate the resource estimation report.\n",
        "    resource_report = estimate_computational_requirements(config)\n",
        "\n",
        "    # Return the generated data structures.\n",
        "    return seed_table, resource_report\n"
      ],
      "metadata": {
        "id": "gSJq5_WkRdSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Market Parameter Generation and Data Structure Initialization\n",
        "\n",
        "# =============================================================================\n",
        "# Task 3, Step 1: Market Parameter Generation\n",
        "# =============================================================================\n",
        "\n",
        "def generate_market_parameters(\n",
        "    seed_table: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the randomized market parameters for each independent run.\n",
        "\n",
        "    For each of the 200 runs, this function creates a unique market scenario\n",
        "    defined by the SDE parameters (A, B, C, D). It uses the `base_seed` for\n",
        "    each run to ensure that the market environment is deterministically generated\n",
        "    and identical for all algorithms competing within that run. The function also\n",
        "    calculates and appends the theoretical \"oracle\" optimal policy gain, which\n",
        "    is crucial for validating the convergence of the ALM-RL agent later.\n",
        "\n",
        "    Args:\n",
        "        seed_table (pd.DataFrame): The DataFrame containing `base_seed` for\n",
        "                                   each run, indexed by `run_id`.\n",
        "        config (Dict[str, Any]): The full STUDY_INPUTS configuration, from which\n",
        "                                 the distribution ranges are extracted.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by `run_id` with columns\n",
        "                      ['base_seed', 'A', 'B', 'C', 'D', 'phi1_star'],\n",
        "                      representing the unique market conditions for each run.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the configuration is missing required distribution keys.\n",
        "        RuntimeError: If generated parameters violate constraints (e.g., D=0).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(seed_table, pd.DataFrame) or \"base_seed\" not in seed_table.columns:\n",
        "        raise TypeError(\"`seed_table` must be a DataFrame with a 'base_seed' column.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"`config` must be a dictionary.\")\n",
        "\n",
        "    try:\n",
        "        # Extract distribution specifications from the configuration.\n",
        "        dist_specs = config[\"INITIAL_RAW_DATA\"][\"market_params_table\"][\"generation\"][\"distributions\"]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Configuration is missing market parameter distribution specs: {e}\")\n",
        "\n",
        "    # --- Parameter Generation ---\n",
        "    # Prepare a list to hold the dictionary record for each run.\n",
        "    market_data = []\n",
        "    # Iterate through each run defined in the seed table's index.\n",
        "    for run_id in seed_table.index:\n",
        "        # Retrieve the unique base seed for this specific run.\n",
        "        base_seed = seed_table.loc[run_id, \"base_seed\"]\n",
        "        # Instantiate a dedicated RNG for this run to ensure stochastic isolation.\n",
        "        rng = np.random.Generator(np.random.PCG64(seed=int(base_seed)))\n",
        "\n",
        "        # Sample each SDE parameter from its specified uniform distribution.\n",
        "        # dx(t) = (A*x(t) + B*u(t))dt + (C*x(t) + D*u(t))dW(t)\n",
        "        A = rng.uniform(low=dist_specs[\"A\"][\"low\"], high=dist_specs[\"A\"][\"high\"])\n",
        "        B = rng.uniform(low=dist_specs[\"B\"][\"low\"], high=dist_specs[\"B\"][\"high\"])\n",
        "        C = rng.uniform(low=dist_specs[\"C\"][\"low\"], high=dist_specs[\"C\"][\"high\"])\n",
        "        D = rng.uniform(low=dist_specs[\"D\"][\"low\"], high=dist_specs[\"D\"][\"high\"])\n",
        "\n",
        "        # --- Post-generation Validation ---\n",
        "        # Ensure the SDE is well-posed. These conditions are guaranteed by the\n",
        "        # specified ranges but are asserted here for rigor.\n",
        "        if not (B > 0 and D > 0):\n",
        "            raise RuntimeError(f\"Generated parameters for run {run_id} are invalid: \"\n",
        "                               f\"B={B}, D={D}. Both must be positive.\")\n",
        "\n",
        "        # --- Oracle Policy Calculation ---\n",
        "        # Calculate the theoretical optimal policy gain for a known market.\n",
        "        # phi1* = -(B + C*D) / D^2\n",
        "        # This is a critical benchmark for evaluating the ALM-RL agent's learning.\n",
        "        phi1_star = -(B + C * D) / (D**2)\n",
        "\n",
        "        # Append the complete record for this run.\n",
        "        market_data.append({\n",
        "            \"run_id\": run_id,\n",
        "            \"base_seed\": base_seed,\n",
        "            \"A\": A, \"B\": B, \"C\": C, \"D\": D,\n",
        "            \"phi1_star\": phi1_star\n",
        "        })\n",
        "\n",
        "    # --- DataFrame Creation ---\n",
        "    # Convert the list of records into a structured DataFrame.\n",
        "    market_params_table = pd.DataFrame(market_data)\n",
        "    # Set run_id as the index for efficient lookups.\n",
        "    market_params_table = market_params_table.set_index(\"run_id\")\n",
        "\n",
        "    return market_params_table\n",
        "\n",
        "# =============================================================================\n",
        "# Task 3, Step 2: ALM-RL Initial Parameter Generation\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_alm_rl_parameters(\n",
        "    seed_table: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the initial parameter states for the ALM-RL agent for each run.\n",
        "\n",
        "    For each of the 200 runs, this function sets the starting values for the\n",
        "    agent's learnable parameters (theta1, theta2, phi1, phi2). The initialization\n",
        "    is stochastic (sampled from a Normal distribution) but reproducible, as it\n",
        "    uses the dedicated `seed_alm_rl` for each run. This ensures that while each\n",
        "    run starts from a different random point, the starting point for a given\n",
        "    run is always the same. Parameters are projected to predefined bounds to\n",
        "    ensure stability from the outset.\n",
        "\n",
        "    Args:\n",
        "        seed_table (pd.DataFrame): The DataFrame containing `seed_alm_rl` for\n",
        "                                   each run, indexed by `run_id`.\n",
        "        config (Dict[str, Any]): The full STUDY_INPUTS configuration, used for\n",
        "                                 initialization and projection bound values.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by `run_id` with columns\n",
        "                      ['x0', 'theta1_0', 'theta2_0', 'phi1_0', 'phi2_0'],\n",
        "                      defining the agent's starting state for each run.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(seed_table, pd.DataFrame) or \"seed_alm_rl\" not in seed_table.columns:\n",
        "        raise TypeError(\"`seed_table` must be a DataFrame with a 'seed_alm_rl' column.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"`config` must be a dictionary.\")\n",
        "\n",
        "    # --- Parameter Extraction ---\n",
        "    try:\n",
        "        # Extract initialization and projection settings from the config.\n",
        "        init_policy = config[\"ALM_RL_CONFIG\"][\"initialization\"]\n",
        "        bounds = config[\"ALM_RL_CONFIG\"][\"projection_bounds\"]\n",
        "        theta_max = bounds[\"theta_max\"]\n",
        "        phi1_max = bounds[\"phi1_max\"]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Configuration is missing ALM-RL initialization specs: {e}\")\n",
        "\n",
        "    # --- Initialization Logic ---\n",
        "    # Prepare a list to hold the initial parameter records.\n",
        "    initial_params_data = []\n",
        "    # Iterate through each run defined in the seed table.\n",
        "    for run_id in seed_table.index:\n",
        "        # Retrieve the unique seed for the ALM-RL agent in this run.\n",
        "        agent_seed = seed_table.loc[run_id, \"seed_alm_rl\"]\n",
        "        # Instantiate a dedicated RNG for this agent's initialization.\n",
        "        rng = np.random.Generator(np.random.PCG64(seed=int(agent_seed)))\n",
        "\n",
        "        # Sample initial critic parameters from a standard normal distribution.\n",
        "        # theta ~ N(0, 1)\n",
        "        theta1_sampled = rng.standard_normal()\n",
        "        theta2_sampled = rng.standard_normal()\n",
        "\n",
        "        # Sample initial policy mean parameter from a standard normal distribution.\n",
        "        # phi1 ~ N(0, 1)\n",
        "        phi1_sampled = rng.standard_normal()\n",
        "\n",
        "        # Apply projection (clipping) to the sampled values to enforce bounds.\n",
        "        # theta_0 = Π_[-100, 100](theta_sampled)\n",
        "        theta1_0 = np.clip(theta1_sampled, -theta_max, theta_max)\n",
        "        theta2_0 = np.clip(theta2_sampled, -theta_max, theta_max)\n",
        "        # phi1_0 = Π_[-100, 100](phi1_sampled)\n",
        "        phi1_0 = np.clip(phi1_sampled, -phi1_max, phi1_max)\n",
        "\n",
        "        # Set fixed initial values as per the specification.\n",
        "        x0 = init_policy[\"x0\"]\n",
        "        phi2_0 = init_policy[\"phi2_value\"]\n",
        "\n",
        "        # Append the complete record of initial parameters for this run.\n",
        "        initial_params_data.append({\n",
        "            \"run_id\": run_id,\n",
        "            \"x0\": x0,\n",
        "            \"theta1_0\": theta1_0,\n",
        "            \"theta2_0\": theta2_0,\n",
        "            \"phi1_0\": phi1_0,\n",
        "            \"phi2_0\": phi2_0\n",
        "        })\n",
        "\n",
        "    # --- DataFrame Creation and Validation ---\n",
        "    # Convert the list of records into a structured DataFrame.\n",
        "    alm_rl_initial_table = pd.DataFrame(initial_params_data).set_index(\"run_id\")\n",
        "\n",
        "    # Rigorous post-generation validation to ensure all values are within bounds.\n",
        "    assert (alm_rl_initial_table['theta1_0'].abs() <= theta_max).all()\n",
        "    assert (alm_rl_initial_table['theta2_0'].abs() <= theta_max).all()\n",
        "    assert (alm_rl_initial_table['phi1_0'].abs() <= phi1_max).all()\n",
        "    assert (alm_rl_initial_table['phi2_0'] == init_policy[\"phi2_value\"]).all()\n",
        "\n",
        "    return alm_rl_initial_table\n",
        "\n",
        "# =============================================================================\n",
        "# Task 3, Step 3: Baseline Algorithm Initial Parameter Generation\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_baseline_parameters(\n",
        "    seed_table: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the initial parameter states for the baseline algorithms.\n",
        "\n",
        "    This function initializes the starting parameters for the traditional\n",
        "    baseline models (DCPPI and ACS). Similar to the ALM-RL initialization,\n",
        "    it uses dedicated, algorithm-specific seeds (`seed_dcppi`, `seed_acs`)\n",
        "    for each run to ensure reproducible, stochastic starting points that are\n",
        "    isolated from all other random processes in the experiment.\n",
        "\n",
        "    Args:\n",
        "        seed_table (pd.DataFrame): The DataFrame containing `seed_dcppi` and\n",
        "                                   `seed_acs` for each run.\n",
        "        config (Dict[str, Any]): The full STUDY_INPUTS configuration, used for\n",
        "                                 ACS tolerance parameter.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by `run_id` with columns\n",
        "                      ['m0_dcppi', 'm0_acs', 'tolerance_delta'], defining the\n",
        "                      baselines' starting state for each run.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_cols = [\"seed_dcppi\", \"seed_acs\"]\n",
        "    if not all(col in seed_table.columns for col in required_cols):\n",
        "        raise TypeError(f\"`seed_table` must contain columns: {required_cols}.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"`config` must be a dictionary.\")\n",
        "\n",
        "    # --- Parameter Extraction ---\n",
        "    try:\n",
        "        # Extract the fixed tolerance parameter for the ACS algorithm.\n",
        "        tolerance_delta = config[\"BASELINES_CONFIG\"][\"acs\"][\"tolerance_delta\"]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Configuration is missing baseline specs: {e}\")\n",
        "\n",
        "    # --- Initialization Logic ---\n",
        "    # Prepare a list to hold the initial parameter records.\n",
        "    initial_params_data = []\n",
        "    # Iterate through each run defined in the seed table.\n",
        "    for run_id in seed_table.index:\n",
        "        # --- DCPPI Initialization ---\n",
        "        # Retrieve the unique seed for the DCPPI agent in this run.\n",
        "        dcppi_seed = seed_table.loc[run_id, \"seed_dcppi\"]\n",
        "        # Instantiate a dedicated RNG for DCPPI's initialization.\n",
        "        rng_dcppi = np.random.Generator(np.random.PCG64(seed=int(dcppi_seed)))\n",
        "        # Sample the initial multiplier from a standard normal distribution.\n",
        "        # m0_dcppi ~ N(0, 1)\n",
        "        m0_dcppi = rng_dcppi.standard_normal()\n",
        "\n",
        "        # --- ACS Initialization ---\n",
        "        # Retrieve the unique seed for the ACS agent in this run.\n",
        "        acs_seed = seed_table.loc[run_id, \"seed_acs\"]\n",
        "        # Instantiate a dedicated RNG for ACS's initialization.\n",
        "        rng_acs = np.random.Generator(np.random.PCG64(seed=int(acs_seed)))\n",
        "        # Sample the initial multiplier from a standard normal distribution.\n",
        "        # m0_acs ~ N(0, 1)\n",
        "        m0_acs = rng_acs.standard_normal()\n",
        "\n",
        "        # Append the complete record for this run.\n",
        "        initial_params_data.append({\n",
        "            \"run_id\": run_id,\n",
        "            \"m0_dcppi\": m0_dcppi,\n",
        "            \"m0_acs\": m0_acs,\n",
        "            \"tolerance_delta\": tolerance_delta\n",
        "        })\n",
        "\n",
        "    # --- DataFrame Creation ---\n",
        "    # Convert the list of records into a structured DataFrame.\n",
        "    baselines_initial_table = pd.DataFrame(initial_params_data).set_index(\"run_id\")\n",
        "\n",
        "    return baselines_initial_table\n",
        "\n",
        "# =============================================================================\n",
        "# Orchestrator Function for Task 3\n",
        "# =============================================================================\n",
        "\n",
        "def generate_initial_conditions(\n",
        "    seed_table: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all initial data structures for the experiment.\n",
        "\n",
        "    This function serves as the main entry point for Task 3. It executes all\n",
        "    three data generation steps in sequence, producing the foundational tables\n",
        "    required to run the simulations:\n",
        "    1. The market parameters for each of the 200 scenarios.\n",
        "    2. The initial parameters for the 200 instances of the ALM-RL agent.\n",
        "    3. The initial parameters for the 200 instances of the baseline agents.\n",
        "\n",
        "    Args:\n",
        "        seed_table (pd.DataFrame): The complete seed map from Task 2.\n",
        "        config (Dict[str, Any]): The full, validated STUDY_INPUTS dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "            - market_params_table: SDE parameters for each run.\n",
        "            - alm_rl_initial_table: Initial agent parameters for ALM-RL.\n",
        "            - baselines_initial_table: Initial agent parameters for baselines.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(seed_table, pd.DataFrame) or not isinstance(config, dict):\n",
        "        raise TypeError(\"Invalid input types for `seed_table` or `config`.\")\n",
        "\n",
        "    # --- Step 1: Generate Market Parameters ---\n",
        "    market_params_table = generate_market_parameters(seed_table, config)\n",
        "\n",
        "    # --- Step 2: Initialize ALM-RL Parameters ---\n",
        "    alm_rl_initial_table = initialize_alm_rl_parameters(seed_table, config)\n",
        "\n",
        "    # --- Step 3: Initialize Baseline Parameters ---\n",
        "    baselines_initial_table = initialize_baseline_parameters(seed_table, config)\n",
        "\n",
        "    # Return the three generated DataFrames.\n",
        "    return market_params_table, alm_rl_initial_table, baselines_initial_table\n"
      ],
      "metadata": {
        "id": "CIBOZiGpSijg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: ALM-RL Algorithm Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# ALM-RL Agent Implementation (Task 4)\n",
        "# =============================================================================\n",
        "\n",
        "class ALM_RL_Agent:\n",
        "    \"\"\"\n",
        "    Implements the continuous-time Soft Actor-Critic agent for ALM.\n",
        "\n",
        "    This class encapsulates the entire logic for the proposed ALM-RL algorithm,\n",
        "    including its parameterization, learning schedules, policy, value function,\n",
        "    and update rules, as described in the research paper. It is designed to be\n",
        "    stateful, managing its own parameters and learning progress across episodes.\n",
        "\n",
        "    The agent's core function is to learn an optimal control policy for managing\n",
        "    asset-liability surplus deviation by interacting with a stochastic\n",
        "    environment. It uses a policy-gradient-based, model-free approach with\n",
        "    entropy regularization to balance exploration and exploitation.\n",
        "\n",
        "    Attributes:\n",
        "        theta (np.ndarray): The critic's parameters [theta1, theta2], representing\n",
        "                            the coefficients of the quadratic value function.\n",
        "                            Shape: (2,).\n",
        "        phi (np.ndarray): The actor's parameters [phi1, phi2], representing the\n",
        "                          linear coefficient of the policy mean and the policy\n",
        "                          variance. Shape: (2,).\n",
        "        config (Dict[str, Any]): A dictionary containing all necessary\n",
        "                                 hyperparameters and settings for the agent,\n",
        "                                 sourced from the main study configuration.\n",
        "        rng (np.random.Generator): The agent's dedicated random number\n",
        "                                   generator for stochastic processes like\n",
        "                                   action selection, ensuring reproducibility.\n",
        "        episode_counter (int): Tracks the number of episodes completed, used\n",
        "                               to index into the learning schedules.\n",
        "        learning_rates (np.ndarray): A pre-computed array of learning rates (alpha)\n",
        "                                     for each episode. Shape: (num_episodes,).\n",
        "        temperatures (np.ndarray): A pre-computed array of entropy temperatures\n",
        "                                   (gamma) for each episode. Shape: (num_episodes,).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_params: Dict[str, float],\n",
        "        config: Dict[str, Any],\n",
        "        num_episodes: int,\n",
        "        seed: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the ALM_RL_Agent instance.\n",
        "\n",
        "        This constructor sets up the agent's initial state, including its\n",
        "        learnable parameters, configuration, random number generator, and\n",
        "        pre-computed learning schedules.\n",
        "\n",
        "        Args:\n",
        "            initial_params (Dict[str, float]): A dictionary with initial values\n",
        "                for 'theta1_0', 'theta2_0', 'phi1_0', 'phi2_0'. These values\n",
        "                are typically stochastic but reproducible.\n",
        "            config (Dict[str, Any]): The ALM_RL_CONFIG section of the main\n",
        "                                     study configuration, containing all\n",
        "                                     hyperparameters for the agent.\n",
        "            num_episodes (int): The total number of episodes the agent will be\n",
        "                                trained for. This is required to pre-compute\n",
        "                                the full learning schedules.\n",
        "            seed (int): The random seed for this specific agent instance. This\n",
        "                        seed governs all stochastic aspects of the agent's\n",
        "                        behavior, primarily action selection, ensuring that its\n",
        "                        trajectory is reproducible given the same environmental\n",
        "                        conditions.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If `initial_params` is missing required keys, or if\n",
        "                        `num_episodes` is not a positive integer.\n",
        "            TypeError: If input arguments have incorrect types.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Validate the type and content of initial_params.\n",
        "        if not isinstance(initial_params, dict) or not all(\n",
        "            k in initial_params for k in ['theta1_0', 'theta2_0', 'phi1_0', 'phi2_0']\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"initial_params must be a dict with keys 'theta1_0', \"\n",
        "                \"'theta2_0', 'phi1_0', 'phi2_0'.\"\n",
        "            )\n",
        "        # Validate the type of config.\n",
        "        if not isinstance(config, dict):\n",
        "            raise TypeError(f\"config must be a dict, but got {type(config)}.\")\n",
        "        # Validate num_episodes to be a positive integer.\n",
        "        if not isinstance(num_episodes, int) or num_episodes <= 0:\n",
        "            raise ValueError(f\"num_episodes must be a positive integer, but got {num_episodes}.\")\n",
        "        # Validate seed to be an integer.\n",
        "        if not isinstance(seed, int):\n",
        "            raise TypeError(f\"seed must be an integer, but got {type(seed)}.\")\n",
        "\n",
        "        # --- Parameter Initialization ---\n",
        "        # Initialize the critic's parameters, theta = [theta1, theta2], using float64 for precision.\n",
        "        self.theta: np.ndarray = np.array(\n",
        "            [initial_params['theta1_0'], initial_params['theta2_0']],\n",
        "            dtype=np.float64\n",
        "        )\n",
        "        # Initialize the actor's parameters, phi = [phi1, phi2], using float64 for precision.\n",
        "        self.phi: np.ndarray = np.array(\n",
        "            [initial_params['phi1_0'], initial_params['phi2_0']],\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # --- Configuration and State ---\n",
        "        # Store the configuration dictionary for later access to hyperparameters.\n",
        "        self.config: Dict[str, Any] = config\n",
        "        # Initialize the dedicated random number generator using the provided seed and PCG64 for reproducibility.\n",
        "        self.rng: np.random.Generator = np.random.Generator(np.random.PCG64(seed))\n",
        "        # Initialize the episode counter to 0 to track learning progress.\n",
        "        self.episode_counter: int = 0\n",
        "\n",
        "        # --- Pre-compute Learning Schedules (Step 2) ---\n",
        "        # Generate and store the learning rate and temperature schedules for the entire training duration.\n",
        "        self.learning_rates, self.temperatures = self._create_schedules(\n",
        "            num_episodes,\n",
        "            self.config['schedules']\n",
        "        )\n",
        "\n",
        "        # Store projection bounds in a private attribute for efficient access during updates.\n",
        "        self._bounds: Dict[str, float] = self.config['projection_bounds']\n",
        "\n",
        "    def _create_schedules(\n",
        "        self,\n",
        "        num_episodes: int,\n",
        "        schedule_config: Dict[str, Any]\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Pre-computes the learning rate and temperature schedules for all episodes.\n",
        "\n",
        "        This method generates two arrays, one for the learning rate (alpha) and\n",
        "        one for the entropy temperature (gamma), based on the power-law formulas\n",
        "        specified in the paper. Pre-computation is an optimization that avoids\n",
        "        repeated calculations during the training loop.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): The total number of episodes.\n",
        "            schedule_config (Dict[str, Any]): The configuration dictionary for\n",
        "                                              the schedules.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]: A tuple containing the learning_rates\n",
        "                                           array and the temperatures array.\n",
        "        \"\"\"\n",
        "        # Create a floating-point array of episode indices from 0 to N-1.\n",
        "        n: np.ndarray = np.arange(num_episodes, dtype=np.float64)\n",
        "\n",
        "        # --- Learning Rate Schedule (alpha_n) ---\n",
        "        # Equation: a_n = (n + 1)^(-0.75)\n",
        "        # This schedule satisfies the Robbins-Monro conditions for stochastic approximation convergence.\n",
        "        lr_exp: float = schedule_config['learning_rate']['exponent']\n",
        "        learning_rates: np.ndarray = (n + 1.0) ** lr_exp\n",
        "\n",
        "        # --- Temperature Schedule (gamma_n) ---\n",
        "        # Equation: b_n = (n + 1)^(0.25)\n",
        "        # This is an increasing sequence that governs the decay of the temperature.\n",
        "        exp_exp: float = schedule_config['exploration_schedule']['exponent']\n",
        "        b_n: np.ndarray = (n + 1.0) ** exp_exp\n",
        "\n",
        "        # Equation: gamma_n = c_gamma / b_n (From Eq. 15)\n",
        "        # The temperature decays over time, shifting focus from exploration to exploitation.\n",
        "        c_gamma: float = schedule_config['temperature']['c_gamma']\n",
        "        temperatures: np.ndarray = c_gamma / b_n\n",
        "\n",
        "        # Return the two pre-computed schedule arrays.\n",
        "        return learning_rates, temperatures\n",
        "\n",
        "    def _compute_value(self, states: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the parameterized value function J(x; theta) for a batch of states.\n",
        "\n",
        "        This function implements the critic's role by estimating the value of being\n",
        "        in a given state (or a vector of states). The functional form is quadratic,\n",
        "        as dictated by the theoretical solution to the LQ problem.\n",
        "\n",
        "        Args:\n",
        "            states (np.ndarray): A NumPy array of surplus deviation values (x).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: A NumPy array of the same shape as `states`, containing\n",
        "                        the computed value for each state.\n",
        "        \"\"\"\n",
        "        # Equation: J(x; θ) = -0.5 * θ₁ * x² + θ₂ (Simplified from Eq. 8)\n",
        "        # This is the parameterized quadratic value function.\n",
        "        theta1, theta2 = self.theta\n",
        "\n",
        "        # The vectorized computation is highly efficient for trajectories.\n",
        "        return -0.5 * theta1 * (states ** 2) + theta2\n",
        "\n",
        "    def _compute_entropy(self) -> float:\n",
        "        \"\"\"\n",
        "        Computes the differential entropy of the current Gaussian policy.\n",
        "\n",
        "        The entropy term encourages exploration by rewarding the agent for\n",
        "        maintaining a stochastic (higher variance) policy.\n",
        "\n",
        "        Returns:\n",
        "            float: The scalar entropy value `p(φ)`. Returns -inf if the policy\n",
        "                   variance is non-positive.\n",
        "        \"\"\"\n",
        "        # Equation: p(φ) = 0.5 * log(2 * π * e * φ₂)\n",
        "        # This is the standard formula for the differential entropy of a 1D Gaussian.\n",
        "        phi2 = self.phi[1]\n",
        "\n",
        "        # --- Stability Check ---\n",
        "        # The logarithm is undefined for non-positive variance.\n",
        "        if phi2 <= 1e-9:  # Use a small epsilon for safe comparison\n",
        "            return -np.inf\n",
        "\n",
        "        # Calculate and return the entropy.\n",
        "        return 0.5 * np.log(2.0 * np.pi * math.e * phi2)\n",
        "\n",
        "    def select_action(self, state: float) -> float:\n",
        "        \"\"\"\n",
        "        Selects an action by sampling from the current policy π(u|x; φ).\n",
        "\n",
        "        This function implements the actor's role. Given the current state, it\n",
        "        computes the parameters of the Gaussian policy and draws a random\n",
        "        action from it.\n",
        "\n",
        "        Args:\n",
        "            state (float): The current surplus deviation `x`.\n",
        "\n",
        "        Returns:\n",
        "            float: The sampled control action `u`.\n",
        "        \"\"\"\n",
        "        # Equation: π(u|x; φ) = N(u | φ₁*x, φ₂) (From Eq. 9)\n",
        "        phi1, phi2 = self.phi\n",
        "\n",
        "        # Calculate the mean of the Gaussian policy, which is linear in the state.\n",
        "        mean: float = phi1 * state\n",
        "        # Calculate the standard deviation (sqrt of variance). Variance is state-independent.\n",
        "        std_dev: float = np.sqrt(phi2)\n",
        "\n",
        "        # Sample an action from the normal distribution using the agent's dedicated RNG.\n",
        "        action: float = self.rng.normal(loc=mean, scale=std_dev)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def _project_parameters(self) -> None:\n",
        "        \"\"\"\n",
        "        Applies projection operators to keep parameters within stable bounds.\n",
        "\n",
        "        This is a critical step for ensuring numerical stability during training.\n",
        "        It prevents the agent's parameters from growing uncontrollably by\n",
        "        clipping them to a predefined hyper-rectangle after each update.\n",
        "        This corresponds to the Π_K operator in the paper.\n",
        "        \"\"\"\n",
        "        # Unpack the bounds for clarity.\n",
        "        theta_max, phi1_max = self._bounds['theta_max'], self._bounds['phi1_max']\n",
        "        phi2_min, phi2_max = self._bounds['phi2_min'], self._bounds['phi2_max']\n",
        "\n",
        "        # Project the critic's parameters.\n",
        "        self.theta[0] = np.clip(self.theta[0], -theta_max, theta_max)\n",
        "        self.theta[1] = np.clip(self.theta[1], -theta_max, theta_max)\n",
        "\n",
        "        # Project the actor's parameters.\n",
        "        self.phi[0] = np.clip(self.phi[0], -phi1_max, phi1_max)\n",
        "        self.phi[1] = np.clip(self.phi[1], phi2_min, phi2_max)\n",
        "\n",
        "    def update_parameters(\n",
        "        self,\n",
        "        trajectory: List[Tuple[float, float]],\n",
        "        terminal_state: float,\n",
        "        penalty_Q: float,\n",
        "        delta_t: float\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Updates the agent's parameters using the collected episode trajectory.\n",
        "\n",
        "        This method implements the core learning logic of the paper. It translates\n",
        "        the continuous-time update rules (Eqs. 16, 17, 18) into their\n",
        "        discretized, vectorized form for efficient computation over the\n",
        "        trajectory of a single episode.\n",
        "\n",
        "        Args:\n",
        "            trajectory (List[Tuple[float, float]]): A list of (state, action)\n",
        "                tuples `(x_k, u_k)` collected during the episode, from k=0 to K-1.\n",
        "            terminal_state (float): The final state `x_K` at the end of the episode.\n",
        "            penalty_Q (float): The interim penalty coefficient Q from the objective.\n",
        "            delta_t (float): The time step size Δt used in the simulation.\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the episode counter exceeds the pre-computed schedule length.\n",
        "        \"\"\"\n",
        "        # --- 0. Pre-update Validation ---\n",
        "        if self.episode_counter >= len(self.learning_rates):\n",
        "            raise IndexError(\"Episode counter exceeds the length of pre-computed schedules.\")\n",
        "\n",
        "        # --- 1. Data Preparation ---\n",
        "        # Unpack the trajectory into separate NumPy arrays for efficient vectorized operations.\n",
        "        # states_k contains [x_0, x_1, ..., x_{K-1}].\n",
        "        states_k: np.ndarray = np.array([s for s, a in trajectory], dtype=np.float64)\n",
        "        # actions_k contains [u_0, u_1, ..., u_{K-1}].\n",
        "        actions_k: np.ndarray = np.array([a for s, a in trajectory], dtype=np.float64)\n",
        "\n",
        "        # Retrieve the learning rate and temperature for the current episode from pre-computed schedules.\n",
        "        alpha: float = self.learning_rates[self.episode_counter]\n",
        "        gamma: float = self.temperatures[self.episode_counter]\n",
        "\n",
        "        # --- 2. TD Error Calculation ---\n",
        "        # Construct an array of all states visited, including the terminal state: [x_0, ..., x_K].\n",
        "        all_states: np.ndarray = np.append(states_k, terminal_state)\n",
        "        # Compute the value function J(x) at all visited states.\n",
        "        values: np.ndarray = self._compute_value(all_states)\n",
        "\n",
        "        # Compute the policy's entropy for the current set of parameters.\n",
        "        entropy: float = self._compute_entropy()\n",
        "\n",
        "        # Calculate the Temporal Difference (TD) errors for each step k from 0 to K-1.\n",
        "        # Equation: δ_k = J(x_{k+1}) - J(x_k) - 0.5*Q*x_k^2*Δt + γ*p*Δt\n",
        "        # This is the discretized Bellman error, which serves as the core learning signal.\n",
        "        td_errors: np.ndarray = (values[1:] - values[:-1] -\n",
        "                                 0.5 * penalty_Q * (states_k ** 2) * delta_t +\n",
        "                                 gamma * entropy * delta_t)\n",
        "\n",
        "        # --- 3. Gradient Calculation and Accumulation ---\n",
        "        # The following calculations are the discrete sums that approximate the integrals\n",
        "        # in the paper's policy gradient and policy evaluation update rules.\n",
        "\n",
        "        # -- Critic Gradients (approximating Eq. 16) --\n",
        "        # Gradient of J w.r.t. theta1: ∂J/∂θ₁ = -0.5 * x_k²\n",
        "        grad_J_dtheta1: np.ndarray = -0.5 * (states_k ** 2)\n",
        "        # Gradient of J w.r.t. theta2: ∂J/∂θ₂ = 1\n",
        "        grad_J_dtheta2: np.ndarray = np.ones_like(states_k)\n",
        "\n",
        "        # Total update for theta = α * Σ_k [ (∂J/∂θ)_k * δ_k ]\n",
        "        update_theta1: float = alpha * np.sum(grad_J_dtheta1 * td_errors)\n",
        "        update_theta2: float = alpha * np.sum(grad_J_dtheta2 * td_errors)\n",
        "\n",
        "        # -- Actor Gradients (approximating Eqs. 17 & 18) --\n",
        "        phi1, phi2 = self.phi\n",
        "        # The action residuals (u_k - μ_k) are a key component of policy gradients.\n",
        "        residuals: np.ndarray = actions_k - phi1 * states_k\n",
        "\n",
        "        # Gradient of the log-policy w.r.t. phi1: ∂logπ/∂φ₁ = (u_k - φ₁*x_k)*x_k / φ₂\n",
        "        grad_logpi_dphi1: np.ndarray = (residuals * states_k) / phi2\n",
        "\n",
        "        # Gradient of the log-policy w.r.t. phi2: ∂logπ/∂φ₂ = ((u_k - φ₁*x_k)² - φ₂) / (2 * φ₂²)\n",
        "        grad_logpi_dphi2: np.ndarray = ((residuals ** 2) - phi2) / (2.0 * phi2 ** 2)\n",
        "\n",
        "        # Total update for phi1 = α * Σ_k [ (∂logπ/∂φ₁)_k * δ_k ] * φ₂\n",
        "        # The multiplication by phi2 is a variance reduction technique mentioned in the paper.\n",
        "        update_phi1: float = alpha * np.sum(grad_logpi_dphi1 * td_errors) * phi2\n",
        "\n",
        "        # Total update for phi2 = -α * Σ_k [ (∂logπ/∂φ₂)_k * δ_k + γ * (∂p/∂φ₂)_k * Δt ]\n",
        "        # Note the minus sign: this is gradient ASCENT on the objective (max reward).\n",
        "        # The entropy's gradient also influences the variance to encourage exploration.\n",
        "        grad_entropy_dphi2: float = 0.5 / phi2\n",
        "        update_phi2: float = -alpha * (np.sum(grad_logpi_dphi2 * td_errors) +\n",
        "                                       len(trajectory) * gamma * grad_entropy_dphi2 * delta_t)\n",
        "\n",
        "        # --- 4. Parameter Update and Projection ---\n",
        "        # Apply the calculated updates to the agent's parameters.\n",
        "        self.theta += np.array([update_theta1, update_theta2])\n",
        "        self.phi += np.array([update_phi1, update_phi2])\n",
        "\n",
        "        # Project parameters back into their pre-defined stable bounds.\n",
        "        self._project_parameters()\n",
        "\n",
        "        # --- 5. Increment Episode Counter ---\n",
        "        # Advance the counter to use the correct learning rate and temperature in the next episode.\n",
        "        self.episode_counter += 1\n"
      ],
      "metadata": {
        "id": "v32Zm7lLTbuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Traditional Baseline Algorithms Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 5, Step 1: Dynamic CPPI Agent Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class DCPPI_Agent:\n",
        "    \"\"\"\n",
        "    Implements the Dynamic Constant Proportion Portfolio Insurance (DCPPI) agent.\n",
        "\n",
        "    This class represents a baseline strategy for Asset-Liability Management.\n",
        "    Unlike traditional CPPI which uses a fixed multiplier, this enhanced version\n",
        "    features an adaptive multiplier `m` that is updated at the end of each\n",
        "    episode. The update rule is heuristic, based on a \"sign consistency\"\n",
        "    score of the recent trajectory, aiming to stabilize the surplus deviation\n",
        "    around the target of zero.\n",
        "\n",
        "    The agent's policy is a simple linear control law: u(t) = -m * x(t).\n",
        "\n",
        "    Attributes:\n",
        "        m (float): The adaptive multiplier, the sole learnable parameter of the agent.\n",
        "        episode_counter (int): Tracks the number of episodes completed, used to\n",
        "                               index into the learning rate schedule.\n",
        "        learning_rates (np.ndarray): A pre-computed array of learning rates (alpha)\n",
        "                                     for each episode, shared with the ALM-RL agent\n",
        "                                     for fair comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_m: float,\n",
        "        config: Dict[str, Any],\n",
        "        num_episodes: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the DCPPI_Agent.\n",
        "\n",
        "        Args:\n",
        "            initial_m (float): The initial value for the adaptive multiplier `m`.\n",
        "            config (Dict[str, Any]): The BASELINES_CONFIG['dcppi'] section of the\n",
        "                                     main study configuration.\n",
        "            num_episodes (int): The total number of episodes for which to\n",
        "                                pre-compute the learning rate schedule.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If input arguments have incorrect types.\n",
        "            ValueError: If `num_episodes` is not a positive integer.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Validate the type of the initial multiplier.\n",
        "        if not isinstance(initial_m, (float, int)):\n",
        "            raise TypeError(f\"initial_m must be a float, but got {type(initial_m)}.\")\n",
        "        # Validate the type of the configuration dictionary.\n",
        "        if not isinstance(config, dict):\n",
        "            raise TypeError(f\"config must be a dict, but got {type(config)}.\")\n",
        "        # Validate that num_episodes is a positive integer.\n",
        "        if not isinstance(num_episodes, int) or num_episodes <= 0:\n",
        "            raise ValueError(f\"num_episodes must be a positive integer, but got {num_episodes}.\")\n",
        "\n",
        "        # --- Parameter Initialization ---\n",
        "        # Set the initial value of the adaptive multiplier.\n",
        "        self.m: float = float(initial_m)\n",
        "\n",
        "        # --- State and Schedule Initialization ---\n",
        "        # Initialize the episode counter to track learning progress.\n",
        "        self.episode_counter: int = 0\n",
        "        # Pre-compute the learning rate schedule. This uses the same formula as the\n",
        "        # ALM-RL agent to ensure a fair comparison of learning dynamics.\n",
        "        self.learning_rates: np.ndarray = self._create_schedule(\n",
        "            num_episodes,\n",
        "            config['learning_rate']\n",
        "        )\n",
        "\n",
        "    def _create_schedule(\n",
        "        self,\n",
        "        num_episodes: int,\n",
        "        schedule_config: Dict[str, Any]\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Pre-computes the learning rate schedule for all episodes.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): The total number of episodes.\n",
        "            schedule_config (Dict[str, Any]): The configuration for the schedule.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The pre-computed array of learning rates.\n",
        "        \"\"\"\n",
        "        # Create a floating-point array of episode indices from 0 to N-1.\n",
        "        n: np.ndarray = np.arange(num_episodes, dtype=np.float64)\n",
        "\n",
        "        # --- Learning Rate Schedule (a_n) ---\n",
        "        # Equation: a_n = (n + 1)^(-0.75)\n",
        "        # This decaying schedule ensures that updates become smaller over time,\n",
        "        # promoting stability and convergence.\n",
        "        lr_exp: float = schedule_config['exponent']\n",
        "        learning_rates: np.ndarray = (n + 1.0) ** lr_exp\n",
        "\n",
        "        return learning_rates\n",
        "\n",
        "    def select_action(self, state: float) -> float:\n",
        "        \"\"\"\n",
        "        Selects an action based on the DCPPI policy.\n",
        "\n",
        "        Args:\n",
        "            state (float): The current surplus deviation `x`.\n",
        "\n",
        "        Returns:\n",
        "            float: The calculated control action `u`.\n",
        "        \"\"\"\n",
        "        # --- DCPPI Policy ---\n",
        "        # Equation: u(t) = -m * x(t) (From Sec. 5.1.1)\n",
        "        # The control action is directly proportional to the deviation, with the\n",
        "        # goal of pushing the deviation back towards zero.\n",
        "        return -self.m * state\n",
        "\n",
        "    def update_multiplier(\n",
        "        self,\n",
        "        trajectory_states: List[float]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Updates the adaptive multiplier `m` based on the episode's trajectory.\n",
        "\n",
        "        The update rule is based on a heuristic \"sign consistency\" score, which\n",
        "        measures the tendency of the trajectory to oscillate or drift.\n",
        "\n",
        "        Args:\n",
        "            trajectory_states (List[float]): A list of the state values `x_k`\n",
        "                                             from the completed episode.\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the episode counter exceeds the pre-computed schedule length.\n",
        "        \"\"\"\n",
        "        # --- Pre-update Validation ---\n",
        "        # Ensure the episode counter is within the bounds of the pre-computed schedule.\n",
        "        if self.episode_counter >= len(self.learning_rates):\n",
        "            raise IndexError(\"Episode counter exceeds the length of pre-computed schedule.\")\n",
        "        # Ensure the trajectory has enough data to compute the update.\n",
        "        if len(trajectory_states) < 2:\n",
        "            # If not enough states to compute a product, do not update.\n",
        "            self.episode_counter += 1\n",
        "            return\n",
        "\n",
        "        # --- 1. Sign Consistency Score Calculation ---\n",
        "        # Use a recent segment of the trajectory for the score, as specified (l=10).\n",
        "        # This makes the update rule responsive to recent performance.\n",
        "        recent_states: np.ndarray = np.array(trajectory_states[-10:], dtype=np.float64)\n",
        "\n",
        "        # Calculate the products of consecutive states: x_i * x_{i+1}.\n",
        "        # A negative product indicates the trajectory crossed zero (oscillation).\n",
        "        # A positive product indicates the trajectory stayed on one side (drift).\n",
        "        consecutive_products: np.ndarray = recent_states[:-1] * recent_states[1:]\n",
        "\n",
        "        # Equation: S = Σ sgn(x_i * x_{i+1})\n",
        "        # Sum the signs of the products. A positive score means more drift than\n",
        "        # oscillation. A negative score means more oscillation than drift.\n",
        "        sign_consistency_score: float = np.sum(np.sign(consecutive_products))\n",
        "\n",
        "        # --- 2. Multiplier Update ---\n",
        "        # Retrieve the learning rate for the current episode.\n",
        "        alpha: float = self.learning_rates[self.episode_counter]\n",
        "\n",
        "        # Equation: m_{n+1} = m_n + a_n * sgn(S)\n",
        "        # The multiplier is adjusted based on the score.\n",
        "        # - If score > 0 (drift), sgn(S)=1, m increases to provide stronger control.\n",
        "        # - If score < 0 (oscillation), sgn(S)=-1, m decreases to dampen control.\n",
        "        # - If score = 0, sgn(S)=0, no update is made.\n",
        "        self.m += alpha * np.sign(sign_consistency_score)\n",
        "\n",
        "        # --- 3. Increment Episode Counter ---\n",
        "        # Advance the counter for the next learning cycle.\n",
        "        self.episode_counter += 1\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Task 5, Step 2: Adaptive Contingent Strategy Agent Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class ACS_Agent:\n",
        "    \"\"\"\n",
        "    Implements the Adaptive Contingent Strategy (ACS) agent.\n",
        "\n",
        "    This class represents a more conservative baseline strategy compared to DCPPI.\n",
        "    Its defining feature is a \"dead-zone\" or tolerance band (`delta`) around the\n",
        "    target surplus deviation. The agent takes no action as long as the deviation\n",
        "    `x(t)` remains within this band (i.e., `|x(t)| <= delta`). This inaction is\n",
        "    designed to reduce unnecessary transactions and noise amplification when\n",
        "    the system is already close to its target.\n",
        "\n",
        "    When the deviation exceeds the tolerance, a control action proportional to the\n",
        "    *excess* deviation is applied. Like the DCPPI agent, it uses an adaptive\n",
        "    multiplier `m` that is updated using the identical sign consistency score\n",
        "    heuristic, allowing for a direct comparison of the two distinct policy\n",
        "    structures under the same learning mechanism.\n",
        "\n",
        "    Attributes:\n",
        "        m (float): The adaptive multiplier, which is the agent's sole learnable\n",
        "                   parameter. It scales the control response.\n",
        "        delta (float): The fixed, non-negative tolerance parameter that defines\n",
        "                       the half-width of the dead-zone.\n",
        "        episode_counter (int): Tracks the number of completed episodes, used to\n",
        "                               index into the learning rate schedule.\n",
        "        learning_rates (np.ndarray): A pre-computed array of learning rates (`a_n`),\n",
        "                                     identical to that of other agents to ensure\n",
        "                                     a fair and controlled comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_m: float,\n",
        "        tolerance_delta: float,\n",
        "        config: Dict[str, Any],\n",
        "        num_episodes: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the ACS_Agent.\n",
        "\n",
        "        Args:\n",
        "            initial_m (float): The initial value for the adaptive multiplier `m`.\n",
        "            tolerance_delta (float): The half-width of the inaction band, `delta`.\n",
        "            config (Dict[str, Any]): The BASELINES_CONFIG['acs'] section of the\n",
        "                                     main study configuration, which contains\n",
        "                                     the learning rate specification.\n",
        "            num_episodes (int): The total number of episodes for which to\n",
        "                                pre-compute the learning rate schedule.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If input arguments have incorrect types.\n",
        "            ValueError: If `num_episodes` is not positive or `tolerance_delta`\n",
        "                        is negative.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Validate the type of the initial multiplier.\n",
        "        if not isinstance(initial_m, (float, int)):\n",
        "            raise TypeError(f\"initial_m must be a float, but got {type(initial_m)}.\")\n",
        "        # Validate the type and value of the tolerance delta.\n",
        "        if not isinstance(tolerance_delta, (float, int)) or tolerance_delta < 0:\n",
        "            raise ValueError(f\"tolerance_delta must be a non-negative float, but got {tolerance_delta}.\")\n",
        "        # Validate the type of the configuration dictionary.\n",
        "        if not isinstance(config, dict):\n",
        "            raise TypeError(f\"config must be a dict, but got {type(config)}.\")\n",
        "        # Validate that num_episodes is a positive integer.\n",
        "        if not isinstance(num_episodes, int) or num_episodes <= 0:\n",
        "            raise ValueError(f\"num_episodes must be a positive integer, but got {num_episodes}.\")\n",
        "\n",
        "        # --- Parameter Initialization ---\n",
        "        # Set the initial value of the adaptive multiplier.\n",
        "        self.m: float = float(initial_m)\n",
        "        # Set the fixed tolerance parameter for the dead-zone.\n",
        "        self.delta: float = float(tolerance_delta)\n",
        "\n",
        "        # --- State and Schedule Initialization ---\n",
        "        # Initialize the episode counter to track learning progress.\n",
        "        self.episode_counter: int = 0\n",
        "        # Pre-compute the learning rate schedule, ensuring consistency with other agents.\n",
        "        self.learning_rates: np.ndarray = self._create_schedule(\n",
        "            num_episodes,\n",
        "            config['learning_rate']\n",
        "        )\n",
        "\n",
        "    def _create_schedule(\n",
        "        self,\n",
        "        num_episodes: int,\n",
        "        schedule_config: Dict[str, Any]\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Pre-computes the learning rate schedule for all episodes.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): The total number of episodes.\n",
        "            schedule_config (Dict[str, Any]): The configuration for the schedule.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The pre-computed array of learning rates.\n",
        "        \"\"\"\n",
        "        # Create a floating-point array of episode indices from 0 to N-1.\n",
        "        n: np.ndarray = np.arange(num_episodes, dtype=np.float64)\n",
        "\n",
        "        # --- Learning Rate Schedule (a_n) ---\n",
        "        # Equation: a_n = (n + 1)^(-0.75)\n",
        "        # This decaying schedule ensures that updates to the multiplier `m`\n",
        "        # become smaller over time, promoting stability and convergence.\n",
        "        lr_exp: float = schedule_config['exponent']\n",
        "        learning_rates: np.ndarray = (n + 1.0) ** lr_exp\n",
        "\n",
        "        return learning_rates\n",
        "\n",
        "    def select_action(self, state: float) -> float:\n",
        "        \"\"\"\n",
        "        Selects a control action based on the ACS policy with a dead-zone.\n",
        "\n",
        "        Args:\n",
        "            state (float): The current surplus deviation `x`.\n",
        "\n",
        "        Returns:\n",
        "            float: The calculated control action `u`.\n",
        "        \"\"\"\n",
        "        # --- ACS Policy ---\n",
        "        # Equation: u(t) = -m * sgn(x(t)) * max(|x(t)| - δ, 0) (From Eq. 27)\n",
        "\n",
        "        # Calculate the absolute deviation from the target.\n",
        "        abs_state: float = np.abs(state)\n",
        "\n",
        "        # --- Dead-Zone Logic ---\n",
        "        # If the deviation is within the tolerance band, take no action. This is\n",
        "        # the defining characteristic of the contingent strategy.\n",
        "        if abs_state <= self.delta:\n",
        "            return 0.0\n",
        "        else:\n",
        "            # If the deviation is outside the band, apply a corrective control action.\n",
        "            # The magnitude of the control is proportional to the *excess* deviation\n",
        "            # beyond the tolerance threshold.\n",
        "            excess_deviation: float = abs_state - self.delta\n",
        "\n",
        "            # The sign of the state determines the direction of the control action\n",
        "            # (e.g., a positive deviation requires a negative action to correct).\n",
        "            state_sign: float = np.sign(state)\n",
        "\n",
        "            # The final action combines the adaptive multiplier, the direction,\n",
        "            # and the magnitude of the excess deviation.\n",
        "            return -self.m * state_sign * excess_deviation\n",
        "\n",
        "    def update_multiplier(\n",
        "        self,\n",
        "        trajectory_states: List[float]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Updates the adaptive multiplier `m` based on the episode's trajectory.\n",
        "\n",
        "        NOTE: This update logic is IDENTICAL to that of the DCPPI agent, as\n",
        "        specified in the paper. This experimental design choice allows for a\n",
        "        direct and fair comparison of the two different policy structures\n",
        "        (linear vs. dead-zone) under the exact same learning mechanism.\n",
        "\n",
        "        Args:\n",
        "            trajectory_states (List[float]): A list of the state values `x_k`\n",
        "                                             from the completed episode.\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the episode counter exceeds the pre-computed schedule length.\n",
        "        \"\"\"\n",
        "        # --- Pre-update Validation ---\n",
        "        # Ensure the episode counter is within the bounds of the pre-computed schedule.\n",
        "        if self.episode_counter >= len(self.learning_rates):\n",
        "            raise IndexError(\"Episode counter exceeds the length of pre-computed schedule.\")\n",
        "        # If the trajectory is too short to compute a product of consecutive states,\n",
        "        # no update is possible. We still increment the counter.\n",
        "        if len(trajectory_states) < 2:\n",
        "            self.episode_counter += 1\n",
        "            return\n",
        "\n",
        "        # --- 1. Sign Consistency Score Calculation (Identical to DCPPI) ---\n",
        "        # Use a recent segment of the trajectory (last 10 steps) for the score.\n",
        "        recent_states: np.ndarray = np.array(trajectory_states[-10:], dtype=np.float64)\n",
        "\n",
        "        # Calculate the products of consecutive states: x_i * x_{i+1}.\n",
        "        consecutive_products: np.ndarray = recent_states[:-1] * recent_states[1:]\n",
        "\n",
        "        # Equation: S = Σ sgn(x_i * x_{i+1})\n",
        "        # Sum the signs of the products. A positive score suggests drift, while\n",
        "        # a negative score suggests oscillation.\n",
        "        sign_consistency_score: float = np.sum(np.sign(consecutive_products))\n",
        "\n",
        "        # --- 2. Multiplier Update (Identical to DCPPI) ---\n",
        "        # Retrieve the learning rate for the current episode from the pre-computed schedule.\n",
        "        alpha: float = self.learning_rates[self.episode_counter]\n",
        "\n",
        "        # Equation: m_{n+1} = m_n + a_n * sgn(S)\n",
        "        # Adjust the multiplier based on the sign of the consistency score.\n",
        "        self.m += alpha * np.sign(sign_consistency_score)\n",
        "\n",
        "        # --- 3. Increment Episode Counter ---\n",
        "        # Advance the counter to prepare for the next learning cycle.\n",
        "        self.episode_counter += 1\n",
        "\n",
        "# =============================================================================\n",
        "# Task 5, Step 3: Model-Based Plugin Agent Implementation - Professional Grade\n",
        "# =============================================================================\n",
        "\n",
        "class MBP_Agent:\n",
        "    \"\"\"\n",
        "    Implements the Model-Based Plugin (MBP) strategy for ALM.\n",
        "\n",
        "    This agent embodies a classic approach to control under uncertainty: first\n",
        "    learn a model of the environment, then act optimally according to that model.\n",
        "    It operates in two distinct phases:\n",
        "\n",
        "    1. Estimation Phase: For an initial period, the agent disregards the state\n",
        "       and injects random noise into the system to ensure rich data collection\n",
        "       for model identification. It populates a sliding window buffer with\n",
        "       `(state, action, next_state)` transitions. At the end of each episode in\n",
        "       this phase, it uses the collected data to perform least squares\n",
        "       regression, estimating the parameters (A, B, C, D) of the assumed\n",
        "       linear SDE model of the environment.\n",
        "\n",
        "    2. Exploitation Phase: Once the data buffer is full, signifying the end of\n",
        "       the primary estimation period, the agent transitions to exploitation. It\n",
        "       \"plugs\" its final parameter estimates into the analytical formula for the\n",
        "       optimal control gain derived from stochastic control theory. For all\n",
        "       subsequent episodes, it follows the resulting deterministic linear\n",
        "       feedback policy.\n",
        "\n",
        "    This strategy's performance is critically dependent on the accuracy of the\n",
        "    parameter estimation in the first phase.\n",
        "\n",
        "    Attributes:\n",
        "        episode_counter (int): Tracks the number of episodes completed.\n",
        "        phase (str): The current operational phase: 'estimation' or 'exploitation'.\n",
        "        exploration_noise_std (float): The decaying standard deviation of the\n",
        "                                       exploration policy used in the estimation phase.\n",
        "        buffer (Deque): A sliding window buffer (FIFO queue) storing recent\n",
        "                        transitions for parameter estimation.\n",
        "        estimates (Dict[str, float]): A dictionary holding the current estimates\n",
        "                                      for the SDE parameters A, B, C, and D.\n",
        "        policy_gain (Optional[float]): The computed optimal policy gain (phi1_star)\n",
        "                                       used in the exploitation phase. Becomes non-None\n",
        "                                       after the estimation phase is complete.\n",
        "        rng (np.random.Generator): The agent's dedicated random number generator,\n",
        "                                   used for the exploration policy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: Dict[str, Any],\n",
        "        delta_t: float,\n",
        "        seed: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the MBP_Agent.\n",
        "\n",
        "        Args:\n",
        "            config (Dict[str, Any]): The BASELINES_CONFIG['mbp'] section of the\n",
        "                                     main study configuration.\n",
        "            delta_t (float): The time step size of the simulation, which is\n",
        "                             essential for the discrete-time approximations in\n",
        "                             the estimation equations.\n",
        "            seed (int): The random seed for this agent instance, ensuring a\n",
        "                        reproducible exploration policy.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If input arguments have incorrect types.\n",
        "            ValueError: If config values are invalid (e.g., negative delta_t).\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Validate the type of the configuration dictionary.\n",
        "        if not isinstance(config, dict):\n",
        "            raise TypeError(f\"config must be a dict, but got {type(config)}.\")\n",
        "        # Validate the time step delta_t.\n",
        "        if not isinstance(delta_t, float) or delta_t <= 0:\n",
        "            raise ValueError(f\"delta_t must be a positive float, but got {delta_t}.\")\n",
        "        # Validate the seed.\n",
        "        if not isinstance(seed, int):\n",
        "            raise TypeError(f\"seed must be an integer, but got {type(seed)}.\")\n",
        "\n",
        "        # --- Configuration and State ---\n",
        "        # Store the configuration dictionary for hyperparameter access.\n",
        "        self.config: Dict[str, Any] = config\n",
        "        # Store the time step size.\n",
        "        self.delta_t: float = delta_t\n",
        "        # Initialize the episode counter.\n",
        "        self.episode_counter: int = 0\n",
        "        # The agent starts in the 'estimation' phase.\n",
        "        self.phase: str = \"estimation\"\n",
        "        # Set the initial standard deviation for the exploration noise.\n",
        "        self.exploration_noise_std: float = config['exploration_noise_initial']\n",
        "\n",
        "        # --- Data Buffer ---\n",
        "        # Initialize a deque with a fixed max length to act as a sliding window for transitions.\n",
        "        buffer_size: int = config['estimation_window']\n",
        "        self.buffer: Deque[Tuple[float, float, float]] = deque(maxlen=buffer_size)\n",
        "\n",
        "        # --- Model Parameters ---\n",
        "        # Initialize parameter estimates to zero. They will be updated by the estimation process.\n",
        "        self.estimates: Dict[str, float] = {\"A\": 0.0, \"B\": 0.0, \"C\": 0.0, \"D\": 0.0}\n",
        "        # The policy gain is initially None and will be computed at the end of the estimation phase.\n",
        "        self.policy_gain: Optional[float] = None\n",
        "\n",
        "        # --- RNG Initialization ---\n",
        "        # Initialize a dedicated RNG for this agent's exploration policy.\n",
        "        self.rng: np.random.Generator = np.random.Generator(np.random.PCG64(seed))\n",
        "\n",
        "    def select_action(self, state: float) -> float:\n",
        "        \"\"\"\n",
        "        Selects a control action based on the agent's current operational phase.\n",
        "\n",
        "        Args:\n",
        "            state (float): The current surplus deviation `x`.\n",
        "\n",
        "        Returns:\n",
        "            float: The calculated control action `u`.\n",
        "        \"\"\"\n",
        "        # --- Phase-Dependent Policy Selection ---\n",
        "        # During the estimation phase, the agent ignores the state and injects\n",
        "        # random noise to explore the system dynamics.\n",
        "        if self.phase == \"estimation\":\n",
        "            # Policy: u_k ~ N(0, sigma_explore^2)\n",
        "            return self.rng.normal(loc=0.0, scale=self.exploration_noise_std)\n",
        "\n",
        "        # During the exploitation phase, the agent applies the deterministic\n",
        "        # policy derived from its learned model.\n",
        "        elif self.phase == \"exploitation\":\n",
        "            # A safeguard to ensure the policy gain has been computed.\n",
        "            if self.policy_gain is None:\n",
        "                # This indicates a logic error in the training loop but prevents a crash.\n",
        "                return 0.0\n",
        "            # Policy: u_k = hat(phi1_star) * x_k\n",
        "            return self.policy_gain * state\n",
        "\n",
        "        # This case should be unreachable given the two-phase design.\n",
        "        return 0.0\n",
        "\n",
        "    def observe_transition(self, state: float, action: float, next_state: float) -> None:\n",
        "        \"\"\"\n",
        "        Records a new transition in the agent's experience buffer.\n",
        "\n",
        "        Args:\n",
        "            state (float): The state `x_k` before the action.\n",
        "            action (float): The action `u_k` taken.\n",
        "            next_state (float): The resulting state `x_{k+1}`.\n",
        "        \"\"\"\n",
        "        # Append the (s, a, s') tuple to the sliding window buffer.\n",
        "        self.buffer.append((state, action, next_state))\n",
        "\n",
        "    def end_of_episode_update(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs end-of-episode tasks: parameter estimation, noise decay,\n",
        "        and potential phase transition.\n",
        "        \"\"\"\n",
        "        # --- 1. Parameter Estimation ---\n",
        "        # Perform estimation only if the buffer has a minimum amount of data\n",
        "        # required for a stable regression (e.g., more samples than parameters).\n",
        "        if len(self.buffer) > 10:\n",
        "            self._estimate_parameters()\n",
        "\n",
        "        # --- 2. Exploration Noise Decay ---\n",
        "        # In the estimation phase, gradually reduce the exploration noise to\n",
        "        # refine data collection around the equilibrium.\n",
        "        if self.phase == \"estimation\":\n",
        "            self.exploration_noise_std *= self.config['exploration_noise_decay']\n",
        "\n",
        "        # --- 3. Phase Transition Logic ---\n",
        "        # Increment the episode counter after all updates for the current episode.\n",
        "        self.episode_counter += 1\n",
        "        # The transition to exploitation occurs exactly once, when the buffer becomes full.\n",
        "        if self.phase == \"estimation\" and len(self.buffer) == self.buffer.maxlen:\n",
        "            # Set the phase to 'exploitation'.\n",
        "            self.phase = \"exploitation\"\n",
        "            # Compute the final, fixed policy gain that will be used for the\n",
        "            # remainder of the experiment.\n",
        "            self._compute_policy_gain()\n",
        "\n",
        "    def _estimate_parameters(self) -> None:\n",
        "        \"\"\"\n",
        "        Estimates the SDE parameters (A, B, C, D) from the transition buffer\n",
        "        using two separate least squares regressions.\n",
        "        \"\"\"\n",
        "        # --- 1. Data Transformation ---\n",
        "        # Convert the deque of tuples into a single NumPy array for efficient vectorized processing.\n",
        "        transitions: np.ndarray = np.array(list(self.buffer), dtype=np.float64)\n",
        "        # Unpack the columns into named variables for clarity.\n",
        "        states_k, actions_k, states_kp1 = transitions[:, 0], transitions[:, 1], transitions[:, 2]\n",
        "\n",
        "        # --- 2. Drift Parameter Estimation (A, B) ---\n",
        "        # The model is: (x_{k+1} - x_k)/dt ≈ A*x_k + B*u_k\n",
        "        # Construct the design matrix X, where each row is [x_k, u_k].\n",
        "        X: np.ndarray = np.vstack([states_k, actions_k]).T\n",
        "        # Construct the response vector Y_drift, representing the observed drift.\n",
        "        Y_drift: np.ndarray = (states_kp1 - states_k) / self.delta_t\n",
        "\n",
        "        # Solve the linear system Y_drift = X * [A, B]^T for the parameters [A, B].\n",
        "        # `lstsq` is used for its numerical stability over direct matrix inversion.\n",
        "        try:\n",
        "            drift_params, _, _, _ = np.linalg.lstsq(X, Y_drift, rcond=None)\n",
        "            # Update the agent's estimates for the drift parameters A and B.\n",
        "            self.estimates['A'], self.estimates['B'] = drift_params\n",
        "        except np.linalg.LinAlgError:\n",
        "            # If the regression fails (e.g., due to singular matrix), we simply\n",
        "            # skip the update for this step and retain the old estimates.\n",
        "            pass\n",
        "\n",
        "        # --- 3. Diffusion Parameter Estimation (C, D) ---\n",
        "        # The model is: |x_{k+1} - x_k - drift*dt| / sqrt(dt) ≈ |C*x_k + D*u_k|\n",
        "        # First, calculate the residuals from our estimated drift model.\n",
        "        # r_k = x_{k+1} - x_k - (Â*x_k + B̂*u_k)*dt\n",
        "        A_hat, B_hat = self.estimates['A'], self.estimates['B']\n",
        "        drift_prediction: np.ndarray = (A_hat * states_k + B_hat * actions_k) * self.delta_t\n",
        "        residuals: np.ndarray = states_kp1 - states_k - drift_prediction\n",
        "\n",
        "        # Construct the response vector Y_diff, representing the observed volatility magnitude.\n",
        "        Y_diff: np.ndarray = np.abs(residuals) / np.sqrt(self.delta_t)\n",
        "\n",
        "        # Solve the linear system Y_diff = X * [C, D]^T for the parameters [C, D].\n",
        "        # We reuse the same design matrix X.\n",
        "        try:\n",
        "            diff_params, _, _, _ = np.linalg.lstsq(X, Y_diff, rcond=None)\n",
        "            # Update the agent's estimates for the diffusion parameters C and D.\n",
        "            self.estimates['C'], self.estimates['D'] = diff_params\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Again, skip the update if the regression fails.\n",
        "            pass\n",
        "\n",
        "    def _compute_policy_gain(self) -> None:\n",
        "        \"\"\"\n",
        "        Computes the \"plugin\" optimal policy gain from the final parameter estimates.\n",
        "        This is performed once at the end of the estimation phase.\n",
        "        \"\"\"\n",
        "        # Retrieve the final estimates for the relevant parameters.\n",
        "        B_hat, C_hat, D_hat = self.estimates['B'], self.estimates['C'], self.estimates['D']\n",
        "\n",
        "        # --- Stability Check ---\n",
        "        # A very small estimated D_hat can lead to an explosive policy gain.\n",
        "        # We cap its minimum absolute value to ensure a stable policy.\n",
        "        if np.abs(D_hat) < 1e-6:\n",
        "            # If D_hat is effectively zero, default to a safe, non-explosive policy gain.\n",
        "            self.policy_gain = 0.0\n",
        "            return\n",
        "\n",
        "        # --- Plugin Policy Gain Calculation ---\n",
        "        # Equation: hat(phi1_star) = -(B̂ + Ĉ*D̂) / D̂^2\n",
        "        # This is the core of the \"plugin\" approach: using the analytical solution\n",
        "        # with the estimated parameters.\n",
        "        self.policy_gain = -(B_hat + C_hat * D_hat) / (D_hat**2)\n"
      ],
      "metadata": {
        "id": "32pn6LuHVpX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Deep RL Environment Wrapper Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 6: Deep RL Environment Wrapper Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class ALMEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A Gymnasium-compliant environment for the Asset-Liability Management problem.\n",
        "\n",
        "    This class wraps the continuous-time SDE dynamics of the ALM problem into a\n",
        "    standard discrete-time RL environment interface, making it compatible with\n",
        "    off-the-shelf Deep RL libraries (e.g., Stable Baselines3, Tianshou).\n",
        "\n",
        "    The environment simulates the evolution of the surplus deviation `x(t)`\n",
        "    governed by the SDE:\n",
        "        dx(t) = (A*x(t) + B*u(t))dt + (C*x(t) + D*u(t))dW(t)\n",
        "    using the Euler-Maruyama discretization method.\n",
        "\n",
        "    The agent's objective is to take control actions `u(t)` to minimize a\n",
        "    quadratic cost function of the deviation `x(t)`, which is equivalent to\n",
        "    maximizing the negative of this cost (the reward).\n",
        "\n",
        "    Attributes:\n",
        "        observation_space (gym.spaces.Box): The space of possible observations,\n",
        "            which is the scalar surplus deviation `x`.\n",
        "        action_space (gym.spaces.Box): The space of possible actions, which is\n",
        "            the scalar control input `u`.\n",
        "    \"\"\"\n",
        "    # Metadata for rendering, not used in this simulation but required by the API.\n",
        "    metadata = {\"render_modes\": []}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        market_params: Dict[str, float],\n",
        "        env_spec: Dict[str, Any]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the ALM Environment.\n",
        "\n",
        "        Args:\n",
        "            market_params (Dict[str, float]): A dictionary containing the true SDE\n",
        "                parameters for this specific environment instance: {'A', 'B', 'C', 'D'}.\n",
        "            env_spec (Dict[str, Any]): The DEEP_RL_ENV_SPEC section of the main\n",
        "                study configuration, containing all other environment settings.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If required keys are missing from the inputs.\n",
        "            TypeError: If inputs have incorrect types.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        if not isinstance(market_params, dict) or not all(k in market_params for k in ['A', 'B', 'C', 'D']):\n",
        "            raise ValueError(\"market_params must be a dict with keys 'A', 'B', 'C', 'D'.\")\n",
        "        if not isinstance(env_spec, dict):\n",
        "            raise TypeError(f\"env_spec must be a dict, but got {type(env_spec)}.\")\n",
        "\n",
        "        # --- SDE and Problem Parameters ---\n",
        "        # Unpack market parameters for the SDE dynamics.\n",
        "        self.A: float = market_params['A']\n",
        "        self.B: float = market_params['B']\n",
        "        self.C: float = market_params['C']\n",
        "        self.D: float = market_params['D']\n",
        "\n",
        "        # Unpack penalty parameters for the reward function.\n",
        "        self.Q: float = env_spec['reward']['Q']\n",
        "        self.H: float = env_spec['reward']['H']\n",
        "\n",
        "        # Unpack time and discretization parameters.\n",
        "        self.delta_t: float = env_spec['dynamics']['delta_t']\n",
        "        self.sqrt_delta_t: float = np.sqrt(self.delta_t)\n",
        "        self.episode_horizon: int = env_spec['episode_horizon']\n",
        "        self.initial_state_val: float = env_spec['initial_state']\n",
        "\n",
        "        # --- State Variables ---\n",
        "        # The current state of the environment (surplus deviation x).\n",
        "        self.state: Optional[np.ndarray] = None\n",
        "        # The current time step within the episode.\n",
        "        self.current_step: int = 0\n",
        "\n",
        "        # --- Gymnasium API Spaces ---\n",
        "        # Define the observation space: a single continuous variable for x.\n",
        "        obs_spec = env_spec['observation_space']\n",
        "        self.observation_space: gym.spaces.Box = gym.spaces.Box(\n",
        "            low=obs_spec['low'],\n",
        "            high=obs_spec['high'],\n",
        "            shape=tuple(obs_spec['shape']),\n",
        "            dtype=np.dtype(obs_spec['dtype'])\n",
        "        )\n",
        "\n",
        "        # Define the action space: a single continuous variable for u.\n",
        "        act_spec = env_spec['action_space']\n",
        "        self.action_space: gym.spaces.Box = gym.spaces.Box(\n",
        "            low=act_spec['low'],\n",
        "            high=act_spec['high'],\n",
        "            shape=tuple(act_spec['shape']),\n",
        "            dtype=np.dtype(act_spec['dtype'])\n",
        "        )\n",
        "\n",
        "        # The random number generator will be initialized in `reset`.\n",
        "        self.rng: Optional[np.random.Generator] = None\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[Dict[str, Any]] = None\n",
        "    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Resets the environment to its initial state.\n",
        "\n",
        "        This method is called at the beginning of each new episode. It resets\n",
        "        the surplus deviation to its starting value and the step counter to zero.\n",
        "        It also re-initializes the random number generator if a seed is provided,\n",
        "        which is crucial for reproducibility.\n",
        "\n",
        "        Args:\n",
        "            seed (Optional[int]): The seed to use for the environment's random\n",
        "                number generator. If provided, it will create a new RNG.\n",
        "            options (Optional[Dict[str, Any]]): Not used in this environment.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, Dict[str, Any]]: A tuple containing the initial\n",
        "                observation and an empty info dictionary.\n",
        "        \"\"\"\n",
        "        # Per Gymnasium API, call super().reset() to handle seeding.\n",
        "        super().reset(seed=seed)\n",
        "        # If a seed is provided, create a new random number generator.\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.Generator(np.random.PCG64(seed))\n",
        "        # If no seed is provided and no RNG exists, create one with a default seed.\n",
        "        elif self.rng is None:\n",
        "            self.rng = np.random.Generator(np.random.PCG64(0))\n",
        "\n",
        "        # Reset the state to the initial surplus deviation.\n",
        "        # The state is wrapped in a NumPy array to match the observation space.\n",
        "        self.state = np.array([self.initial_state_val], dtype=np.float32)\n",
        "        # Reset the episode step counter.\n",
        "        self.current_step = 0\n",
        "\n",
        "        # The info dictionary is empty as there is no auxiliary data to return.\n",
        "        info: Dict[str, Any] = {}\n",
        "        # Return the initial observation and info dictionary.\n",
        "        return self.state, info\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        action: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Executes one time step in the environment's dynamics.\n",
        "\n",
        "        This method applies the agent's action `u` to the system, simulates the\n",
        "        SDE for one time step `delta_t`, calculates the reward, and determines\n",
        "        if the episode has ended.\n",
        "\n",
        "        Args:\n",
        "            action (np.ndarray): The action provided by the agent. Shape (1,).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]: A tuple\n",
        "                containing the next observation, the reward for the step, a\n",
        "                `terminated` flag, a `truncated` flag, and an info dictionary.\n",
        "        \"\"\"\n",
        "        # --- Pre-computation Validation ---\n",
        "        if self.state is None or self.rng is None:\n",
        "            raise RuntimeError(\"Environment has not been reset. Call reset() before step().\")\n",
        "\n",
        "        # --- 1. Action Processing ---\n",
        "        # Clip the action to the defined action space bounds for stability.\n",
        "        clipped_action: float = np.clip(\n",
        "            action, self.action_space.low, self.action_space.high\n",
        "        )[0]\n",
        "\n",
        "        # Get the current state value.\n",
        "        current_x: float = self.state[0]\n",
        "\n",
        "        # --- 2. Reward Calculation ---\n",
        "        # The reward is calculated based on the state *before* the transition.\n",
        "        # Equation: r_k = -0.5 * Q * x_k^2 * delta_t\n",
        "        reward: float = -0.5 * self.Q * (current_x ** 2) * self.delta_t\n",
        "\n",
        "        # --- 3. SDE Dynamics (Euler-Maruyama) ---\n",
        "        # Equation: x_{k+1} = x_k + (A*x_k + B*u_k)*dt + (C*x_k + D*u_k)*sqrt(dt)*Z_k\n",
        "        # where Z_k ~ N(0, 1).\n",
        "\n",
        "        # Calculate the drift term.\n",
        "        drift: float = (self.A * current_x + self.B * clipped_action) * self.delta_t\n",
        "        # Sample a random shock from a standard normal distribution.\n",
        "        random_shock: float = self.rng.standard_normal()\n",
        "        # Calculate the diffusion term, ensuring correct sqrt(dt) scaling.\n",
        "        diffusion: float = (self.C * current_x + self.D * clipped_action) * self.sqrt_delta_t * random_shock\n",
        "\n",
        "        # Update the state by applying the drift and diffusion.\n",
        "        next_x: float = current_x + drift + diffusion\n",
        "\n",
        "        # --- 4. State Update and Termination Check ---\n",
        "        # Update the internal state, ensuring it conforms to the observation space dtype.\n",
        "        self.state = np.array([next_x], dtype=np.float32)\n",
        "        # Increment the step counter.\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check for episode termination. In this environment, the episode only\n",
        "        # ends when the time horizon is reached.\n",
        "        # `terminated` is for task-specific end conditions (e.g., goal reached).\n",
        "        terminated: bool = False\n",
        "        # `truncated` is for time-limit-based endings.\n",
        "        truncated: bool = self.current_step >= self.episode_horizon\n",
        "\n",
        "        # --- 5. Terminal Reward ---\n",
        "        # If the episode is truncated, add the terminal penalty to the final reward.\n",
        "        if truncated:\n",
        "            # Equation: r_T = -0.5 * H * x_K^2\n",
        "            terminal_penalty: float = -0.5 * self.H * (self.state[0] ** 2)\n",
        "            reward += terminal_penalty\n",
        "\n",
        "        # The info dictionary is empty.\n",
        "        info: Dict[str, Any] = {}\n",
        "\n",
        "        # Return the standard 5-tuple for a Gymnasium step.\n",
        "        return self.state, reward, terminated, truncated, info\n"
      ],
      "metadata": {
        "id": "Yv6EUS3DaPDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Deep RL Algorithms Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7, Step 1: Soft Actor-Critic (SAC) Implementation\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 1: Neural Network Architectures\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    The SAC Actor (Policy) Network.\n",
        "\n",
        "    This network implements a stochastic policy by mapping a state observation\n",
        "    to the parameters of a probability distribution over actions. Specifically,\n",
        "    for a given state `s`, it outputs the mean and log standard deviation of a\n",
        "    Gaussian distribution. To ensure actions lie within a bounded interval, a\n",
        "    `tanh` squashing function is applied to the samples from this Gaussian.\n",
        "    The final output is then scaled and shifted to match the environment's\n",
        "    specific action space.\n",
        "\n",
        "    The architecture follows the specification:\n",
        "    Input -> Linear(32) -> ReLU -> Linear(32) -> ReLU -> [Linear(action_dim), Linear(action_dim)]\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int, action_scale: float, action_bias: float):\n",
        "        \"\"\"\n",
        "        Initializes the Actor network.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): The dimensionality of the state space.\n",
        "            action_dim (int): The dimensionality of the action space.\n",
        "            action_scale (float): The scaling factor to apply to the tanh output.\n",
        "                                  Typically `(high - low) / 2`.\n",
        "            action_bias (float): The bias to apply to the scaled tanh output.\n",
        "                                 Typically `(high + low) / 2`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Define the shared hidden layers of the network.\n",
        "        self.fc1 = nn.Linear(state_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "        # Define the output heads for the mean and log standard deviation.\n",
        "        self.fc_mean = nn.Linear(32, action_dim)\n",
        "        self.fc_log_std = nn.Linear(32, action_dim)\n",
        "\n",
        "        # Store action space scaling and bias as tensors for computation.\n",
        "        self.action_scale = torch.tensor(action_scale, dtype=torch.float32)\n",
        "        self.action_bias = torch.tensor(action_bias, dtype=torch.float32)\n",
        "\n",
        "        # Define constants for clipping the log standard deviation. This is a\n",
        "        # common practice in SAC implementations to prevent numerical instability\n",
        "        # from variances that are too large or too small.\n",
        "        self.LOG_STD_MAX = 2\n",
        "        self.LOG_STD_MIN = -20\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Performs the forward pass to compute the policy distribution parameters.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states. Shape: (batch_size, state_dim).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the mean and\n",
        "                log standard deviation of the policy distribution.\n",
        "        \"\"\"\n",
        "        # Pass the state through the first hidden layer with ReLU activation.\n",
        "        x = F.relu(self.fc1(state))\n",
        "        # Pass through the second hidden layer with ReLU activation.\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Compute the mean of the Gaussian distribution from the final hidden state.\n",
        "        mean = self.fc_mean(x)\n",
        "\n",
        "        # Compute the log standard deviation.\n",
        "        log_std = self.fc_log_std(x)\n",
        "        # Clip the log_std to the predefined range for stability.\n",
        "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Samples an action from the policy and computes its log probability.\n",
        "\n",
        "        This method is central to SAC's training. It uses the reparameterization\n",
        "        trick (`rsample`) to allow gradients to flow back through the sampling\n",
        "        process. It also correctly calculates the log probability of the final,\n",
        "        squashed action, which is essential for both the actor and critic updates.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states. Shape: (batch_size, state_dim).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The sampled and scaled action. Shape: (batch_size, action_dim).\n",
        "                - The log probability of that action. Shape: (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Get the distribution parameters (mean, log_std) from the network's forward pass.\n",
        "        mean, log_std = self.forward(state)\n",
        "        # Exponentiate log_std to get the standard deviation.\n",
        "        std = log_std.exp()\n",
        "\n",
        "        # Create a Normal (Gaussian) distribution object.\n",
        "        normal = Normal(mean, std)\n",
        "\n",
        "        # Sample from the distribution using the reparameterization trick.\n",
        "        # This draws a sample `z ~ N(0,1)` and computes `x = mean + std * z`.\n",
        "        x_t = normal.rsample()\n",
        "\n",
        "        # Apply the tanh squashing function to bound the action to [-1, 1].\n",
        "        y_t = torch.tanh(x_t)\n",
        "\n",
        "        # Scale and shift the squashed action to match the environment's action space.\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "\n",
        "        # --- Log-Probability Correction for Tanh Squashing ---\n",
        "        # The probability density of `action` is different from `x_t` due to the\n",
        "        # non-linear tanh transformation. The change of variables formula requires\n",
        "        # subtracting log|dy/dx| from the original log probability.\n",
        "        # log p(y) = log p(x) - log(1 - tanh(x)^2)\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # The correction term must also account for the action scaling.\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6) # 1e-6 for numerical stability\n",
        "        # Sum the log probabilities across the action dimensions (if action_dim > 1).\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    The SAC Critic (Q-Function) Network.\n",
        "\n",
        "    This network estimates the state-action value function, Q(s, a). It takes a\n",
        "    state and an action as input and outputs a single scalar value representing\n",
        "    the expected cumulative discounted future reward. SAC uses a \"twin critic\"\n",
        "    architecture, meaning two of these networks are trained independently to\n",
        "    mitigate the overestimation bias common in Q-learning methods.\n",
        "\n",
        "    The architecture follows the specification:\n",
        "    Input (state+action) -> Linear(32) -> ReLU -> Linear(32) -> ReLU -> Linear(1)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int):\n",
        "        \"\"\"\n",
        "        Initializes the Critic network.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): The dimensionality of the state space.\n",
        "            action_dim (int): The dimensionality of the action space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # The input to the first layer is the concatenated state and action.\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        # The final layer outputs a single scalar Q-value.\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass to compute the Q-value for a batch of\n",
        "        state-action pairs.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states. Shape: (batch_size, state_dim).\n",
        "            action (torch.Tensor): A batch of actions. Shape: (batch_size, action_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The estimated Q-values. Shape: (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Concatenate the state and action tensors along the feature dimension (dim=1).\n",
        "        x = torch.cat([state, action], 1)\n",
        "\n",
        "        # Pass the combined tensor through the network layers.\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # The final output is the estimated Q-value.\n",
        "        q_value = self.fc3(x)\n",
        "\n",
        "        return q_value\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 2: Experience Replay Buffer\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A simple and efficient replay buffer for off-policy reinforcement learning.\n",
        "\n",
        "    This buffer stores transitions `(s, a, s', r, d)` collected from the\n",
        "    environment. When the agent is updated, it samples a random minibatch of\n",
        "    these transitions to break temporal correlations and improve learning\n",
        "    stability. This implementation uses separate NumPy arrays for each component\n",
        "    of the transition, which is more memory-efficient than storing tuples.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int, max_size: int):\n",
        "        \"\"\"\n",
        "        Initializes the ReplayBuffer.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimensionality of the state space.\n",
        "            action_dim (int): Dimensionality of the action space.\n",
        "            max_size (int): The maximum number of transitions to store.\n",
        "        \"\"\"\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        # Pre-allocate NumPy arrays for storing transition data.\n",
        "        self.state = np.zeros((max_size, state_dim), dtype=np.float32)\n",
        "        self.action = np.zeros((max_size, action_dim), dtype=np.float32)\n",
        "        self.next_state = np.zeros((max_size, state_dim), dtype=np.float32)\n",
        "        self.reward = np.zeros((max_size, 1), dtype=np.float32)\n",
        "        self.done = np.zeros((max_size, 1), dtype=np.float32)\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, next_state: np.ndarray, reward: float, done: bool):\n",
        "        \"\"\"\n",
        "        Adds a new transition to the buffer.\n",
        "\n",
        "        If the buffer is full, this will overwrite the oldest transition.\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): The state observation.\n",
        "            action (np.ndarray): The action taken.\n",
        "            next_state (np.ndarray): The resulting state observation.\n",
        "            reward (float): The reward received.\n",
        "            done (bool): A flag indicating if the episode terminated.\n",
        "        \"\"\"\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.done[self.ptr] = done\n",
        "\n",
        "        # Update the pointer, wrapping around to the beginning if the end is reached.\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        # Update the current size of the buffer, capping at max_size.\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:\n",
        "        \"\"\"\n",
        "        Samples a random minibatch of transitions from the buffer.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): The number of transitions to sample.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, ...]: A tuple of PyTorch tensors for states,\n",
        "                actions, next_states, rewards, and done flags.\n",
        "        \"\"\"\n",
        "        # Generate random integer indices for sampling.\n",
        "        ind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        # Return the batch as a tuple of PyTorch FloatTensors.\n",
        "        return (\n",
        "            torch.FloatTensor(self.state[ind]),\n",
        "            torch.FloatTensor(self.action[ind]),\n",
        "            torch.FloatTensor(self.next_state[ind]),\n",
        "            torch.FloatTensor(self.reward[ind]),\n",
        "            torch.FloatTensor(self.done[ind])\n",
        "        )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 3: The SAC Agent\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class SAC_Agent:\n",
        "    \"\"\"\n",
        "    The main Soft Actor-Critic (SAC) agent class.\n",
        "\n",
        "    This class orchestrates the entire SAC algorithm. It holds the actor and\n",
        "    critic networks (and their target counterparts), the optimizers, the replay\n",
        "    buffer, and all relevant hyperparameters. It provides the main interface\n",
        "    for interacting with the environment (`select_action`) and for training\n",
        "    the networks (`update_parameters`).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_space: Any, # gymnasium.spaces.Box\n",
        "        config: Dict[str, Any],\n",
        "        seed: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the SAC_Agent.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimensionality of the state space.\n",
        "            action_space (Any): The Gymnasium action space object, used to\n",
        "                                determine action dimensions and bounds.\n",
        "            config (Dict[str, Any]): The configuration dictionary containing all\n",
        "                                     SAC-specific and general training hyperparameters.\n",
        "            seed (int): The random seed for ensuring reproducibility.\n",
        "        \"\"\"\n",
        "        # Set the random seed for PyTorch (for network initialization) and NumPy\n",
        "        # (for replay buffer sampling) to ensure full reproducibility.\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # --- Hyperparameters ---\n",
        "        # The problem is undiscounted as it's finite-horizon.\n",
        "        self.gamma: float = 1.0\n",
        "        # The interpolation factor for soft target updates.\n",
        "        self.tau: float = config['sac']['target_smoothing_tau']\n",
        "        # The entropy regularization coefficient.\n",
        "        self.alpha: float = config['sac']['entropy_temperature_alpha']\n",
        "        # The size of minibatches sampled from the replay buffer.\n",
        "        self.batch_size: int = config['training_defaults']['batch_size']\n",
        "        # Learning rates for the actor and critic networks.\n",
        "        lr_actor: float = config['training_defaults']['lr_actor']\n",
        "        lr_critic: float = config['training_defaults']['lr_critic']\n",
        "\n",
        "        # --- Action Space Scaling ---\n",
        "        # Calculate scale and bias to map the tanh output [-1, 1] to the env's action space.\n",
        "        action_scale: float = (action_space.high - action_space.low) / 2.0\n",
        "        action_bias: float = (action_space.high + action_space.low) / 2.0\n",
        "\n",
        "        # --- Networks ---\n",
        "        # Initialize the Actor Network and its Adam optimizer.\n",
        "        self.actor = Actor(state_dim, action_space.shape[0], action_scale, action_bias)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "\n",
        "        # Initialize the first Critic Network, its target, and its optimizer.\n",
        "        self.critic_1 = Critic(state_dim, action_space.shape[0])\n",
        "        self.critic_1_target = copy.deepcopy(self.critic_1)\n",
        "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(), lr=lr_critic)\n",
        "\n",
        "        # Initialize the second Critic Network (the \"twin\"), its target, and its optimizer.\n",
        "        self.critic_2 = Critic(state_dim, action_space.shape[0])\n",
        "        self.critic_2_target = copy.deepcopy(self.critic_2)\n",
        "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(), lr=lr_critic)\n",
        "\n",
        "        # --- Replay Buffer ---\n",
        "        # Initialize the experience replay buffer.\n",
        "        self.replay_buffer = ReplayBuffer(state_dim, action_space.shape[0], config['sac']['replay_buffer_size'])\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Selects an action for a given state during environment interaction.\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): The current state observation.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The action to take in the environment.\n",
        "        \"\"\"\n",
        "        # Convert the NumPy state to a PyTorch tensor.\n",
        "        state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
        "\n",
        "        # Use `torch.no_grad()` to disable gradient calculations for performance,\n",
        "        # as we are only performing inference here.\n",
        "        with torch.no_grad():\n",
        "            # Sample an action from the actor's policy.\n",
        "            action, _ = self.actor.sample(state_tensor)\n",
        "\n",
        "        # Convert the action tensor back to a NumPy array for the environment.\n",
        "        return action.cpu().numpy().flatten()\n",
        "\n",
        "    def update_parameters(self):\n",
        "        \"\"\"\n",
        "        Performs a single gradient update step for the actor and critic networks.\n",
        "        \"\"\"\n",
        "        # The agent can only be updated if the replay buffer contains enough\n",
        "        # samples to form at least one minibatch.\n",
        "        if self.replay_buffer.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # --- 1. Sample a minibatch of transitions from the replay buffer ---\n",
        "        state, action, next_state, reward, done = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # --- 2. Compute the Critic Target (y) ---\n",
        "        # All calculations for the target are done without tracking gradients.\n",
        "        with torch.no_grad():\n",
        "            # Sample an action for the next state from the *current* policy.\n",
        "            next_action, next_log_pi = self.actor.sample(next_state)\n",
        "\n",
        "            # Compute the Q-values for the next state-action pair using the *target* critics.\n",
        "            q1_next_target = self.critic_1_target(next_state, next_action)\n",
        "            q2_next_target = self.critic_2_target(next_state, next_action)\n",
        "\n",
        "            # Clipped Double-Q Learning: use the minimum of the two target Q-values.\n",
        "            min_q_next_target = torch.min(q1_next_target, q2_next_target)\n",
        "\n",
        "            # Compute the soft Q-value target, including the entropy term.\n",
        "            # Equation: y = r + gamma * (1 - done) * (min_Q_target - alpha * log_pi)\n",
        "            target_q = reward + (1 - done) * self.gamma * (min_q_next_target - self.alpha * next_log_pi)\n",
        "\n",
        "        # --- 3. Update the Critic Networks ---\n",
        "        # Compute the Q-values for the original state-action pairs from the online critics.\n",
        "        current_q1 = self.critic_1(state, action)\n",
        "        current_q2 = self.critic_2(state, action)\n",
        "\n",
        "        # Compute the Mean Squared Error loss for each critic against the shared target.\n",
        "        critic_1_loss = F.mse_loss(current_q1, target_q)\n",
        "        critic_2_loss = F.mse_loss(current_q2, target_q)\n",
        "\n",
        "        # Perform the gradient descent step for the first critic.\n",
        "        self.critic_1_optimizer.zero_grad()\n",
        "        critic_1_loss.backward()\n",
        "        self.critic_1_optimizer.step()\n",
        "\n",
        "        # Perform the gradient descent step for the second critic.\n",
        "        self.critic_2_optimizer.zero_grad()\n",
        "        critic_2_loss.backward()\n",
        "        self.critic_2_optimizer.step()\n",
        "\n",
        "        # --- 4. Update the Actor Network ---\n",
        "        # Sample a new action and its log probability for the actor loss calculation.\n",
        "        pi, log_pi = self.actor.sample(state)\n",
        "\n",
        "        # Compute the Q-values for this new action using the online critics.\n",
        "        q1_pi = self.critic_1(state, pi)\n",
        "        q2_pi = self.critic_2(state, pi)\n",
        "        # Use the minimum of the two Q-values in the actor's objective.\n",
        "        min_q_pi = torch.min(q1_pi, q2_pi)\n",
        "\n",
        "        # Compute the actor's loss function.\n",
        "        # Equation: L_actor = E[alpha * log_pi - min_Q]\n",
        "        # We want to maximize this, so we take the negative for gradient descent.\n",
        "        actor_loss = (self.alpha * log_pi - min_q_pi).mean()\n",
        "\n",
        "        # Perform the gradient descent step for the actor.\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # --- 5. Soft Update the Target Networks ---\n",
        "        # This is a slow-moving average of the online network parameters, which\n",
        "        # stabilizes training.\n",
        "        # Equation: theta_target = tau * theta_online + (1 - tau) * theta_target\n",
        "        for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Normal\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7, Step 2: Proximal Policy Optimization (PPO) Implementation\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 1: On-Policy Trajectory Storage\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class TrajectoryBuffer:\n",
        "    \"\"\"\n",
        "    A buffer for storing trajectories for on-policy algorithms like PPO.\n",
        "\n",
        "    Unlike a replay buffer, this storage is cleared after every policy update.\n",
        "    It collects a batch of full or partial trajectories generated by the\n",
        "    current policy before the learning step occurs.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize lists to store each component of the transitions.\n",
        "        self.states: List[np.ndarray] = []\n",
        "        self.actions: List[np.ndarray] = []\n",
        "        self.log_probs: List[np.ndarray] = []\n",
        "        self.rewards: List[float] = []\n",
        "        self.dones: List[bool] = []\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clears all stored trajectories.\"\"\"\n",
        "        # Use `del` to free the memory.\n",
        "        del self.states[:]\n",
        "        del self.actions[:]\n",
        "        del self.log_probs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.dones[:]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 2: Actor-Critic Network\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    The PPO Actor-Critic Network.\n",
        "\n",
        "    This module contains both the policy (actor) and the value function (critic)\n",
        "    networks. For this problem, they are implemented as separate networks for\n",
        "    clarity, but they could share initial layers in more complex settings.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int):\n",
        "        super().__init__()\n",
        "        # --- Actor Network ---\n",
        "        # Maps state to the parameters of a Gaussian policy.\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim)  # Outputs the mean\n",
        "        )\n",
        "        # A learnable parameter for the log standard deviation of the policy.\n",
        "        # This is often more stable than having the network output it directly.\n",
        "        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
        "\n",
        "        # --- Critic Network ---\n",
        "        # Maps state to a single scalar value estimate V(s).\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Selects an action for a given state during trajectory collection.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): The current state.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing the action, its log probability, and the state value.\n",
        "        \"\"\"\n",
        "        # Get the mean of the action distribution from the actor network.\n",
        "        action_mean = self.actor(state)\n",
        "        # Get the standard deviation from the learnable log_std parameter.\n",
        "        action_std = self.actor_log_std.exp()\n",
        "\n",
        "        # Create the Normal distribution.\n",
        "        dist = Normal(action_mean, action_std)\n",
        "        # Sample an action.\n",
        "        action = dist.sample()\n",
        "        # Get the log probability of the sampled action.\n",
        "        action_log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Get the value estimate from the critic network.\n",
        "        state_value = self.critic(state)\n",
        "\n",
        "        return action, action_log_prob, state_value\n",
        "\n",
        "    def evaluate(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Evaluates a given state-action pair during the update phase.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states.\n",
        "            action (torch.Tensor): A batch of actions taken in those states.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing the log probability of the actions, the state values,\n",
        "            and the entropy of the policy distribution.\n",
        "        \"\"\"\n",
        "        # Get the mean of the action distribution.\n",
        "        action_mean = self.actor(state)\n",
        "        # Get the standard deviation.\n",
        "        action_std = self.actor_log_std.exp().expand_as(action_mean)\n",
        "\n",
        "        # Create the Normal distribution.\n",
        "        dist = Normal(action_mean, action_std)\n",
        "\n",
        "        # Calculate the log probability of the given actions under the current policy.\n",
        "        action_log_prob = dist.log_prob(action).sum(1, keepdim=True)\n",
        "        # Calculate the entropy of the distribution.\n",
        "        dist_entropy = dist.entropy().sum(1, keepdim=True)\n",
        "        # Get the value estimate for the states.\n",
        "        state_value = self.critic(state)\n",
        "\n",
        "        return action_log_prob, state_value, dist_entropy\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 3: The PPO Agent\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class PPO_Agent:\n",
        "    \"\"\"The main Proximal Policy Optimization (PPO) agent class.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        config: Dict[str, Any],\n",
        "        seed: int\n",
        "    ):\n",
        "        # Set random seeds for reproducibility.\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # --- Hyperparameters ---\n",
        "        self.gamma = config['ppo']['discount_gamma']\n",
        "        self.gae_lambda = config['ppo']['gae_lambda']\n",
        "        self.clip_epsilon = config['ppo']['clipping_epsilon']\n",
        "        self.epochs = config['ppo']['optimization_epochs']\n",
        "        self.batch_size = config['training_defaults']['batch_size']\n",
        "        lr = config['training_defaults']['lr_actor'] # Use same LR for both\n",
        "\n",
        "        # --- Networks and Optimizer ---\n",
        "        self.policy = ActorCritic(state_dim, action_dim)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "        # --- Trajectory Storage ---\n",
        "        self.buffer = TrajectoryBuffer()\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Selects an action for interaction and stores transition elements.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
        "            action, log_prob, _ = self.policy.act(state_tensor)\n",
        "\n",
        "        # Store the necessary data in the buffer.\n",
        "        self.buffer.states.append(state)\n",
        "        self.buffer.actions.append(action.cpu().numpy().flatten())\n",
        "        self.buffer.log_probs.append(log_prob.cpu().numpy().flatten())\n",
        "\n",
        "        return action.cpu().numpy().flatten()\n",
        "\n",
        "    def update_parameters(self):\n",
        "        \"\"\"Performs the PPO update over multiple epochs and minibatches.\"\"\"\n",
        "        # --- 1. Compute Advantages and Returns using GAE ---\n",
        "        rewards = np.array(self.buffer.rewards, dtype=np.float32)\n",
        "        dones = np.array(self.buffer.dones, dtype=np.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            states_tensor = torch.FloatTensor(np.array(self.buffer.states))\n",
        "            # Get value estimates for all states in the trajectory.\n",
        "            values = self.policy.critic(states_tensor).cpu().numpy().flatten()\n",
        "\n",
        "        advantages = np.zeros_like(rewards)\n",
        "        last_advantage = 0\n",
        "        # Iterate backwards through the trajectory to compute GAE.\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            # Check if it's a terminal state (or end of partial trajectory).\n",
        "            is_terminal = (t == len(rewards) - 1)\n",
        "            next_value = 0 if is_terminal else values[t + 1]\n",
        "\n",
        "            # Compute the TD error (delta).\n",
        "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
        "            # Compute the advantage using the recursive GAE formula.\n",
        "            advantages[t] = last_advantage = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_advantage\n",
        "\n",
        "        # Compute returns for the critic update.\n",
        "        returns = advantages + values\n",
        "\n",
        "        # --- 2. Prepare Data for Update ---\n",
        "        # Convert all data to tensors.\n",
        "        old_states = torch.FloatTensor(np.array(self.buffer.states))\n",
        "        old_actions = torch.FloatTensor(np.array(self.buffer.actions))\n",
        "        old_log_probs = torch.FloatTensor(np.array(self.buffer.log_probs)).unsqueeze(1)\n",
        "        advantages = torch.FloatTensor(advantages).unsqueeze(1)\n",
        "        returns = torch.FloatTensor(returns).unsqueeze(1)\n",
        "\n",
        "        # Normalize advantages (a standard practice for stability).\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # --- 3. Perform Multi-Epoch Minibatch Updates ---\n",
        "        num_samples = len(old_states)\n",
        "        for _ in range(self.epochs):\n",
        "            # Create a random permutation of indices.\n",
        "            indices = np.random.permutation(num_samples)\n",
        "            # Iterate over minibatches.\n",
        "            for start in range(0, num_samples, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                batch_indices = indices[start:end]\n",
        "\n",
        "                # Get the minibatch data.\n",
        "                mb_states = old_states[batch_indices]\n",
        "                mb_actions = old_actions[batch_indices]\n",
        "                mb_log_probs = old_log_probs[batch_indices]\n",
        "                mb_advantages = advantages[batch_indices]\n",
        "                mb_returns = returns[batch_indices]\n",
        "\n",
        "                # --- 4. Compute Losses ---\n",
        "                # Evaluate the minibatch with the current policy.\n",
        "                log_probs, state_values, dist_entropy = self.policy.evaluate(mb_states, mb_actions)\n",
        "\n",
        "                # Calculate the importance sampling ratio.\n",
        "                ratios = torch.exp(log_probs - mb_log_probs)\n",
        "\n",
        "                # Calculate the two surrogate objectives.\n",
        "                surr1 = ratios * mb_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages\n",
        "\n",
        "                # Actor Loss (Clipped Surrogate Objective).\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Critic Loss (Mean Squared Error).\n",
        "                critic_loss = F.mse_loss(state_values, mb_returns)\n",
        "\n",
        "                # Total Loss (including an entropy bonus for exploration).\n",
        "                # The entropy bonus is subtracted as we are minimizing the loss.\n",
        "                total_loss = actor_loss + 0.5 * critic_loss - 0.01 * dist_entropy.mean()\n",
        "\n",
        "                # --- 5. Perform Gradient Update ---\n",
        "                self.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # --- 6. Clear the buffer for the next trajectory collection phase ---\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7, Step 2: Proximal Policy Optimization (PPO)\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 1: On-Policy Trajectory Storage\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class TrajectoryBuffer:\n",
        "    \"\"\"\n",
        "    A buffer for storing trajectories for on-policy algorithms like PPO.\n",
        "\n",
        "    This data structure is designed specifically for on-policy learning. It\n",
        "    collects a sequence of transitions `(s, a, log_prob, r, done)` generated by\n",
        "    the *current* policy. Unlike an off-policy replay buffer, this buffer is\n",
        "    ephemeral: its contents are used for a single round of policy updates and\n",
        "    then immediately cleared to make way for new trajectories from the updated\n",
        "    policy. This ensures that the agent learns from its most recent experience.\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Initializes the TrajectoryBuffer.\"\"\"\n",
        "        # Initialize empty lists to store each component of the transitions.\n",
        "        self.states: List[np.ndarray] = []\n",
        "        self.actions: List[np.ndarray] = []\n",
        "        self.log_probs: List[np.ndarray] = []\n",
        "        self.rewards: List[float] = []\n",
        "        self.dones: List[bool] = []\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"\n",
        "        Clears all stored trajectories from the buffer.\n",
        "\n",
        "        This method is called after the `update_parameters` step to ensure the\n",
        "        next batch of data is collected with the newly updated policy.\n",
        "        \"\"\"\n",
        "        # Use `del slice` to efficiently clear the lists and release memory.\n",
        "        del self.states[:]\n",
        "        del self.actions[:]\n",
        "        del self.log_probs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.dones[:]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 2: Actor-Critic Network\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    The PPO Actor-Critic Network.\n",
        "\n",
        "    This module encapsulates both the policy network (the actor) and the value\n",
        "    function network (the critic). In PPO, the actor determines which action to\n",
        "    take, and the critic evaluates the quality of the state the agent is in.\n",
        "\n",
        "    - The Actor is a stochastic policy that maps states to a Gaussian\n",
        "      distribution over actions.\n",
        "    - The Critic is a value function that maps states to a single scalar value,\n",
        "      `V(s)`, representing the expected return from that state.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int):\n",
        "        \"\"\"\n",
        "        Initializes the ActorCritic network.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): The dimensionality of the state space.\n",
        "            action_dim (int): The dimensionality of the action space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Actor Network ---\n",
        "        # This network outputs the mean of the Gaussian action distribution.\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim)\n",
        "        )\n",
        "\n",
        "        # --- Learnable Action Standard Deviation ---\n",
        "        # We use a learnable parameter for the log standard deviation of the\n",
        "        # policy. This is a common technique in PPO for ensuring a stable\n",
        "        # variance and exploration level.\n",
        "        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
        "\n",
        "        # --- Critic Network ---\n",
        "        # This network estimates the state-value function V(s).\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Selects an action for a given state during trajectory collection.\n",
        "\n",
        "        This method is used for interacting with the environment. It performs a\n",
        "        forward pass to get the action distribution, samples an action, and\n",
        "        computes its log probability.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): The current state observation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The sampled action.\n",
        "                - The log probability of the sampled action.\n",
        "                - The estimated value of the state from the critic.\n",
        "        \"\"\"\n",
        "        # Get the mean of the action distribution from the actor network.\n",
        "        action_mean = self.actor(state)\n",
        "        # Get the standard deviation by exponentiating the learnable log_std parameter.\n",
        "        action_std = self.actor_log_std.exp()\n",
        "\n",
        "        # Create the Normal (Gaussian) distribution object.\n",
        "        dist = Normal(action_mean, action_std)\n",
        "        # Sample an action from the distribution.\n",
        "        action = dist.sample()\n",
        "        # Get the log probability of the sampled action.\n",
        "        action_log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Get the value estimate for the current state from the critic network.\n",
        "        state_value = self.critic(state)\n",
        "\n",
        "        return action, action_log_prob, state_value\n",
        "\n",
        "    def evaluate(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Evaluates given state-action pairs during the PPO update phase.\n",
        "\n",
        "        This method is used during training. It computes the log probability of\n",
        "        actions that were previously taken, the value of the states where they\n",
        "        were taken, and the entropy of the current policy distribution.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states from the trajectory buffer.\n",
        "            action (torch.Tensor): The corresponding actions taken in those states.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The log probability of the actions under the current policy.\n",
        "                - The estimated values of the states from the critic.\n",
        "                - The entropy of the policy distribution for the given states.\n",
        "        \"\"\"\n",
        "        # Get the mean of the action distribution for the batch of states.\n",
        "        action_mean = self.actor(state)\n",
        "        # Get the standard deviation, expanding it to match the batch size.\n",
        "        action_std = self.actor_log_std.exp().expand_as(action_mean)\n",
        "\n",
        "        # Create the Normal distribution object for the current policy.\n",
        "        dist = Normal(action_mean, action_std)\n",
        "\n",
        "        # Calculate the log probability of the given (old) actions under the current policy.\n",
        "        action_log_prob = dist.log_prob(action).sum(1, keepdim=True)\n",
        "        # Calculate the entropy of the policy distribution.\n",
        "        dist_entropy = dist.entropy().sum(1, keepdim=True)\n",
        "        # Get the value estimate for the states from the critic.\n",
        "        state_value = self.critic(state)\n",
        "\n",
        "        return action_log_prob, state_value, dist_entropy\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 3: The PPO Agent\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class PPO_Agent:\n",
        "    \"\"\"\n",
        "    The main Proximal Policy Optimization (PPO) agent class.\n",
        "\n",
        "    This class orchestrates the PPO learning algorithm. It manages the\n",
        "    actor-critic network, the optimizer, and the on-policy trajectory buffer.\n",
        "    Its main responsibilities are to select actions for environment interaction\n",
        "    and to perform the PPO update step using the collected data.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        config: Dict[str, Any],\n",
        "        seed: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the PPO_Agent.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimensionality of the state space.\n",
        "            action_dim (int): Dimensionality of the action space.\n",
        "            config (Dict[str, Any]): Configuration dictionary with PPO hyperparameters.\n",
        "            seed (int): The random seed for ensuring reproducibility.\n",
        "        \"\"\"\n",
        "        # Set random seeds for PyTorch and NumPy for full reproducibility.\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # --- Hyperparameters ---\n",
        "        self.gamma: float = config['ppo']['discount_gamma']\n",
        "        self.gae_lambda: float = config['ppo']['gae_lambda']\n",
        "        self.clip_epsilon: float = config['ppo']['clipping_epsilon']\n",
        "        self.epochs: int = config['ppo']['optimization_epochs']\n",
        "        self.batch_size: int = config['training_defaults']['batch_size']\n",
        "        lr: float = config['training_defaults']['lr_actor'] # Use the same LR for actor and critic\n",
        "\n",
        "        # --- Networks and Optimizer ---\n",
        "        # Initialize the combined Actor-Critic network.\n",
        "        self.policy = ActorCritic(state_dim, action_dim)\n",
        "        # Initialize the Adam optimizer for all parameters in the ActorCritic network.\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "        # --- Trajectory Storage ---\n",
        "        # Initialize the on-policy buffer.\n",
        "        self.buffer = TrajectoryBuffer()\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Selects an action for interaction and stores necessary transition data.\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): The current state observation from the environment.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The action to take in the environment.\n",
        "        \"\"\"\n",
        "        # Use `torch.no_grad()` as we are only performing inference, not training.\n",
        "        with torch.no_grad():\n",
        "            # Convert the NumPy state to a PyTorch tensor.\n",
        "            state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
        "            # Get the action and its log probability from the policy network.\n",
        "            action, log_prob, _ = self.policy.act(state_tensor)\n",
        "\n",
        "        # Store the state, action, and log_prob in the on-policy buffer.\n",
        "        self.buffer.states.append(state)\n",
        "        self.buffer.actions.append(action.cpu().numpy().flatten())\n",
        "        self.buffer.log_probs.append(log_prob.cpu().numpy().flatten())\n",
        "\n",
        "        # Return the action as a NumPy array for the environment.\n",
        "        return action.cpu().numpy().flatten()\n",
        "\n",
        "    def update_parameters(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs the PPO update using data from the trajectory buffer.\n",
        "\n",
        "        This is the core learning step, involving GAE computation and multiple\n",
        "        epochs of minibatch updates using the clipped surrogate objective.\n",
        "        \"\"\"\n",
        "        # --- 1. Compute Advantages and Returns using GAE ---\n",
        "        # Convert lists of rewards and dones from the buffer to NumPy arrays.\n",
        "        rewards = np.array(self.buffer.rewards, dtype=np.float32)\n",
        "        dones = np.array(self.buffer.dones, dtype=np.float32)\n",
        "\n",
        "        # Get value estimates for all states in the trajectory in a single batch.\n",
        "        with torch.no_grad():\n",
        "            states_tensor = torch.FloatTensor(np.array(self.buffer.states))\n",
        "            values = self.policy.critic(states_tensor).cpu().numpy().flatten()\n",
        "\n",
        "        # Initialize an array to store the computed advantages.\n",
        "        advantages = np.zeros_like(rewards)\n",
        "        last_advantage = 0.0\n",
        "        # Iterate backwards through the trajectory to compute GAE.\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            # Determine the value of the next state. If it's a terminal state, the value is 0.\n",
        "            next_value = values[t + 1] if t < len(rewards) - 1 else 0.0\n",
        "\n",
        "            # Compute the TD error (delta): δ_t = r_t + γ * V(s_{t+1}) * (1 - done_t) - V(s_t)\n",
        "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
        "            # Compute the advantage using the recursive GAE formula: A_t = δ_t + γ * λ * A_{t+1} * (1 - done_t)\n",
        "            advantages[t] = last_advantage = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_advantage\n",
        "\n",
        "        # Compute returns for the critic update: R_t = A_t + V(s_t)\n",
        "        returns = advantages + values\n",
        "\n",
        "        # --- 2. Prepare Data for Update ---\n",
        "        # Convert all collected and computed data into PyTorch tensors.\n",
        "        old_states = torch.FloatTensor(np.array(self.buffer.states))\n",
        "        old_actions = torch.FloatTensor(np.array(self.buffer.actions))\n",
        "        old_log_probs = torch.FloatTensor(np.array(self.buffer.log_probs)).unsqueeze(1)\n",
        "        advantages = torch.FloatTensor(advantages).unsqueeze(1)\n",
        "        returns = torch.FloatTensor(returns).unsqueeze(1)\n",
        "\n",
        "        # Normalize advantages (a standard and crucial practice for PPO stability).\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # --- 3. Perform Multi-Epoch Minibatch Updates ---\n",
        "        num_samples = len(old_states)\n",
        "        for _ in range(self.epochs):\n",
        "            # Create a random permutation of indices for shuffling the data.\n",
        "            indices = np.random.permutation(num_samples)\n",
        "            # Iterate over the data in minibatches.\n",
        "            for start in range(0, num_samples, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                batch_indices = indices[start:end]\n",
        "\n",
        "                # Extract the minibatch data using the shuffled indices.\n",
        "                mb_states, mb_actions, mb_log_probs = old_states[batch_indices], old_actions[batch_indices], old_log_probs[batch_indices]\n",
        "                mb_advantages, mb_returns = advantages[batch_indices], returns[batch_indices]\n",
        "\n",
        "                # --- 4. Compute Losses ---\n",
        "                # Evaluate the minibatch with the current (updated) policy.\n",
        "                log_probs, state_values, dist_entropy = self.policy.evaluate(mb_states, mb_actions)\n",
        "\n",
        "                # Calculate the importance sampling ratio: r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)\n",
        "                ratios = torch.exp(log_probs - mb_log_probs)\n",
        "\n",
        "                # Calculate the two surrogate objectives for the PPO clip loss.\n",
        "                surr1 = ratios * mb_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages\n",
        "\n",
        "                # Actor Loss: The negative of the minimum of the two surrogate objectives.\n",
        "                # L_CLIP = E[min(r_t * A_t, clip(r_t, 1-ε, 1+ε) * A_t)]\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Critic Loss: Mean Squared Error between the predicted values and the computed returns.\n",
        "                critic_loss = F.mse_loss(state_values, mb_returns)\n",
        "\n",
        "                # Total Loss: A weighted sum of actor loss, critic loss, and an entropy bonus.\n",
        "                # The entropy bonus encourages exploration and is subtracted as we are minimizing the loss.\n",
        "                total_loss = actor_loss + 0.5 * critic_loss - 0.01 * dist_entropy.mean()\n",
        "\n",
        "                # --- 5. Perform Gradient Update ---\n",
        "                self.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # --- 6. Clear the on-policy buffer for the next trajectory collection phase ---\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7, Step 3: DDPG Implementation - Professional Grade\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 1: Neural Network Architectures\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class DDPG_Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    The DDPG Actor (Deterministic Policy) Network.\n",
        "\n",
        "    This network implements a deterministic policy, meaning it maps a state\n",
        "    observation directly to a single, specific action, rather than to a\n",
        "    distribution over actions. A `tanh` activation function is used on the\n",
        "    output layer to bound the action to the range [-1, 1]. This output is then\n",
        "    linearly transformed (scaled and shifted) to match the environment's\n",
        "    specific action space bounds.\n",
        "\n",
        "    The architecture follows the specification:\n",
        "    Input -> Linear(32) -> ReLU -> Linear(32) -> ReLU -> Linear(action_dim) -> Tanh\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int, action_scale: float, action_bias: float):\n",
        "        \"\"\"\n",
        "        Initializes the DDPG Actor network.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): The dimensionality of the state space.\n",
        "            action_dim (int): The dimensionality of the action space.\n",
        "            action_scale (float): The scaling factor to apply to the tanh output.\n",
        "                                  Calculated as `(high - low) / 2`.\n",
        "            action_bias (float): The bias/offset to apply to the scaled tanh output.\n",
        "                                 Calculated as `(high + low) / 2`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Define the network layers as a sequential module.\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Tanh()  # Tanh activation to bound the output to [-1, 1].\n",
        "        )\n",
        "\n",
        "        # Store action space scaling and bias as non-learnable tensors (buffers).\n",
        "        self.register_buffer('action_scale', torch.tensor(action_scale, dtype=torch.float32))\n",
        "        self.register_buffer('action_bias', torch.tensor(action_bias, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass to compute the deterministic action.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states. Shape: (batch_size, state_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The deterministic actions for the given states, scaled\n",
        "                          to the environment's action space.\n",
        "        \"\"\"\n",
        "        # Pass the state through the network to get the tanh-bounded action.\n",
        "        tanh_action = self.network(state)\n",
        "\n",
        "        # Scale and shift the action to match the environment's action space.\n",
        "        return tanh_action * self.action_scale + self.action_bias\n",
        "\n",
        "class DDPG_Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    The DDPG Critic (Q-Function) Network.\n",
        "\n",
        "    This network estimates the state-action value function, Q(s, a). It takes a\n",
        "    state and an action as input and outputs a single scalar Q-value, representing\n",
        "    the expected cumulative future reward.\n",
        "\n",
        "    The architecture follows the specification:\n",
        "    Input (state+action) -> Linear(32) -> ReLU -> Linear(32) -> ReLU -> Linear(1)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int):\n",
        "        \"\"\"\n",
        "        Initializes the DDPG Critic network.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): The dimensionality of the state space.\n",
        "            action_dim (int): The dimensionality of the action space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # The input to the first layer is the concatenated state and action.\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        # The final layer outputs a single scalar Q-value.\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass to compute the Q-value for a batch of\n",
        "        state-action pairs.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): A batch of states.\n",
        "            action (torch.Tensor): A batch of actions.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The estimated Q-values.\n",
        "        \"\"\"\n",
        "        # Concatenate the state and action tensors along the feature dimension (dim=1).\n",
        "        x = torch.cat([state, action], 1)\n",
        "\n",
        "        # Pass the combined tensor through the network layers.\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # The final output is the estimated Q-value.\n",
        "        q_value = self.fc3(x)\n",
        "\n",
        "        return q_value\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Component 2: The DDPG Agent\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class DDPG_Agent:\n",
        "    \"\"\"\n",
        "    The main Deep Deterministic Policy Gradient (DDPG) agent class.\n",
        "\n",
        "    This class orchestrates the DDPG algorithm. It manages the actor and critic\n",
        "    networks, their corresponding target networks, optimizers, the replay buffer,\n",
        "    and the exploration noise process. It provides the main interface for\n",
        "    interacting with the environment (`select_action`) and for training the\n",
        "    networks (`update_parameters`).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_space: Any, # gymnasium.spaces.Box\n",
        "        config: Dict[str, Any],\n",
        "        seed: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the DDPG_Agent.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimensionality of the state space.\n",
        "            action_space (Any): The Gymnasium action space object, used for\n",
        "                                action scaling and clipping.\n",
        "            config (Dict[str, Any]): Configuration dictionary with DDPG hyperparameters.\n",
        "            seed (int): The random seed for ensuring reproducibility of network\n",
        "                        initialization and exploration noise.\n",
        "        \"\"\"\n",
        "        # Set random seeds for PyTorch and NumPy for full reproducibility.\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # --- Hyperparameters ---\n",
        "        # The problem is undiscounted (finite-horizon).\n",
        "        self.gamma: float = 1.0\n",
        "        # The interpolation factor for soft target network updates.\n",
        "        self.tau: float = config['ddpg']['target_smoothing_tau']\n",
        "        # The size of minibatches sampled from the replay buffer.\n",
        "        self.batch_size: int = config['training_defaults']['batch_size']\n",
        "        # Learning rates for the actor and critic networks.\n",
        "        lr_actor: float = config['training_defaults']['lr_actor']\n",
        "        lr_critic: float = config['training_defaults']['lr_critic']\n",
        "\n",
        "        # --- Exploration Noise ---\n",
        "        # The standard deviation of the Gaussian noise added for exploration.\n",
        "        self.noise_std: float = config['ddpg']['exploration_noise_std']\n",
        "        # The multiplicative decay factor for the noise standard deviation.\n",
        "        self.noise_decay: float = config['ddpg']['exploration_noise_decay']\n",
        "        # Store action space bounds for clipping.\n",
        "        self.action_low: np.ndarray = action_space.low\n",
        "        self.action_high: np.ndarray = action_space.high\n",
        "\n",
        "        # --- Action Space Scaling ---\n",
        "        # Calculate scale and bias to map the actor's tanh output to the env's action space.\n",
        "        action_scale: float = (action_space.high - action_space.low) / 2.0\n",
        "        action_bias: float = (action_space.high + action_space.low) / 2.0\n",
        "\n",
        "        # --- Networks ---\n",
        "        # Initialize the Actor Network, its target network, and its Adam optimizer.\n",
        "        self.actor = DDPG_Actor(state_dim, action_space.shape[0], action_scale, action_bias)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "\n",
        "        # Initialize the Critic Network, its target network, and its Adam optimizer.\n",
        "        self.critic = DDPG_Critic(state_dim, action_space.shape[0])\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
        "\n",
        "        # --- Replay Buffer ---\n",
        "        # Initialize the experience replay buffer.\n",
        "        self.replay_buffer = ReplayBuffer(state_dim, action_space.shape[0], config['ddpg']['replay_buffer_size'])\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Selects an action using the deterministic policy plus exploration noise.\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): The current state observation from the environment.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The final, clipped action to take in the environment.\n",
        "        \"\"\"\n",
        "        # Get the deterministic action from the actor network.\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
        "            action = self.actor(state_tensor).cpu().numpy().flatten()\n",
        "\n",
        "        # --- Add Exploration Noise ---\n",
        "        # Add zero-mean Gaussian noise to the deterministic action to encourage exploration.\n",
        "        noise = np.random.normal(0, self.noise_std, size=action.shape)\n",
        "        noisy_action = action + noise\n",
        "\n",
        "        # --- Clip Action ---\n",
        "        # Clip the final action to ensure it remains within the valid environment bounds.\n",
        "        return np.clip(noisy_action, self.action_low, self.action_high)\n",
        "\n",
        "    def end_of_episode_update(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs tasks at the end of an episode, primarily decaying exploration noise.\n",
        "        \"\"\"\n",
        "        # Decay the exploration noise standard deviation multiplicatively.\n",
        "        self.noise_std *= self.noise_decay\n",
        "\n",
        "    def update_parameters(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs a single gradient update step for the actor and critic networks.\n",
        "        \"\"\"\n",
        "        # The agent can only be updated if the replay buffer has enough samples.\n",
        "        if self.replay_buffer.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # --- 1. Sample a minibatch of transitions from the replay buffer ---\n",
        "        state, action, next_state, reward, done = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # --- 2. Compute the Critic Target (y) ---\n",
        "        # All calculations for the target are performed without tracking gradients.\n",
        "        with torch.no_grad():\n",
        "            # Get the next action from the *target actor* network.\n",
        "            next_action = self.actor_target(next_state)\n",
        "            # Get the Q-value for the next state-action pair from the *target critic* network.\n",
        "            target_q_next = self.critic_target(next_state, next_action)\n",
        "\n",
        "            # Compute the TD target for the critic update.\n",
        "            # Equation: y = r + gamma * (1 - done) * Q_target(s', mu_target(s'))\n",
        "            target_q = reward + (1 - done) * self.gamma * target_q_next\n",
        "\n",
        "        # --- 3. Update the Critic Network ---\n",
        "        # Get the current Q-value estimate from the online critic for the sampled transitions.\n",
        "        current_q = self.critic(state, action)\n",
        "\n",
        "        # Compute the Mean Squared Error loss between the current Q-values and the TD target.\n",
        "        critic_loss = F.mse_loss(current_q, target_q)\n",
        "\n",
        "        # Perform the gradient descent step for the critic.\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # --- 4. Update the Actor Network (Deterministic Policy Gradient) ---\n",
        "        # Get the actions for the batch states from the *online actor*.\n",
        "        # These actions must remain attached to the computation graph.\n",
        "        actor_actions = self.actor(state)\n",
        "        # Calculate the critic's Q-value for these state-action pairs.\n",
        "        q_for_actor_loss = self.critic(state, actor_actions)\n",
        "\n",
        "        # The actor loss is the negative mean of these Q-values. By minimizing this\n",
        "        # loss, we are performing gradient ascent on the Q-function.\n",
        "        # Equation: L_actor = -E[Q(s, mu(s))]\n",
        "        actor_loss = -q_for_actor_loss.mean()\n",
        "\n",
        "        # Perform the gradient descent step for the actor. The gradient flows\n",
        "        # from the critic's output back through the actor's parameters via the chain rule.\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # --- 5. Soft Update the Target Networks ---\n",
        "        # This is a slow-moving average of the online network parameters, which\n",
        "        # is a key technique for stabilizing DDPG training.\n",
        "        # Equation: theta_target = tau * theta_online + (1 - tau) * theta_target\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
      ],
      "metadata": {
        "id": "bSxZgKTBeKql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Experimental Execution Pipeline\n",
        "\n",
        "# =============================================================================\n",
        "# Task 8: Experimental Execution Pipeline - Professional Grade\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1 (Helper): Agent Initialization Factory\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def _initialize_all_agents_for_run(\n",
        "    run_id: int,\n",
        "    config: Dict[str, Any],\n",
        "    seed_table: pd.DataFrame,\n",
        "    alm_rl_initial_table: pd.DataFrame,\n",
        "    baselines_initial_table: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Factory function to initialize all 7 agents for a specific run.\n",
        "\n",
        "    This function is a critical setup step that encapsulates all initialization\n",
        "    logic. It ensures each agent is instantiated with its unique, reproducible\n",
        "    seed and correct starting parameters as defined in the pre-generated data\n",
        "    tables for the given `run_id`. This isolation of initialization logic\n",
        "    improves modularity and simplifies the main execution worker.\n",
        "\n",
        "    Args:\n",
        "        run_id (int): The ID of the run for which to initialize agents.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        seed_table (pd.DataFrame): Table of all seeds for each run and algorithm.\n",
        "        alm_rl_initial_table (pd.DataFrame): Initial parameters for the ALM-RL agent.\n",
        "        baselines_initial_table (pd.DataFrame): Initial parameters for baseline agents.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary mapping algorithm names to their\n",
        "                        initialized agent instances.\n",
        "    \"\"\"\n",
        "    # Extract run-specific seeds and general configuration parameters.\n",
        "    run_seeds = seed_table.loc[run_id].to_dict()\n",
        "    num_episodes = config['EXPERIMENT_META']['num_episodes_per_run']\n",
        "\n",
        "    # Initialize the dictionary to hold all agent instances for this run.\n",
        "    agents: Dict[str, Any] = {}\n",
        "\n",
        "    # --- ALM-RL Agent Initialization ---\n",
        "    agents['ALM_RL'] = ALM_RL_Agent(\n",
        "        initial_params=alm_rl_initial_table.loc[run_id].to_dict(),\n",
        "        config=config['ALM_RL_CONFIG'],\n",
        "        num_episodes=num_episodes,\n",
        "        seed=int(run_seeds['seed_alm_rl'])\n",
        "    )\n",
        "\n",
        "    # --- Traditional Baselines Initialization ---\n",
        "    agents['DCPPI'] = DCPPI_Agent(\n",
        "        initial_m=baselines_initial_table.loc[run_id, 'm0_dcppi'],\n",
        "        config=config['BASELINES_CONFIG']['dcppi'],\n",
        "        num_episodes=num_episodes\n",
        "    )\n",
        "    agents['ACS'] = ACS_Agent(\n",
        "        initial_m=baselines_initial_table.loc[run_id, 'm0_acs'],\n",
        "        tolerance_delta=baselines_initial_table.loc[run_id, 'tolerance_delta'],\n",
        "        config=config['BASELINES_CONFIG']['acs'],\n",
        "        num_episodes=num_episodes\n",
        "    )\n",
        "\n",
        "    # --- Model-Based Baseline Initialization ---\n",
        "    agents['MBP'] = MBP_Agent(\n",
        "        config=config['BASELINES_CONFIG']['mbp'],\n",
        "        delta_t=config['TIME_AND_PENALTIES']['delta_t'],\n",
        "        seed=int(run_seeds['seed_mbp'])\n",
        "    )\n",
        "\n",
        "    # --- Deep RL Baselines Initialization ---\n",
        "    env_spec = config['DEEP_RL_ENV_SPEC']\n",
        "    state_dim = env_spec['observation_space']['shape'][0]\n",
        "    # A temporary, lightweight environment is created solely to provide the\n",
        "    # action_space object required by the Deep RL agent constructors.\n",
        "    temp_env = ALMEnv(market_params={'A':0,'B':1,'C':0,'D':0}, env_spec=env_spec)\n",
        "\n",
        "    agents['SAC'] = SAC_Agent(\n",
        "        state_dim=state_dim,\n",
        "        action_space=temp_env.action_space,\n",
        "        config=config['DEEP_RL_ARCH_AND_TRAINING'],\n",
        "        seed=int(run_seeds['seed_sac'])\n",
        "    )\n",
        "    agents['PPO'] = PPO_Agent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=temp_env.action_space.shape[0],\n",
        "        config=config['DEEP_RL_ARCH_AND_TRAINING'],\n",
        "        seed=int(run_seeds['seed_ppo'])\n",
        "    )\n",
        "    agents['DDPG'] = DDPG_Agent(\n",
        "        state_dim=state_dim,\n",
        "        action_space=temp_env.action_space,\n",
        "        config=config['DEEP_RL_ARCH_AND_TRAINING'],\n",
        "        seed=int(run_seeds['seed_ddpg'])\n",
        "    )\n",
        "    # The temporary environment is no longer needed and is closed.\n",
        "    temp_env.close()\n",
        "\n",
        "    return agents\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2 (Helper): Universal Episode Runner\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def _run_episode_and_update(\n",
        "    agent: Any,\n",
        "    market_params: Dict[str, float],\n",
        "    config: Dict[str, Any],\n",
        "    sde_rng: np.random.Generator,\n",
        "    deep_rl_env: Optional[gym.Env] = None,\n",
        "    episode_seed: Optional[int] = None\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Runs a single, complete episode for any given agent, handling dispatch logic.\n",
        "\n",
        "    This universal function is the core workhorse of the simulation. It abstracts\n",
        "    the agent-environment interaction loop and correctly dispatches to the\n",
        "    appropriate simulation logic (custom SDE loop vs. Gym loop) and agent\n",
        "    update methods based on the type of agent provided.\n",
        "\n",
        "    Args:\n",
        "        agent: The agent instance to run (e.g., `ALM_RL_Agent`, `SAC_Agent`).\n",
        "        market_params: The SDE parameters {A, B, C, D} for the environment.\n",
        "        config: The main study configuration dictionary.\n",
        "        sde_rng: The seeded RNG for the SDE's Brownian motion (for non-Gym agents).\n",
        "        deep_rl_env: The Gymnasium environment instance (required for Deep RL agents).\n",
        "        episode_seed: The seed for the Gym environment's reset method.\n",
        "\n",
        "    Returns:\n",
        "        float: The total reward accumulated during the episode.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    total_reward = 0.0\n",
        "    K = config['TIME_AND_PENALTIES']['K']\n",
        "    delta_t = config['TIME_AND_PENALTIES']['delta_t']\n",
        "    Q, H = config['TIME_AND_PENALTIES']['Q'], config['TIME_AND_PENALTIES']['H']\n",
        "    A, B, C, D = market_params['A'], market_params['B'], market_params['C'], market_params['D']\n",
        "\n",
        "    # --- Dispatch based on agent type ---\n",
        "    is_deep_rl = isinstance(agent, (SAC_Agent, PPO_Agent, DDPG_Agent))\n",
        "\n",
        "    if is_deep_rl:\n",
        "        # --- Deep RL Agent Episode Loop (using Gymnasium Env) ---\n",
        "        if deep_rl_env is None or episode_seed is None:\n",
        "            raise ValueError(\"Deep RL agents require a Gym environment and episode seed.\")\n",
        "\n",
        "        state, _ = deep_rl_env.reset(seed=episode_seed)\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = deep_rl_env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Agent-specific data handling and intra-episode updates.\n",
        "            if isinstance(agent, (SAC_Agent, DDPG_Agent)):\n",
        "                agent.replay_buffer.add(state, action, next_state, reward, done)\n",
        "                agent.update_parameters() # Off-policy agents can update at any step.\n",
        "            elif isinstance(agent, PPO_Agent):\n",
        "                agent.buffer.rewards.append(reward)\n",
        "                agent.buffer.dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "    else:\n",
        "        # --- Non-Deep RL Agent Episode Loop (custom SDE simulation) ---\n",
        "        x = config['ALM_RL_CONFIG']['initialization']['x0']\n",
        "        sqrt_delta_t = np.sqrt(delta_t)\n",
        "        trajectory_for_update = []\n",
        "\n",
        "        for _ in range(K):\n",
        "            action = agent.select_action(x)\n",
        "            total_reward += -0.5 * Q * (x ** 2) * delta_t\n",
        "\n",
        "            # SDE simulation step using the provided, seeded RNG.\n",
        "            drift = (A * x + B * action) * delta_t\n",
        "            diffusion = (C * x + D * action) * sqrt_delta_t * sde_rng.standard_normal()\n",
        "            next_x = x + drift + diffusion\n",
        "\n",
        "            # Agent-specific data collection for end-of-episode update.\n",
        "            if isinstance(agent, MBP_Agent):\n",
        "                agent.observe_transition(x, action, next_x)\n",
        "            else: # ALM-RL, DCPPI, ACS\n",
        "                trajectory_for_update.append((x, action))\n",
        "\n",
        "            x = next_x\n",
        "\n",
        "        total_reward += -0.5 * H * (x ** 2)\n",
        "\n",
        "        # Trigger agent-specific end-of-episode updates.\n",
        "        if isinstance(agent, ALM_RL_Agent):\n",
        "            agent.update_parameters(trajectory_for_update, x, Q, delta_t)\n",
        "        elif isinstance(agent, (DCPPI_Agent, ACS_Agent)):\n",
        "            states_for_update = [s for s, a in trajectory_for_update] + [x]\n",
        "            agent.update_multiplier(states_for_update)\n",
        "        elif isinstance(agent, MBP_Agent):\n",
        "            agent.end_of_episode_update()\n",
        "\n",
        "    # --- Final End-of-Episode Logic ---\n",
        "    # Handle agents that have specific logic after an episode concludes.\n",
        "    if isinstance(agent, DDPG_Agent):\n",
        "        agent.end_of_episode_update() # Decays exploration noise.\n",
        "    elif isinstance(agent, PPO_Agent):\n",
        "        agent.update_parameters() # PPO performs its update on the full trajectory.\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1 & 3: Main Worker Function for a Single Independent Run\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def execute_single_run_worker(\n",
        "    run_id: int,\n",
        "    config: Dict[str, Any],\n",
        "    market_params_table: pd.DataFrame,\n",
        "    seed_table: pd.DataFrame,\n",
        "    alm_rl_initial_table: pd.DataFrame,\n",
        "    baselines_initial_table: pd.DataFrame\n",
        ") -> Tuple[int, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Executes a full simulation run (all episodes) for all algorithms for a\n",
        "    single `run_id`. This function is designed to be the target for a parallel\n",
        "    worker process, making it a self-contained unit of work.\n",
        "\n",
        "    Args:\n",
        "        run_id (int): The ID of the independent run to execute (from 0 to 199).\n",
        "        (other args are the main configuration and data tables required for setup)\n",
        "\n",
        "    Returns:\n",
        "        Tuple[int, Dict[str, np.ndarray]]: A tuple containing the `run_id` and\n",
        "            a dictionary mapping algorithm names to their full episode reward arrays.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialization ---\n",
        "    market_params = market_params_table.loc[run_id].to_dict()\n",
        "    base_seed = int(seed_table.loc[run_id, 'base_seed'])\n",
        "    num_episodes = config['EXPERIMENT_META']['num_episodes_per_run']\n",
        "\n",
        "    # Instantiate all 7 agents for this specific run.\n",
        "    agents = _initialize_all_agents_for_run(\n",
        "        run_id, config, seed_table, alm_rl_initial_table, baselines_initial_table\n",
        "    )\n",
        "\n",
        "    # Prepare a dictionary to store the results for this run.\n",
        "    results = {name: np.zeros(num_episodes, dtype=np.float64) for name in agents}\n",
        "\n",
        "    # --- 2. Environment Setup ---\n",
        "    # Create a single Gym Env instance to be shared by all Deep RL agents in this run.\n",
        "    deep_rl_env = ALMEnv(market_params=market_params, env_spec=config['DEEP_RL_ENV_SPEC'])\n",
        "\n",
        "    # --- 3. Main Simulation Loop ---\n",
        "    for agent_name, agent in agents.items():\n",
        "        # --- RIGOROUS SEEDING FOR FAIRNESS ---\n",
        "        # For each agent, we create a new SDE random number generator seeded with\n",
        "        # the run's base_seed. This is the critical step that guarantees the\n",
        "        # underlying market noise `dW(t)` is identical for every agent.\n",
        "        sde_rng = np.random.Generator(np.random.PCG64(base_seed))\n",
        "\n",
        "        for episode_n in range(num_episodes):\n",
        "            # The seed for the Gym environment's reset method is also derived\n",
        "            # deterministically from the base_seed to ensure the same noise sequence.\n",
        "            episode_seed = base_seed + episode_n\n",
        "\n",
        "            # Run one full episode and get the total reward.\n",
        "            episode_reward = _run_episode_and_update(\n",
        "                agent=agent,\n",
        "                market_params=market_params,\n",
        "                config=config,\n",
        "                sde_rng=sde_rng,\n",
        "                deep_rl_env=deep_rl_env,\n",
        "                episode_seed=episode_seed\n",
        "            )\n",
        "            # Store the result.\n",
        "            results[agent_name][episode_n] = episode_reward\n",
        "\n",
        "    # Clean up the environment instance.\n",
        "    deep_rl_env.close()\n",
        "\n",
        "    # Return the run_id and the collected results.\n",
        "    return run_id, results\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Orchestrator\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def execute_full_experiment(\n",
        "    config: Dict[str, Any],\n",
        "    market_params_table: pd.DataFrame,\n",
        "    seed_table: pd.DataFrame,\n",
        "    alm_rl_initial_table: pd.DataFrame,\n",
        "    baselines_initial_table: pd.DataFrame,\n",
        "    num_workers: int = 4\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire experimental pipeline, running all simulations in parallel.\n",
        "\n",
        "    This top-level function manages the execution of all 200 independent runs.\n",
        "    It distributes the work across multiple CPU cores for efficiency, initializes\n",
        "    a memory-efficient storage solution for the results, and gathers the results\n",
        "    from all worker processes.\n",
        "\n",
        "    Args:\n",
        "        (all inputs are the data structures generated by previous tasks)\n",
        "        num_workers (int): The number of parallel worker processes to use.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 3D array of shape (num_runs, num_algorithms, num_episodes)\n",
        "                    containing all episode rewards from the experiment.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialize Settings and Results Storage ---\n",
        "    num_runs = config['EXPERIMENT_META']['num_independent_runs']\n",
        "    num_episodes = config['EXPERIMENT_META']['num_episodes_per_run']\n",
        "\n",
        "    # Define a canonical order for algorithms for consistent indexing into the results array.\n",
        "    algorithm_order = ['ALM_RL', 'DCPPI', 'ACS', 'MBP', 'SAC', 'PPO', 'DDPG']\n",
        "    num_algorithms = len(algorithm_order)\n",
        "    alg_to_idx = {name: i for i, name in enumerate(algorithm_order)}\n",
        "\n",
        "    # Initialize the final results array. For this scale (~224MB), an in-memory\n",
        "    # NumPy array is sufficient and simpler than a memory-mapped file.\n",
        "    all_results = np.zeros((num_runs, num_algorithms, num_episodes), dtype=np.float64)\n",
        "\n",
        "    # --- 2. Prepare Arguments for Parallel Workers ---\n",
        "    # Create a list of argument tuples, one for each call to the worker function.\n",
        "    tasks = []\n",
        "    for run_id in range(num_runs):\n",
        "        tasks.append((\n",
        "            run_id, config, market_params_table, seed_table,\n",
        "            alm_rl_initial_table, baselines_initial_table\n",
        "        ))\n",
        "\n",
        "    # --- 3. Execute All Runs in Parallel ---\n",
        "    print(f\"Starting experimental execution with {num_workers} workers...\")\n",
        "    # Use a multiprocessing Pool to manage the worker processes.\n",
        "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
        "        # Use tqdm to create a progress bar that tracks the completion of runs.\n",
        "        with tqdm(total=num_runs, desc=\"Executing Runs\") as pbar:\n",
        "            # `starmap` distributes the tasks to the workers and collects results.\n",
        "            for run_id, run_results in pool.starmap(execute_single_run_worker, tasks):\n",
        "                # This loop processes results as they are returned by the workers.\n",
        "                # Store the results from the completed run into the correct slice\n",
        "                # of the main results array.\n",
        "                for agent_name, rewards in run_results.items():\n",
        "                    if agent_name in alg_to_idx:\n",
        "                        alg_idx = alg_to_idx[agent_name]\n",
        "                        all_results[run_id, alg_idx, :] = rewards\n",
        "                # Update the progress bar.\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(\"Experimental execution complete.\")\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "T7a343Y-kSMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Performance Metrics Computation and Data Collection\n",
        "\n",
        "# =============================================================================\n",
        "# Task 9: Performance Metrics Computation and Data Collection\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Raw Data Validation\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def validate_raw_results(\n",
        "    all_results: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs integrity checks on the raw experimental results array.\n",
        "\n",
        "    This function acts as a critical assertion point, ensuring that the raw data\n",
        "    from the simulation pipeline is structurally sound and plausible before any\n",
        "    further analysis is performed.\n",
        "\n",
        "    Args:\n",
        "        all_results (np.ndarray): The 3D array of raw episode rewards from the\n",
        "                                  experiment, with shape (num_runs, num_algorithms,\n",
        "                                  num_episodes).\n",
        "        config (Dict[str, Any]): The main study configuration dictionary, used\n",
        "                                 to verify array dimensions.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the array has an incorrect shape, contains invalid\n",
        "                    numerical values (NaN, inf), or has rewards greater than zero.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(all_results, np.ndarray):\n",
        "        raise TypeError(f\"all_results must be a NumPy array, but got {type(all_results)}.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(f\"config must be a dict, but got {type(config)}.\")\n",
        "\n",
        "    # --- 1. Shape Validation ---\n",
        "    # Verify that the array dimensions match the experiment's configuration.\n",
        "    num_runs = config['EXPERIMENT_META']['num_independent_runs']\n",
        "    num_episodes = config['EXPERIMENT_META']['num_episodes_per_run']\n",
        "    # The number of algorithms is derived from the canonical order.\n",
        "    num_algorithms = len(['ALM_RL', 'DCPPI', 'ACS', 'MBP', 'SAC', 'PPO', 'DDPG'])\n",
        "    expected_shape = (num_runs, num_algorithms, num_episodes)\n",
        "    if all_results.shape != expected_shape:\n",
        "        raise ValueError(f\"Results array has incorrect shape. Expected {expected_shape}, \"\n",
        "                         f\"but got {all_results.shape}.\")\n",
        "\n",
        "    # --- 2. Numerical Stability Validation ---\n",
        "    # Check for any NaN values, which indicate a numerical error during simulation.\n",
        "    if np.isnan(all_results).any():\n",
        "        raise ValueError(\"Results array contains NaN values. Simulation was unstable.\")\n",
        "    # Check for any infinity values, another sign of numerical instability.\n",
        "    if np.isinf(all_results).any():\n",
        "        raise ValueError(\"Results array contains infinity values. Simulation was unstable.\")\n",
        "\n",
        "    # --- 3. Reward Plausibility Validation ---\n",
        "    # The objective is a quadratic cost, so all rewards must be non-positive.\n",
        "    if (all_results > 1e-9).any(): # Use a small tolerance for floating point\n",
        "        raise ValueError(\"Results array contains positive reward values, which is \"\n",
        "                         \"inconsistent with the cost-based objective function.\")\n",
        "\n",
        "    print(\"Raw results validation successful: Shape, stability, and values are correct.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Moving Average Smoothing\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def compute_smoothed_learning_curves(\n",
        "    all_results: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    algorithm_order: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes smoothed learning curves and interquartile ranges for all algorithms.\n",
        "\n",
        "    This function processes the raw 3D results array by:\n",
        "    1. Transforming it into a long-form pandas DataFrame.\n",
        "    2. Applying a centered rolling mean to each individual run's reward series.\n",
        "    3. Aggregating across all runs to compute the mean, 25th, and 75th\n",
        "       percentiles of the smoothed rewards for each algorithm at each episode.\n",
        "\n",
        "    Args:\n",
        "        all_results (np.ndarray): The 3D array of raw episode rewards.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        algorithm_order (List[str]): The canonical list of algorithm names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a MultiIndex (algorithm, episode) and\n",
        "                      columns ['mean', 'q25', 'q75'], ready for plotting.\n",
        "    \"\"\"\n",
        "    # --- 1. Data Transformation to Long-Form DataFrame ---\n",
        "    # This format is ideal for pandas operations like groupby and rolling.\n",
        "    num_runs, num_algorithms, num_episodes = all_results.shape\n",
        "\n",
        "    # Create indices for runs, algorithms, and episodes.\n",
        "    run_ids = np.arange(num_runs)\n",
        "    alg_ids = np.arange(num_algorithms)\n",
        "    episode_ids = np.arange(num_episodes)\n",
        "\n",
        "    # Create a meshgrid to align indices with the flattened data.\n",
        "    mesh_run, mesh_alg, mesh_episode = np.meshgrid(run_ids, alg_ids, episode_ids, indexing='ij')\n",
        "\n",
        "    # Create the long-form DataFrame.\n",
        "    df_long = pd.DataFrame({\n",
        "        'run': mesh_run.flatten(),\n",
        "        'algorithm_idx': mesh_alg.flatten(),\n",
        "        'episode': mesh_episode.flatten(),\n",
        "        'reward': all_results.flatten()\n",
        "    })\n",
        "    # Map algorithm indices to their string names for clarity.\n",
        "    df_long['algorithm'] = df_long['algorithm_idx'].map(dict(enumerate(algorithm_order)))\n",
        "\n",
        "    # --- 2. Apply Rolling Mean Smoothing ---\n",
        "    # Get the smoothing window size from the configuration.\n",
        "    window_size = config['EVALUATION_SETTINGS']['smoothing']['moving_average_window']\n",
        "\n",
        "    # Group by each individual experimental run and algorithm.\n",
        "    # Then, apply a centered rolling mean to the 'reward' series for each group.\n",
        "    # `min_periods=1` ensures the window is adaptive at the series boundaries.\n",
        "    df_long['smoothed_reward'] = df_long.groupby(['run', 'algorithm'])['reward'].transform(\n",
        "        lambda x: x.rolling(window=window_size, center=True, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "    # --- 3. Aggregate Across Runs ---\n",
        "    # Now, group by algorithm and episode to compute statistics over the 200 runs.\n",
        "    grouped = df_long.groupby(['algorithm', 'episode'])['smoothed_reward']\n",
        "\n",
        "    # Compute the mean and the 25th/75th percentiles.\n",
        "    learning_curves = pd.DataFrame({\n",
        "        'mean': grouped.mean(),\n",
        "        'q25': grouped.quantile(0.25),\n",
        "        'q75': grouped.quantile(0.75)\n",
        "    })\n",
        "\n",
        "    return learning_curves\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Terminal Performance Extraction\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def extract_terminal_performance(\n",
        "    all_results: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    algorithm_order: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the terminal performance for each run and algorithm.\n",
        "\n",
        "    Terminal performance is defined as the average reward over a specified\n",
        "    final window of episodes. This reduces noise from the last few episodes\n",
        "    and provides a stable measure of converged performance for statistical testing.\n",
        "\n",
        "    Args:\n",
        "        all_results (np.ndarray): The 3D array of raw episode rewards.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        algorithm_order (List[str]): The canonical list of algorithm names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of shape (num_runs, num_algorithms) where each\n",
        "                      cell contains the mean terminal performance.\n",
        "    \"\"\"\n",
        "    # --- 1. Parameter Extraction ---\n",
        "    # Get the size of the terminal window from the configuration.\n",
        "    terminal_window = config['EVALUATION_SETTINGS']['significance_testing']['terminal_window']\n",
        "\n",
        "    # --- 2. Slicing and Aggregation ---\n",
        "    # Select the last `terminal_window` episodes for all runs and algorithms.\n",
        "    # The slice `:, :, -terminal_window:]` is efficient and precise.\n",
        "    terminal_rewards = all_results[:, :, -terminal_window:]\n",
        "\n",
        "    # Compute the mean over the last axis (the episode dimension).\n",
        "    # This collapses the 3D array into a 2D array of terminal performances.\n",
        "    mean_terminal_performance = np.mean(terminal_rewards, axis=2)\n",
        "\n",
        "    # --- 3. DataFrame Creation ---\n",
        "    # Convert the 2D NumPy array into a pandas DataFrame for clarity and ease of use.\n",
        "    df_terminal = pd.DataFrame(\n",
        "        mean_terminal_performance,\n",
        "        columns=algorithm_order,\n",
        "        index=pd.Index(range(all_results.shape[0]), name='run_id')\n",
        "    )\n",
        "\n",
        "    return df_terminal\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Orchestrator for Task 9\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def process_performance_metrics(\n",
        "    all_results: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full performance metrics computation pipeline.\n",
        "\n",
        "    This function takes the raw experimental results and produces the two key\n",
        "    data structures required for all subsequent analysis and visualization:\n",
        "    the smoothed learning curves and the terminal performance data.\n",
        "\n",
        "    Args:\n",
        "        all_results (np.ndarray): The 3D array of raw episode rewards from Task 8.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "            - `learning_curves`: DataFrame of smoothed statistics for plotting.\n",
        "            - `terminal_performance`: DataFrame of terminal performance for testing.\n",
        "    \"\"\"\n",
        "    # --- 1. Validate Raw Data ---\n",
        "    # It is critical to ensure the input data is sound before processing.\n",
        "    validate_raw_results(all_results, config)\n",
        "\n",
        "    # --- 2. Define Canonical Algorithm Order ---\n",
        "    # This ensures consistent indexing and labeling throughout the analysis.\n",
        "    algorithm_order = ['ALM_RL', 'DCPPI', 'ACS', 'MBP', 'SAC', 'PPO', 'DDPG']\n",
        "\n",
        "    # --- 3. Compute Smoothed Learning Curves ---\n",
        "    # This function handles the complex smoothing and aggregation for Figure 1.\n",
        "    learning_curves = compute_smoothed_learning_curves(all_results, config, algorithm_order)\n",
        "    print(\"Smoothed learning curves computed successfully.\")\n",
        "\n",
        "    # --- 4. Extract Terminal Performance ---\n",
        "    # This function extracts the data needed for the statistical tests in Figure 2.\n",
        "    terminal_performance = extract_terminal_performance(all_results, config, algorithm_order)\n",
        "    print(\"Terminal performance extracted successfully.\")\n",
        "\n",
        "    return learning_curves, terminal_performance\n"
      ],
      "metadata": {
        "id": "CasFnFj9nEeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Statistical Analysis and Significance Testing\n",
        "\n",
        "# =============================================================================\n",
        "# Task 10: Statistical Analysis and Significance Testing\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1 (Helper): Wilcoxon Signed-Rank Test Function\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def _perform_one_sided_wilcoxon_test(\n",
        "    perf_a: np.ndarray,\n",
        "    perf_b: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Performs a one-sided Wilcoxon signed-rank test.\n",
        "\n",
        "    This test is used to determine if the median of the differences between two\n",
        "    paired samples is greater than zero. It is a non-parametric test, suitable\n",
        "    when the distribution of differences cannot be assumed to be normal.\n",
        "\n",
        "    Hypothesis Test:\n",
        "    - H0: The median of (perf_a - perf_b) is less than or equal to 0.\n",
        "    - H1: The median of (perf_a - perf_b) is greater than 0. (i.e., A outperforms B)\n",
        "\n",
        "    Args:\n",
        "        perf_a (np.ndarray): The performance scores for algorithm A (e.g., rows).\n",
        "        perf_b (np.ndarray): The performance scores for algorithm B (e.g., columns).\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated p-value of the test.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if perf_a.shape != perf_b.shape:\n",
        "        raise ValueError(\"Input arrays must have the same shape.\")\n",
        "\n",
        "    # --- Calculate Differences ---\n",
        "    # The test is performed on the paired differences between the two samples.\n",
        "    differences = perf_a - perf_b\n",
        "\n",
        "    # --- Handle Edge Case: No Difference ---\n",
        "    # If all differences are zero, the test statistic is undefined. The p-value\n",
        "    # in this case is 1, as there is no evidence to reject the null hypothesis.\n",
        "    if np.all(np.isclose(differences, 0)):\n",
        "        return 1.0\n",
        "\n",
        "    # --- Perform Statistical Test ---\n",
        "    # Suppress warnings that scipy may raise for ties or zero-differences,\n",
        "    # as these are handled by the test itself.\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", UserWarning)\n",
        "        # `alternative='greater'` specifies the one-sided test H1: median > 0.\n",
        "        _, p_value = wilcoxon(x=differences, alternative='greater')\n",
        "\n",
        "    return p_value\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: P-Value Matrix Construction\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def construct_p_value_matrix(\n",
        "    terminal_performance: pd.DataFrame,\n",
        "    algorithm_order: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the pairwise p-value matrix for all algorithms.\n",
        "\n",
        "    This function systematically performs a one-sided Wilcoxon signed-rank test\n",
        "    for every ordered pair of algorithms. The resulting matrix entry `P[i, j]`\n",
        "    contains the p-value for the hypothesis that algorithm `i` (row)\n",
        "    outperforms algorithm `j` (column).\n",
        "\n",
        "    Args:\n",
        "        terminal_performance (pd.DataFrame): A DataFrame of shape\n",
        "            (num_runs, num_algorithms) containing the terminal performance scores.\n",
        "        algorithm_order (List[str]): The canonical list of algorithm names,\n",
        "            defining the order of rows and columns in the output matrix.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A square DataFrame of shape (num_algorithms, num_algorithms)\n",
        "                      containing the pairwise p-values.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialization ---\n",
        "    # Initialize an empty DataFrame with the algorithm names as both index and columns.\n",
        "    num_algorithms = len(algorithm_order)\n",
        "    p_value_matrix = pd.DataFrame(\n",
        "        np.ones((num_algorithms, num_algorithms)),\n",
        "        index=algorithm_order,\n",
        "        columns=algorithm_order\n",
        "    )\n",
        "\n",
        "    # --- 2. Pairwise Test Execution ---\n",
        "    # Use itertools.product to iterate through all (row, column) pairs.\n",
        "    for row_alg, col_alg in product(algorithm_order, repeat=2):\n",
        "        # --- Self-Comparison ---\n",
        "        # An algorithm cannot outperform itself; the p-value is 1.\n",
        "        if row_alg == col_alg:\n",
        "            continue\n",
        "\n",
        "        # --- Extract Paired Data ---\n",
        "        # Get the performance vectors for the two algorithms being compared.\n",
        "        perf_row = terminal_performance[row_alg].values\n",
        "        perf_col = terminal_performance[col_alg].values\n",
        "\n",
        "        # --- Perform Test and Store Result ---\n",
        "        # Perform the one-sided test: H1 is that `row_alg` outperforms `col_alg`.\n",
        "        p_value = _perform_one_sided_wilcoxon_test(perf_row, perf_col)\n",
        "        # Store the p-value in the matrix.\n",
        "        p_value_matrix.loc[row_alg, col_alg] = p_value\n",
        "\n",
        "    return p_value_matrix\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3 (Enhancement): Effect Size and Multiple Comparison Correction\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def _compute_cliffs_delta(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes Cliff's Delta, a non-parametric measure of effect size.\n",
        "\n",
        "    Cliff's Delta measures the degree of overlap between two distributions. It\n",
        "    is defined as the probability that a randomly selected value from the first\n",
        "    distribution (`x`) is greater than a randomly selected value from the second\n",
        "    distribution (`y`), minus the reverse probability. The value ranges from -1\n",
        "    (all values in `y` are greater than all values in `x`) to +1 (all values\n",
        "    in `x` are greater than all values in `y`). A value of 0 indicates complete\n",
        "    overlap.\n",
        "\n",
        "    This measure is particularly useful as a complement to p-values from tests\n",
        "    like Wilcoxon, as it quantifies the *magnitude* of the difference, not just\n",
        "    its statistical significance.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): The first sample of data.\n",
        "        y (np.ndarray): The second sample of data.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated Cliff's Delta value, ranging from -1 to 1.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n",
        "        raise TypeError(\"Input samples x and y must be NumPy arrays.\")\n",
        "    if x.ndim != 1 or y.ndim != 1:\n",
        "        raise ValueError(\"Input samples x and y must be 1-dimensional arrays.\")\n",
        "\n",
        "    # --- Effect Size Calculation ---\n",
        "    # This vectorized operation creates a matrix of all pairwise comparisons.\n",
        "    # `x[:, None]` reshapes x into a column vector, enabling broadcasting\n",
        "    # against the row vector y. The result is a matrix where `M[i, j] = x[i] - y[j]`.\n",
        "    comparisons = np.sign(x[:, None] - y)\n",
        "\n",
        "    # Equation: δ = ( #(x_i > y_j) - #(x_i < y_j) ) / (n * m)\n",
        "    # The mean of the sign matrix is mathematically equivalent to this formula.\n",
        "    # `np.sign` returns +1 for `x > y`, -1 for `x < y`, and 0 for `x = y`.\n",
        "    # The mean of these values gives the desired effect size.\n",
        "    return np.mean(comparisons)\n",
        "\n",
        "def compute_effect_size_matrix(\n",
        "    terminal_performance: pd.DataFrame,\n",
        "    algorithm_order: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a matrix of Cliff's Delta effect sizes for all algorithm pairs.\n",
        "\n",
        "    This function systematically computes the effect size for every ordered pair\n",
        "    of algorithms, providing a quantitative measure of the magnitude of\n",
        "    performance differences. The resulting matrix entry `D[i, j]` contains the\n",
        "    Cliff's Delta for algorithm `i` (row) versus algorithm `j` (column).\n",
        "\n",
        "    A positive value `D[i, j]` indicates that algorithm `i` tends to have higher\n",
        "    performance scores than algorithm `j`.\n",
        "\n",
        "    Args:\n",
        "        terminal_performance (pd.DataFrame): A DataFrame of shape\n",
        "            (num_runs, num_algorithms) containing the terminal performance scores.\n",
        "        algorithm_order (List[str]): The canonical list of algorithm names,\n",
        "            defining the order of rows and columns in the output matrix.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A square DataFrame of shape (num_algorithms, num_algorithms)\n",
        "                      containing the pairwise Cliff's Delta effect sizes.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(terminal_performance, pd.DataFrame):\n",
        "        raise TypeError(\"terminal_performance must be a pandas DataFrame.\")\n",
        "    if not all(col in terminal_performance.columns for col in algorithm_order):\n",
        "        raise ValueError(\"All names in algorithm_order must exist as columns in terminal_performance.\")\n",
        "\n",
        "    # --- 1. Initialization ---\n",
        "    # Initialize an empty DataFrame filled with zeros, with algorithm names\n",
        "    # as both the index and columns for clear labeling.\n",
        "    num_algorithms = len(algorithm_order)\n",
        "    effect_size_matrix = pd.DataFrame(\n",
        "        np.zeros((num_algorithms, num_algorithms)),\n",
        "        index=algorithm_order,\n",
        "        columns=algorithm_order,\n",
        "        dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # --- 2. Pairwise Effect Size Calculation ---\n",
        "    # Use itertools.product to efficiently iterate through all (row, column) pairs.\n",
        "    for row_alg, col_alg in product(algorithm_order, repeat=2):\n",
        "        # The effect size of an algorithm against itself is trivially zero.\n",
        "        if row_alg == col_alg:\n",
        "            continue\n",
        "\n",
        "        # Extract the performance vectors for the two algorithms being compared.\n",
        "        perf_row = terminal_performance[row_alg].values\n",
        "        perf_col = terminal_performance[col_alg].values\n",
        "\n",
        "        # Compute the Cliff's Delta effect size for this pair.\n",
        "        delta = _compute_cliffs_delta(perf_row, perf_col)\n",
        "\n",
        "        # Store the calculated effect size in the matrix.\n",
        "        effect_size_matrix.loc[row_alg, col_alg] = delta\n",
        "\n",
        "    return effect_size_matrix\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Orchestrator for Task 10\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def perform_statistical_analysis(\n",
        "    terminal_performance: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full statistical analysis pipeline.\n",
        "\n",
        "    This function takes the terminal performance data and produces the key\n",
        "    statistical artifacts for the study: the p-value matrix for significance\n",
        "    testing and a corresponding matrix of effect sizes for interpreting the\n",
        "    magnitude of the differences.\n",
        "\n",
        "    Args:\n",
        "        terminal_performance (pd.DataFrame): DataFrame of terminal performance\n",
        "            scores from Task 9.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "            - `p_value_matrix`: DataFrame of pairwise p-values.\n",
        "            - `effect_size_matrix`: DataFrame of pairwise Cliff's Delta effect sizes.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(terminal_performance, pd.DataFrame) or terminal_performance.empty:\n",
        "        raise ValueError(\"terminal_performance must be a non-empty pandas DataFrame.\")\n",
        "\n",
        "    # --- 2. Define Canonical Algorithm Order ---\n",
        "    # This must match the order used in previous and subsequent steps.\n",
        "    algorithm_order = list(terminal_performance.columns)\n",
        "\n",
        "    # --- 3. Construct P-Value Matrix ---\n",
        "    # This function performs the core hypothesis testing.\n",
        "    print(\"Constructing p-value matrix using Wilcoxon signed-rank tests...\")\n",
        "    p_value_matrix = construct_p_value_matrix(terminal_performance, algorithm_order)\n",
        "    print(\"P-value matrix constructed successfully.\")\n",
        "\n",
        "    # --- 4. Compute Effect Size Matrix (for added rigor) ---\n",
        "    # This provides context to the p-values by quantifying the magnitude of differences.\n",
        "    print(\"Computing effect size matrix (Cliff's Delta)...\")\n",
        "    effect_size_matrix = compute_effect_size_matrix(terminal_performance, algorithm_order)\n",
        "    print(\"Effect size matrix computed successfully.\")\n",
        "\n",
        "    # Note: Multiple comparison correction could be added here as a further step,\n",
        "    # but for reproducing the paper, the raw p-value matrix is the primary output.\n",
        "\n",
        "    return p_value_matrix, effect_size_matrix\n"
      ],
      "metadata": {
        "id": "qURLKWcMxpgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Visualization and Results Generation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 11: Visualization and Results Generation\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Learning Curves Figure Generation\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def plot_learning_curves(\n",
        "    learning_curves: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    save_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates and saves a publication-quality plot of the learning curves.\n",
        "\n",
        "    This function visualizes the performance of all algorithms over the training\n",
        "    episodes. It plots the mean smoothed reward across all runs and shades the\n",
        "    area representing the interquartile range (25th to 75th percentile),\n",
        "    faithfully reproducing the style of Figure 1 in the paper.\n",
        "\n",
        "    Args:\n",
        "        learning_curves (pd.DataFrame): A DataFrame with a MultiIndex\n",
        "            (algorithm, episode) and columns ['mean', 'q25', 'q75'].\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        save_path (str): The file path (e.g., 'learning_curves.png') where the\n",
        "                         plot will be saved.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup Plot Style and Figure ---\n",
        "    # Set a professional, clean plot style.\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    # Create the figure and axes objects with a specified size and DPI for high quality.\n",
        "    fig, ax = plt.subplots(figsize=(12, 7), dpi=300)\n",
        "\n",
        "    # --- 2. Define Plotting Order and Colors ---\n",
        "    # Define a canonical order and color palette for consistency.\n",
        "    algorithm_order = ['ALM_RL', 'SAC', 'PPO', 'DDPG', 'MBP', 'DCPPI', 'ACS']\n",
        "    # Use a professional, colorblind-friendly palette.\n",
        "    palette = sns.color_palette(\"deep\", n_colors=len(algorithm_order))\n",
        "    color_map = dict(zip(algorithm_order, palette))\n",
        "\n",
        "    # --- 3. Plot Data for Each Algorithm ---\n",
        "    # Iterate through the algorithms in the specified order.\n",
        "    for alg_name in algorithm_order:\n",
        "        # Select the data for the current algorithm.\n",
        "        alg_data = learning_curves.loc[alg_name]\n",
        "\n",
        "        # Plot the mean smoothed reward as a solid line.\n",
        "        ax.plot(\n",
        "            alg_data.index,\n",
        "            alg_data['mean'],\n",
        "            label=alg_name,\n",
        "            color=color_map[alg_name],\n",
        "            linewidth=2\n",
        "        )\n",
        "\n",
        "        # Shade the interquartile range (IQR) between the 25th and 75th percentiles.\n",
        "        ax.fill_between(\n",
        "            alg_data.index,\n",
        "            alg_data['q25'],\n",
        "            alg_data['q75'],\n",
        "            color=color_map[alg_name],\n",
        "            alpha=0.2, # Use a light transparency for the shading.\n",
        "            linewidth=0\n",
        "        )\n",
        "\n",
        "    # --- 4. Finalize Plot Formatting ---\n",
        "    # Set the title and axis labels with appropriate font sizes.\n",
        "    ax.set_title(\"Performance of ALM Strategies over 200 Randomized Scenarios\", fontsize=16, pad=20)\n",
        "    ax.set_xlabel(\"Episodes\", fontsize=12)\n",
        "    ax.set_ylabel(\"Average Reward\", fontsize=12)\n",
        "\n",
        "    # Set the limits for the x-axis to match the experiment's duration.\n",
        "    num_episodes = config['EXPERIMENT_META']['num_episodes_per_run']\n",
        "    ax.set_xlim(0, num_episodes)\n",
        "\n",
        "    # Format axis ticks for clarity.\n",
        "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "\n",
        "    # Add a legend, positioning it in a standard location.\n",
        "    ax.legend(title=\"Algorithm\", fontsize=10, title_fontsize=11, loc='lower right')\n",
        "\n",
        "    # Ensure a tight layout to prevent labels from being cut off.\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # --- 5. Save the Figure ---\n",
        "    # Save the plot to the specified file path.\n",
        "    try:\n",
        "        fig.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Learning curves plot saved successfully to '{save_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving plot: {e}\")\n",
        "\n",
        "    # Close the plot to free up memory.\n",
        "    plt.close(fig)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Statistical Significance Heatmap Generation\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def plot_p_value_heatmap(\n",
        "    p_value_matrix: pd.DataFrame,\n",
        "    save_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates and saves a heatmap of the statistical significance p-values.\n",
        "\n",
        "    This function visualizes the p-value matrix from the pairwise Wilcoxon\n",
        "    tests, reproducing the style of Figure 2 in the paper. Each cell `(i, j)`\n",
        "    shows the p-value for the hypothesis that algorithm `i` (row) outperforms\n",
        "    algorithm `j` (column).\n",
        "\n",
        "    Args:\n",
        "        p_value_matrix (pd.DataFrame): The square DataFrame of pairwise p-values.\n",
        "        save_path (str): The file path (e.g., 'p_value_heatmap.png') where the\n",
        "                         plot will be saved.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup Plot Style and Figure ---\n",
        "    sns.set_theme(style=\"white\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 8), dpi=300)\n",
        "\n",
        "    # --- 2. Generate the Heatmap ---\n",
        "    # Use seaborn's heatmap function for a professional visualization.\n",
        "    sns.heatmap(\n",
        "        p_value_matrix,\n",
        "        ax=ax,\n",
        "        annot=True,          # Display the p-values in each cell.\n",
        "        fmt=\".4f\",           # Format annotations to four decimal places.\n",
        "        cmap=\"Blues_r\",      # Use a reversed blue colormap (darker for lower p-values).\n",
        "        linewidths=0.5,      # Add thin lines between cells.\n",
        "        linecolor='white',   # Use white lines for a clean look.\n",
        "        cbar=True,           # Show the color bar.\n",
        "        cbar_kws={'label': 'p-value'}, # Add a label to the color bar.\n",
        "        vmin=0.0,            # Anchor the color scale at 0.\n",
        "        vmax=1.0             # Anchor the color scale at 1.\n",
        "    )\n",
        "\n",
        "    # --- 3. Finalize Plot Formatting ---\n",
        "    # Set titles and labels.\n",
        "    ax.set_title(\"Pairwise Statistical Significance (One-Sided Wilcoxon Test)\", fontsize=14, pad=20)\n",
        "    ax.set_xlabel(\"Column Algorithm\", fontsize=12)\n",
        "    ax.set_ylabel(\"Row Algorithm (Hypothesized Outperformer)\", fontsize=12)\n",
        "\n",
        "    # Ensure axis labels are not rotated and are centered.\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.tick_params(axis='y', rotation=0)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # --- 4. Save the Figure ---\n",
        "    try:\n",
        "        fig.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"P-value heatmap saved successfully to '{save_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving plot: {e}\")\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Summary Statistics Table Generation\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def generate_summary_statistics_table(\n",
        "    terminal_performance: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a formatted table of summary statistics for terminal performance.\n",
        "\n",
        "    Args:\n",
        "        terminal_performance (pd.DataFrame): DataFrame of terminal performance\n",
        "            scores from Task 9.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A formatted DataFrame containing key descriptive\n",
        "                      statistics for each algorithm, sorted by mean performance.\n",
        "    \"\"\"\n",
        "    # --- 1. Compute Statistics ---\n",
        "    # Use pandas' built-in aggregation functions for efficiency.\n",
        "    summary = terminal_performance.agg(['mean', 'std', 'median', 'min', 'max']).T\n",
        "\n",
        "    # Compute percentiles separately to calculate the IQR.\n",
        "    q25 = terminal_performance.quantile(0.25)\n",
        "    q75 = terminal_performance.quantile(0.75)\n",
        "\n",
        "    # Add IQR to the summary table.\n",
        "    summary['iqr'] = q75 - q25\n",
        "\n",
        "    # --- 2. Formatting and Sorting ---\n",
        "    # Rename columns for clarity in the final report.\n",
        "    summary.columns = [\n",
        "        'Mean', 'Std Dev', 'Median', 'Min', 'Max', 'IQR'\n",
        "    ]\n",
        "\n",
        "    # Sort the table by mean performance in descending order to rank the algorithms.\n",
        "    summary = summary.sort_values(by='Mean', ascending=False)\n",
        "\n",
        "    # Format the numerical values to a consistent number of decimal places.\n",
        "    formatted_summary = summary.style.format({\n",
        "        'Mean': '{:.4f}',\n",
        "        'Std Dev': '{:.4f}',\n",
        "        'Median': '{:.4f}',\n",
        "        'Min': '{:.4f}',\n",
        "        'Max': '{:.4f}',\n",
        "        'IQR': '{:.4f}'\n",
        "    }).set_caption(\"Summary Statistics of Terminal Performance (Average of Last 500 Episodes)\")\n",
        "\n",
        "    print(\"Summary statistics table generated successfully.\")\n",
        "\n",
        "    # Return the styled DataFrame object.\n",
        "    return formatted_summary\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Orchestrator for Task 11\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def generate_all_visualizations_and_tables(\n",
        "    learning_curves: pd.DataFrame,\n",
        "    p_value_matrix: pd.DataFrame,\n",
        "    terminal_performance: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \".\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all final plots and tables for the study.\n",
        "\n",
        "    Args:\n",
        "        learning_curves (pd.DataFrame): The processed learning curve data.\n",
        "        p_value_matrix (pd.DataFrame): The matrix of statistical test results.\n",
        "        terminal_performance (pd.DataFrame): The terminal performance data.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        output_dir (str): The directory where output files will be saved.\n",
        "    \"\"\"\n",
        "    # --- 1. Generate Learning Curves Plot ---\n",
        "    plot_learning_curves(\n",
        "        learning_curves=learning_curves,\n",
        "        config=config,\n",
        "        save_path=f\"{output_dir}/figure1_learning_curves.png\"\n",
        "    )\n",
        "\n",
        "    # --- 2. Generate P-Value Heatmap ---\n",
        "    plot_p_value_heatmap(\n",
        "        p_value_matrix=p_value_matrix,\n",
        "        save_path=f\"{output_dir}/figure2_p_value_heatmap.png\"\n",
        "    )\n",
        "\n",
        "    # --- 3. Generate and Display Summary Table ---\n",
        "    summary_table = generate_summary_statistics_table(\n",
        "        terminal_performance=terminal_performance\n",
        "    )\n",
        "    # Display the table in the console (or save to a file).\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Terminal Performance Summary\")\n",
        "    print(\"=\"*50)\n",
        "    # To display a styled DataFrame, you might need to be in a Jupyter environment.\n",
        "    # In a script, you can save it to HTML or another format.\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(summary_table)\n",
        "    except ImportError:\n",
        "        print(summary_table.data.to_string()) # Print the raw data if not in a rich display env\n",
        "\n",
        "    # Save the table to an HTML file for easy viewing.\n",
        "    summary_table.to_html(f\"{output_dir}/table1_summary_statistics.html\")\n",
        "    print(f\"\\nSummary table saved to '{output_dir}/table1_summary_statistics.html'\")\n"
      ],
      "metadata": {
        "id": "zKIdjg9WzlWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Orchestrator Function Creation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 12: Main Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def run_alm_rl_reproduction_pipeline(\n",
        "    study_params: Dict[str, Any],\n",
        "    output_dir: str = \"alm_rl_reproduction_output\",\n",
        "    num_workers: int = 4\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end reproduction pipeline for the paper.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point to run\n",
        "    the entire experiment. It sequentially executes all necessary tasks, from\n",
        "    initial parameter validation to the final generation of plots and tables,\n",
        "    ensuring a robust, reproducible, and automated workflow.\n",
        "\n",
        "    The pipeline is structured as follows:\n",
        "    1.  **Validation:** Rigorously validates the input configuration.\n",
        "    2.  **Setup:** Generates all necessary seeds and estimates resource needs.\n",
        "    3.  **Initialization:** Creates the specific market scenarios and initial\n",
        "        agent parameters for all 200 independent runs.\n",
        "    4.  **Execution:** Runs the main simulation loop for all 7 algorithms across\n",
        "        all 200 scenarios, leveraging parallel processing for efficiency.\n",
        "    5.  **Processing:** Computes smoothed learning curves and terminal performance\n",
        "        metrics from the raw simulation results.\n",
        "    6.  **Analysis:** Performs statistical significance testing on the terminal\n",
        "        performance data.\n",
        "    7.  **Visualization:** Generates and saves all final figures and tables.\n",
        "\n",
        "    Args:\n",
        "        study_params (Dict[str, Any]): The main configuration dictionary for\n",
        "                                       the entire study (i.e., STUDY_INPUTS).\n",
        "        output_dir (str): The path to the directory where all outputs (data,\n",
        "                          plots, tables) will be saved. The directory will be\n",
        "                          created if it does not exist.\n",
        "        num_workers (int): The number of parallel CPU cores to use for the main\n",
        "                           experimental execution phase.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the key final\n",
        "            DataFrames generated during the analysis: 'learning_curves',\n",
        "            'terminal_performance', and 'p_value_matrix'.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input validation fails at the first step.\n",
        "        Exception: Propagates any exceptions that occur during the pipeline execution.\n",
        "    \"\"\"\n",
        "    # --- 0. Setup and Initialization ---\n",
        "    # Start a timer to measure the total execution time of the pipeline.\n",
        "    start_time = time.time()\n",
        "    print(\"=\"*80)\n",
        "    print(\"Starting ALM-RL Reproduction Pipeline\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the main output directory and subdirectories for organized storage.\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "    (output_path / \"data\").mkdir(exist_ok=True)\n",
        "    (output_path / \"figures\").mkdir(exist_ok=True)\n",
        "    (output_path / \"tables\").mkdir(exist_ok=True)\n",
        "    print(f\"Output will be saved to: {output_path.resolve()}\")\n",
        "\n",
        "    try:\n",
        "        # --- Task 1: Parameter Validation ---\n",
        "        print(\"\\n[TASK 1/7] Validating input study parameters...\")\n",
        "        validate_study_inputs(study_params)\n",
        "        print(\"Validation successful.\")\n",
        "\n",
        "        # --- Task 2: Computational Environment Setup ---\n",
        "        print(\"\\n[TASK 2/7] Setting up computational environment and RNG...\")\n",
        "        seed_table, resource_report = setup_computation_and_rng(study_params)\n",
        "        seed_table.to_csv(output_path / \"data\" / \"seed_table.csv\")\n",
        "        print(\"Seed table generated and saved.\")\n",
        "        print(f\"Resource Estimation: {resource_report['memory_estimation']['optimized_memory_mb_results_only']:.2f} MB required for results.\")\n",
        "\n",
        "        # --- Task 3: Market Parameter and Initial State Generation ---\n",
        "        print(\"\\n[TASK 3/7] Generating initial conditions for all runs...\")\n",
        "        market_params_table, alm_rl_initial_table, baselines_initial_table = generate_initial_conditions(\n",
        "            seed_table=seed_table,\n",
        "            config=study_params\n",
        "        )\n",
        "        market_params_table.to_csv(output_path / \"data\" / \"market_params_table.csv\")\n",
        "        alm_rl_initial_table.to_csv(output_path / \"data\" / \"alm_rl_initial_table.csv\")\n",
        "        baselines_initial_table.to_csv(output_path / \"data\" / \"baselines_initial_table.csv\")\n",
        "        print(\"Market scenarios and initial agent parameters generated and saved.\")\n",
        "\n",
        "        # --- Task 8: Experimental Execution Pipeline ---\n",
        "        # This is the most computationally intensive step.\n",
        "        print(\"\\n[TASK 4-8/7] Executing full experimental pipeline...\")\n",
        "        all_results = execute_full_experiment(\n",
        "            config=study_params,\n",
        "            market_params_table=market_params_table,\n",
        "            seed_table=seed_table,\n",
        "            alm_rl_initial_table=alm_rl_initial_table,\n",
        "            baselines_initial_table=baselines_initial_table,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        # Save the raw results array for potential future analysis.\n",
        "        np.save(output_path / \"data\" / \"raw_results.npy\", all_results)\n",
        "        print(\"Raw experimental results saved.\")\n",
        "\n",
        "        # --- Task 9: Performance Metrics Computation ---\n",
        "        print(\"\\n[TASK 9/7] Processing performance metrics...\")\n",
        "        learning_curves, terminal_performance = process_performance_metrics(\n",
        "            all_results=all_results,\n",
        "            config=study_params\n",
        "        )\n",
        "        learning_curves.to_csv(output_path / \"data\" / \"learning_curves.csv\")\n",
        "        terminal_performance.to_csv(output_path / \"data\" / \"terminal_performance.csv\")\n",
        "        print(\"Smoothed learning curves and terminal performance data generated and saved.\")\n",
        "\n",
        "        # --- Task 10: Statistical Analysis ---\n",
        "        print(\"\\n[TASK 10/7] Performing statistical analysis...\")\n",
        "        p_value_matrix, effect_size_matrix = perform_statistical_analysis(\n",
        "            terminal_performance=terminal_performance,\n",
        "            config=study_params\n",
        "        )\n",
        "        p_value_matrix.to_csv(output_path / \"data\" / \"p_value_matrix.csv\")\n",
        "        effect_size_matrix.to_csv(output_path / \"data\" / \"effect_size_matrix.csv\")\n",
        "        print(\"P-value and effect size matrices generated and saved.\")\n",
        "\n",
        "        # --- Task 11: Visualization and Results Generation ---\n",
        "        print(\"\\n[TASK 11/7] Generating final visualizations and tables...\")\n",
        "        generate_all_visualizations_and_tables(\n",
        "            learning_curves=learning_curves,\n",
        "            p_value_matrix=p_value_matrix,\n",
        "            terminal_performance=terminal_performance,\n",
        "            config=study_params,\n",
        "            output_dir=str(output_path)\n",
        "        )\n",
        "        print(\"All figures and tables generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception during the pipeline, log it, and re-raise.\n",
        "        print(\"\\n\" + \"!\"*80)\n",
        "        print(f\"FATAL ERROR: The pipeline failed with the following exception:\\n{e}\")\n",
        "        print(\"!\"*80)\n",
        "        raise\n",
        "\n",
        "    # --- Completion ---\n",
        "    end_time = time.time()\n",
        "    total_duration = end_time - start_time\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALM-RL Reproduction Pipeline Completed Successfully!\")\n",
        "    print(f\"Total execution time: {total_duration / 60:.2f} minutes.\")\n",
        "    print(f\"All outputs are saved in: {output_path.resolve()}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return the final data artifacts for interactive use.\n",
        "    return {\n",
        "        \"learning_curves\": learning_curves,\n",
        "        \"terminal_performance\": terminal_performance,\n",
        "        \"p_value_matrix\": p_value_matrix,\n",
        "        \"effect_size_matrix\": effect_size_matrix\n",
        "    }\n"
      ],
      "metadata": {
        "id": "e9a8YkQV3UiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Robustness Analysis Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 13: Robustness Analysis Implementation - Professional Grade\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Hyperparameter Sensitivity Analysis\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def run_hyperparameter_sensitivity(\n",
        "    base_config: Dict[str, Any],\n",
        "    output_dir: Path,\n",
        "    num_workers: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a sensitivity analysis on key ALM-RL agent hyperparameters.\n",
        "\n",
        "    This function systematically explores the performance landscape of the\n",
        "    ALM-RL agent by running a grid search over a predefined set of crucial\n",
        "    hyperparameters. For each combination, it executes a reduced-scale\n",
        "    experiment (fewer runs and episodes) to assess the impact on the agent's\n",
        "    mean terminal reward. This analysis helps identify robust parameter ranges\n",
        "    and understand the sensitivity of the algorithm to its configuration.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The baseline STUDY_INPUTS configuration,\n",
        "            which serves as a template for modification.\n",
        "        output_dir (Path): The directory where intermediate data and final\n",
        "                           results for this analysis will be saved.\n",
        "        num_workers (int): The number of parallel worker processes to use for\n",
        "                           executing the experimental runs.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the mean terminal reward for each\n",
        "                      hyperparameter combination tested.\n",
        "    \"\"\"\n",
        "    # Announce the start of the analysis phase.\n",
        "    print(\"\\n--- Starting Hyperparameter Sensitivity Analysis ---\")\n",
        "\n",
        "    # --- 1. Define Hyperparameter Grid ---\n",
        "    # Define the specific hyperparameters and the range of values to test for each.\n",
        "    param_grid = {\n",
        "        'lr_exponent': [-0.6, -0.75, -0.9],\n",
        "        'c_gamma': [0.5, 1.0, 2.0],\n",
        "        'phi2_max': [50.0, 100.0, 200.0]\n",
        "    }\n",
        "\n",
        "    # Generate all unique combinations of the specified hyperparameter values.\n",
        "    combinations = list(itertools.product(*param_grid.values()))\n",
        "    # Initialize a list to store the results of each test run.\n",
        "    results = []\n",
        "\n",
        "    # --- 2. Iterate Through Hyperparameter Combinations ---\n",
        "    for i, combo in enumerate(combinations):\n",
        "        # Unpack the current combination of hyperparameters.\n",
        "        lr_exp, c_gam, p2_max = combo\n",
        "        # Log the current test being executed.\n",
        "        print(f\"\\nRunning sensitivity test {i+1}/{len(combinations)}: \"\n",
        "              f\"lr_exp={lr_exp}, c_gamma={c_gam}, phi2_max={p2_max}\")\n",
        "\n",
        "        # --- 3. Create Modified Configuration for the Test Run ---\n",
        "        # Create a deep copy of the base configuration to prevent side effects between tests.\n",
        "        config = copy.deepcopy(base_config)\n",
        "\n",
        "        # Reduce the scale of the experiment for computational feasibility.\n",
        "        config['EXPERIMENT_META']['num_independent_runs'] = 50\n",
        "        config['EXPERIMENT_META']['num_episodes_per_run'] = 5000\n",
        "\n",
        "        # Inject the current hyperparameter combination into the copied configuration.\n",
        "        config['ALM_RL_CONFIG']['schedules']['learning_rate']['exponent'] = lr_exp\n",
        "        config['ALM_RL_CONFIG']['schedules']['temperature']['c_gamma'] = c_gam\n",
        "        config['ALM_RL_CONFIG']['projection_bounds']['phi2_max'] = p2_max\n",
        "\n",
        "        # --- 4. Run Reduced-Scale Experiment ---\n",
        "        # Use a try-except block to ensure the entire sweep continues even if one run fails.\n",
        "        try:\n",
        "            # Execute the full end-to-end pipeline with the modified configuration.\n",
        "            seed_table, _ = setup_computation_and_rng(config)\n",
        "            market_params, alm_rl_init, base_init = generate_initial_conditions(seed_table, config)\n",
        "            raw_results = execute_full_experiment(\n",
        "                config, market_params, seed_table, alm_rl_init, base_init, num_workers\n",
        "            )\n",
        "            _, terminal_perf = process_performance_metrics(raw_results, config)\n",
        "\n",
        "            # --- 5. Store Result ---\n",
        "            # Extract and store the mean terminal performance of the ALM-RL agent.\n",
        "            alm_rl_perf = terminal_perf['ALM_RL'].mean()\n",
        "            results.append({\n",
        "                'lr_exponent': lr_exp,\n",
        "                'c_gamma': c_gam,\n",
        "                'phi2_max': p2_max,\n",
        "                'mean_terminal_reward': alm_rl_perf\n",
        "            })\n",
        "        except Exception as e:\n",
        "            # If a run fails, log the error and record NaN for the performance.\n",
        "            print(f\"  -> Test failed with error: {e}\")\n",
        "            results.append({\n",
        "                'lr_exponent': lr_exp,\n",
        "                'c_gamma': c_gam,\n",
        "                'phi2_max': p2_max,\n",
        "                'mean_terminal_reward': np.nan\n",
        "            })\n",
        "\n",
        "    # --- 6. Compile, Save, and Return Results ---\n",
        "    # Convert the list of result dictionaries into a pandas DataFrame.\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # Save the results to a CSV file for later analysis and reporting.\n",
        "    results_df.to_csv(output_dir / \"data\" / \"hyperparameter_sensitivity_results.csv\", index=False)\n",
        "    print(\"\\nHyperparameter sensitivity analysis complete.\")\n",
        "    return results_df\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Market Parameter Robustness Testing\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def run_market_robustness_test(\n",
        "    base_config: Dict[str, Any],\n",
        "    output_dir: Path,\n",
        "    num_workers: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Tests agent robustness against extreme market parameter configurations.\n",
        "\n",
        "    This function generates a set of challenging market scenarios by sampling\n",
        "    from extended parameter ranges for the SDE dynamics (A, B, C, D). It uses\n",
        "    Latin Hypercube Sampling (LHS) to ensure a more uniform and efficient\n",
        "    coverage of the high-dimensional parameter space compared to simple random\n",
        "    sampling. It then runs a reduced-scale experiment to assess agent\n",
        "    performance under these more volatile and unpredictable conditions.\n",
        "\n",
        "    Args:\n",
        "        (Same as previous function)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the performance of all agents in\n",
        "                      each of the generated extreme market scenarios.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Market Parameter Robustness Test ---\")\n",
        "\n",
        "    # --- 1. Define Extended SDE Parameter Ranges ---\n",
        "    extended_ranges = {\n",
        "        'A': [-0.1, 0.1],\n",
        "        'B': [0.025, 0.3],\n",
        "        'C': [0.05, 0.4],\n",
        "        'D': [0.05, 0.4]\n",
        "    }\n",
        "\n",
        "    # --- 2. Generate Scenarios with Latin Hypercube Sampling (LHS) ---\n",
        "    num_scenarios = 100\n",
        "    # Initialize the LHS sampler with the correct dimensionality and a seed for reproducibility.\n",
        "    sampler = qmc.LatinHypercube(d=len(extended_ranges), seed=base_config['EXPERIMENT_META']['rng_policy']['master_seed'])\n",
        "    # Generate a sample in the unit hypercube.\n",
        "    sample = sampler.random(n=num_scenarios)\n",
        "\n",
        "    # Scale the unit hypercube sample to the specified extended ranges.\n",
        "    scaled_sample = qmc.scale(sample, [v[0] for v in extended_ranges.values()], [v[1] for v in extended_ranges.values()])\n",
        "\n",
        "    # Create a DataFrame to hold the new, extreme market scenarios.\n",
        "    market_params_extreme = pd.DataFrame(scaled_sample, columns=extended_ranges.keys())\n",
        "    # Calculate the corresponding oracle policy gain for each scenario.\n",
        "    market_params_extreme['phi1_star'] = -(market_params_extreme['B'] + market_params_extreme['C'] * market_params_extreme['D']) / (market_params_extreme['D']**2)\n",
        "    market_params_extreme.index.name = 'run_id'\n",
        "\n",
        "    # --- 3. Run Reduced-Scale Experiment on Extreme Scenarios ---\n",
        "    # Create a deep copy of the base configuration.\n",
        "    config = copy.deepcopy(base_config)\n",
        "    # Adjust the number of runs to match the number of generated scenarios.\n",
        "    config['EXPERIMENT_META']['num_independent_runs'] = num_scenarios\n",
        "    config['EXPERIMENT_META']['num_episodes_per_run'] = 5000 # Use shorter runs for feasibility.\n",
        "\n",
        "    # Execute the full pipeline with the modified config and the new market parameters.\n",
        "    seed_table, _ = setup_computation_and_rng(config)\n",
        "    _, alm_rl_init, base_init = generate_initial_conditions(seed_table, config)\n",
        "    raw_results = execute_full_experiment(\n",
        "        config, market_params_extreme, seed_table, alm_rl_init, base_init, num_workers\n",
        "    )\n",
        "    _, terminal_perf = process_performance_metrics(raw_results, config)\n",
        "\n",
        "    # --- 4. Compile, Save, and Return Results ---\n",
        "    # Combine the input market parameters with the resulting terminal performance.\n",
        "    results_df = market_params_extreme.join(terminal_perf)\n",
        "    # Save the combined results to a CSV file.\n",
        "    results_df.to_csv(output_dir / \"data\" / \"market_robustness_results.csv\")\n",
        "    print(\"\\nMarket parameter robustness test complete.\")\n",
        "    return results_df\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Discretization and Numerical Stability Analysis\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def run_discretization_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    output_dir: Path,\n",
        "    num_workers: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the impact of the SDE discretization step size `delta_t` on performance.\n",
        "\n",
        "    This function runs a series of reduced-scale experiments, each with a\n",
        "    different `delta_t`. This tests the sensitivity of the algorithms to the\n",
        "    coarseness of the discrete-time approximation of the continuous-time\n",
        "    environment. A robust algorithm should show graceful degradation in\n",
        "    performance as `delta_t` increases.\n",
        "\n",
        "    Args:\n",
        "        (Same as previous functions)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the mean terminal performance for\n",
        "                      each algorithm at each tested `delta_t`.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Discretization Analysis ---\")\n",
        "\n",
        "    # --- 1. Define Discretization Steps to Test ---\n",
        "    delta_t_values = [0.005, 0.01, 0.02, 0.05]\n",
        "    results = []\n",
        "\n",
        "    # --- 2. Iterate Through delta_t Values ---\n",
        "    for dt in delta_t_values:\n",
        "        print(f\"\\nRunning discretization test for delta_t = {dt}...\")\n",
        "\n",
        "        # --- 3. Create Modified Configuration ---\n",
        "        config = copy.deepcopy(base_config)\n",
        "        # Set a reduced scale for the experiment.\n",
        "        config['EXPERIMENT_META']['num_independent_runs'] = 50\n",
        "        config['EXPERIMENT_META']['num_episodes_per_run'] = 5000\n",
        "\n",
        "        # Update delta_t and the corresponding episode horizon K = floor(T / dt).\n",
        "        config['TIME_AND_PENALTIES']['delta_t'] = dt\n",
        "        config['TIME_AND_PENALTIES']['K'] = int(np.floor(\n",
        "            config['TIME_AND_PENALTIES']['T'] / dt\n",
        "        ))\n",
        "        # Ensure the Deep RL environment specification is also updated to match.\n",
        "        config['DEEP_RL_ENV_SPEC']['dynamics']['delta_t'] = dt\n",
        "        config['DEEP_RL_ENV_SPEC']['episode_horizon'] = config['TIME_AND_PENALTIES']['K']\n",
        "\n",
        "        # --- 4. Run Reduced-Scale Experiment ---\n",
        "        try:\n",
        "            # Execute the full pipeline with the modified configuration.\n",
        "            seed_table, _ = setup_computation_and_rng(config)\n",
        "            market_params, alm_rl_init, base_init = generate_initial_conditions(seed_table, config)\n",
        "            raw_results = execute_full_experiment(\n",
        "                config, market_params, seed_table, alm_rl_init, base_init, num_workers\n",
        "            )\n",
        "            _, terminal_perf = process_performance_metrics(raw_results, config)\n",
        "\n",
        "            # --- 5. Store Results ---\n",
        "            # Calculate the mean performance for each algorithm under this dt.\n",
        "            perf_summary = terminal_perf.mean().to_dict()\n",
        "            perf_summary['delta_t'] = dt\n",
        "            results.append(perf_summary)\n",
        "        except Exception as e:\n",
        "            # Handle potential failures, e.g., if an algorithm is unstable with a large dt.\n",
        "            print(f\"  -> Test failed with error: {e}\")\n",
        "            # Store NaN results on failure for a complete record.\n",
        "            alg_names = [col.replace('seed_', '').upper() for col in base_config['INITIAL_RAW_DATA']['algorithm_seeds_table']['columns'] if 'seed' in col]\n",
        "            perf_summary = {alg: np.nan for alg in alg_names}\n",
        "            perf_summary['delta_t'] = dt\n",
        "            results.append(perf_summary)\n",
        "\n",
        "    # --- 6. Compile, Save, and Return Results ---\n",
        "    # Convert the list of results into a DataFrame, using delta_t as the index.\n",
        "    results_df = pd.DataFrame(results).set_index('delta_t')\n",
        "    # Save the results to a CSV file.\n",
        "    results_df.to_csv(output_dir / \"data\" / \"discretization_analysis_results.csv\")\n",
        "    print(\"\\nDiscretization analysis complete.\")\n",
        "    return results_df\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Orchestrator for Task 13\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def run_robustness_analysis(\n",
        "    study_params: Dict[str, Any],\n",
        "    output_dir: str = \"alm_rl_robustness_output\",\n",
        "    num_workers: int = 4\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete robustness and sensitivity analysis pipeline.\n",
        "\n",
        "    This master function serves as the single entry point to execute all three\n",
        "    robustness tests: hyperparameter sensitivity, market parameter robustness,\n",
        "    and discretization analysis. It manages the workflow and saves all results\n",
        "    to a structured output directory.\n",
        "\n",
        "    Args:\n",
        "        study_params (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_dir (str): The path to the directory where all outputs will be saved.\n",
        "        num_workers (int): The number of parallel CPU cores to use.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the results DataFrames\n",
        "            from each of the three analysis components.\n",
        "    \"\"\"\n",
        "    # Announce the start of the entire analysis pipeline.\n",
        "    print(\"=\"*80)\n",
        "    print(\"Starting Full Robustness Analysis Pipeline\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Setup Output Directory ---\n",
        "    # Use pathlib for robust path management.\n",
        "    output_path = Path(output_dir)\n",
        "    # Create the main directory and a subdirectory for data files.\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "    (output_path / \"data\").mkdir(exist_ok=True)\n",
        "\n",
        "    # --- Execute Each Analysis Sequentially ---\n",
        "    # Run the hyperparameter sensitivity analysis.\n",
        "    hyperparam_results = run_hyperparameter_sensitivity(study_params, output_path, num_workers)\n",
        "\n",
        "    # Run the market parameter robustness test.\n",
        "    market_robustness_results = run_market_robustness_test(study_params, output_path, num_workers)\n",
        "\n",
        "    # Run the discretization analysis.\n",
        "    discretization_results = run_discretization_analysis(study_params, output_path, num_workers)\n",
        "\n",
        "    # Announce the successful completion of the pipeline.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Robustness Analysis Pipeline Completed Successfully!\")\n",
        "    print(f\"All outputs are saved in: {output_path.resolve()}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return a dictionary containing all the final results DataFrames.\n",
        "    return {\n",
        "        \"hyperparameter_sensitivity\": hyperparam_results,\n",
        "        \"market_robustness\": market_robustness_results,\n",
        "        \"discretization_analysis\": discretization_results\n",
        "    }\n"
      ],
      "metadata": {
        "id": "x620wDHJ5jBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution Script\n",
        "# =============================================================================\n",
        "\n",
        "def main(\n",
        "    study_params: Dict[str, Any],\n",
        "    run_reproduction: bool = True,\n",
        "    run_robustness: bool = True,\n",
        "    num_workers: int = -1\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    The main entry point for the entire ALM-RL research project.\n",
        "\n",
        "    This function orchestrates the two primary phases of the study:\n",
        "    1.  **Reproduction Pipeline:** Executes the full end-to-end experiment as\n",
        "        described in the paper, from data generation to final visualizations,\n",
        "        to reproduce the core results.\n",
        "    2.  **Robustness Analysis:** Executes a series of additional experiments to\n",
        "        test the sensitivity and robustness of the proposed algorithm to changes\n",
        "        in hyperparameters, market conditions, and simulation parameters.\n",
        "\n",
        "    Args:\n",
        "        study_params (Dict[str, Any]): The main configuration dictionary for\n",
        "                                       the entire study (i.e., STUDY_INPUTS).\n",
        "        run_reproduction (bool): If True, runs the main reproduction pipeline.\n",
        "        run_robustness (bool): If True, runs the robustness analysis pipeline.\n",
        "        num_workers (int): The number of parallel CPU cores to use. If -1, it\n",
        "                           defaults to the number of available cores minus one.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key final DataFrames from\n",
        "                        the executed pipelines for interactive analysis.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup ---\n",
        "    # Determine the number of workers for parallel processing.\n",
        "    if num_workers == -1:\n",
        "        # Default to using all available cores minus one for system stability.\n",
        "        num_workers = max(1, os.cpu_count() - 1)\n",
        "\n",
        "    # Initialize a dictionary to hold all final results.\n",
        "    final_results: Dict[str, Any] = {}\n",
        "\n",
        "    # --- 2. Execute Main Reproduction Pipeline ---\n",
        "    if run_reproduction:\n",
        "        # Define the output directory for the main experiment.\n",
        "        reproduction_output_dir = \"alm_rl_reproduction_output\"\n",
        "\n",
        "        # Execute the full reproduction pipeline.\n",
        "        reproduction_results = run_alm_rl_reproduction_pipeline(\n",
        "            study_params=study_params,\n",
        "            output_dir=reproduction_output_dir,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        # Store the results from this phase.\n",
        "        final_results[\"reproduction_analysis\"] = reproduction_results\n",
        "\n",
        "    # --- 3. Execute Robustness Analysis Pipeline ---\n",
        "    if run_robustness:\n",
        "        # Define the output directory for the robustness tests.\n",
        "        robustness_output_dir = \"alm_rl_robustness_output\"\n",
        "\n",
        "        # Execute the full robustness analysis pipeline.\n",
        "        robustness_results = run_robustness_analysis(\n",
        "            study_params=study_params,\n",
        "            output_dir=robustness_output_dir,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        # Store the results from this phase.\n",
        "        final_results[\"robustness_analysis\"] = robustness_results\n",
        "\n",
        "    # --- 4. Final Report ---\n",
        "    # Print a summary of the completed work.\n",
        "    print(\"\\n\" + \"#\"*80)\n",
        "    print(\"Main Execution Script Finished\")\n",
        "    print(\"#\"*80)\n",
        "    if not run_reproduction and not run_robustness:\n",
        "        print(\"No pipelines were selected to run.\")\n",
        "    else:\n",
        "        print(\"Final results dictionary contains the following keys:\")\n",
        "        pprint.pprint(list(final_results.keys()))\n",
        "\n",
        "    # Return the aggregated results.\n",
        "    return final_results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block serves as the main entry point when the script is executed.\n",
        "\n",
        "    # In a real application, the STUDY_INPUTS dictionary would be loaded from a\n",
        "    # configuration file (e.g., YAML or JSON). For this self-contained example,\n",
        "    # we assume it is defined in the scope.\n",
        "    # from config.study_inputs import STUDY_INPUTS\n",
        "\n",
        "    # To run the script, you would need to have the STUDY_INPUTS dictionary\n",
        "    # and all the previously defined functions (run_alm_rl_reproduction_pipeline,\n",
        "    # run_robustness_analysis, and all their dependencies) available.\n",
        "\n",
        "    # Example usage:\n",
        "    # try:\n",
        "    #     # Run both the main experiment and the robustness analysis.\n",
        "    #     all_project_results = main(\n",
        "    #         study_params=STUDY_INPUTS,\n",
        "    #         run_reproduction=True,\n",
        "    #         run_robustness=True,\n",
        "    #         num_workers=8  # Manually specify number of cores\n",
        "    #     )\n",
        "    # except Exception as e:\n",
        "    #     print(f\"\\nAn error occurred during the main execution: {e}\")\n",
        "\n",
        "    print(\"Main script entry point reached. Define STUDY_INPUTS and uncomment the 'main' call to run.\")\n"
      ],
      "metadata": {
        "id": "AZL4qN_JDf3f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}